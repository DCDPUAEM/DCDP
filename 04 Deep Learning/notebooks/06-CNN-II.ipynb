{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TgLUQd5WLOm"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/notebooks/06-CNN-II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFc78aDaWLOo"
      },
      "source": [
        "<h1>Clasificación con Redes Neuronales Convolucionales</h1>\n",
        "\n",
        "<h3>Parte II: Generadores, Embeddings y Modelos Pre-entrenados\n",
        "\n",
        "En esta notebook usaremos una red neuronal convolucional (CNN) para clasificar el dataset *cats vs dogs* de kaggle. Observaremos, además, el efecto del dropout y analizaremos la información de las capas ocultas para ganar intuición sobre el funcionamiento interno de este tipo de redes.\n",
        "\n",
        "Además, usaremos modelos pre-entrenados y con estos obtendremos embeddings para clasificar las imágenes. Para esto, usaremos la clase `Model` de Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq-bgFf-WLOq"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN71Ul1JWLOx"
      },
      "source": [
        "Verifiquemos que el entorno de ejecución en Colab sea GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln6imMSY8vLe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('GPU presente en: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EjrtHWCWLOv"
      },
      "source": [
        "# [El dataset Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG3DcGuhdyaT"
      },
      "source": [
        "El conjunto de datos de Dogs vs Cats fue publicado por Kaggle como parte de una competencia de visión computacional a fines de 2013, cuando las CNNs no eran muy comunes.\n",
        "\n",
        "Se puede descargar el dataset original en: https://www.kaggle.com/c/dogs-vs-cats/data.\n",
        "\n",
        "Esta notebook se puede usar con dos conjuntos de datos:\n",
        "\n",
        "* Usaremos el conjunto de datos original de entrenamiento, dado que contiene las etiquetas de las clases. Este conjunto contiene 25,000 imágenes de perros y gatos (12,500 de cada clase) y tiene un tamaño de 543 MB. Ya se encuentra dividido en *train*, *validation* y *test*. [Download](https://drive.google.com/file/d/1Q3xOfn2Up9uIOLviS66oYH_oFFK-IGpW/view?usp=sharing)\n",
        "\n",
        "* Usaremos un conjunto reducido de datos, el cual contiene 1000 imágenes de cada clase para entrenamiento, 500 para validación y 500 para prueba. Todos los datos se sacaron del conjunto de entrenamiento original. [Download](https://drive.google.com/file/d/1Ce3u8dwYYriLkz5OpcGn72xIQENIHZX5/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C4obFfa2rjL"
      },
      "source": [
        "Copiaremos el dataset desde un vínculo de Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsjdNtihfL8n"
      },
      "outputs": [],
      "source": [
        "!pip install -qq gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC-8WGEgidmb"
      },
      "source": [
        "Descargamos el dataset desde Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LDaaZ3udhLI"
      },
      "outputs": [],
      "source": [
        "# ----- Versión completa -----\n",
        "# !gdown --id 1Q3xOfn2Up9uIOLviS66oYH_oFFK-IGpW\n",
        "\n",
        "# ----- Copia de la versión completa -----\n",
        "# !gdown 1hchhNQ_3WNncaXVD3kX58EIppcYFt-E2\n",
        "\n",
        "# ----- Versión reducida -----\n",
        "!gdown 1Ce3u8dwYYriLkz5OpcGn72xIQENIHZX5\n",
        "\n",
        "# ----- Copia de la versión reducida -----\n",
        "# !gdown 1NK9LvrVwsEQM0UHkFHq_GYCF2fjGrwAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXbCkAPa2w66"
      },
      "source": [
        "Descomprimimos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G3WswyR8vho"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# file_name = '/content/cnn_perros_gatos.zip'\n",
        "# file_name = '/content/cnn_perros_gatos-copia.zip'\n",
        "file_name = '/content/cnn_perros_gatos-small.zip'\n",
        "# file_name = '/content/cnn_perros_gatos-small-copia.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as myzip:\n",
        "    myzip.extractall()\n",
        "    print('Listo')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Exploremos la ruta de archivos del dataset"
      ],
      "metadata": {
        "id": "WZrlRcMpqa7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el balance de clases"
      ],
      "metadata": {
        "id": "FmPCSI8xqZbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path = '/content/cnn_perros_gatos/train'\n",
        "\n",
        "num_train_dogs = len(os.listdir(path + '/dogs'))\n",
        "num_train_cats = len(os.listdir(path + '/cats'))\n",
        "\n",
        "print(f'Número de imágenes de entrenamiento de perros: {num_train_dogs}')\n",
        "print(f'Número de imágenes de entrenamiento de gatos: {num_train_cats}')\n",
        "\n",
        "ratio = num_train_dogs / (num_train_dogs + num_train_cats)\n",
        "\n",
        "plt.figure()\n",
        "plt.suptitle(f'Número de imágenes por clase\\nRatio:{round(ratio,3)}')\n",
        "plt.bar(['Perros', 'Gatos'], [num_train_dogs, num_train_cats])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DckIM6u_vgb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFqw4JUe-P2O"
      },
      "source": [
        "Exploramos las carpetas de entrenamiento, validación y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMypQXip8vzN"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "print('Para entrenamiento:')\n",
        "\n",
        "train_dogs = 'cnn_perros_gatos/train/dogs'\n",
        "print(f'\\t{len(os.listdir(train_dogs))} Perros.')\n",
        "train_cats = 'cnn_perros_gatos/train/cats'\n",
        "print(f'\\t{len(os.listdir(train_cats))} Gatos.')\n",
        "\n",
        "print('\\nPara validación:')\n",
        "validation_dogs = 'cnn_perros_gatos/validation/dogs'\n",
        "print(f'\\t{len(os.listdir(validation_dogs))} Perros.')\n",
        "validation_cats = 'cnn_perros_gatos/validation/cats'\n",
        "print(f'\\t{len(os.listdir(validation_cats))} Gatos.')\n",
        "\n",
        "print('\\nPara prueba:')\n",
        "test_dogs = 'cnn_perros_gatos/test/dogs'\n",
        "print(f'\\t{len(os.listdir(test_dogs))} Perros.')\n",
        "test_cats = 'cnn_perros_gatos/test/cats'\n",
        "print(f'\\t{len(os.listdir(test_cats))} Gatos.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos algunas imágenes del dataset, como podemos ver:\n",
        "\n",
        "* Son archivos jpeg\n",
        "* Tienen diferentes tamaños"
      ],
      "metadata": {
        "id": "TtPRJtxIhLJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq ipyplot"
      ],
      "metadata": {
        "id": "0EVbHiCae8nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import ipyplot\n",
        "import random, os\n",
        "import numpy as np\n",
        "\n",
        "path_1 = '/content/cnn_perros_gatos/train/cats'\n",
        "path_2 = '/content/cnn_perros_gatos/train/dogs'\n",
        "\n",
        "filenames_1 = np.random.choice(os.listdir(path_1), 5, replace=False)\n",
        "filenames_2 = np.random.choice(os.listdir(path_2), 5, replace=False)\n",
        "# random.sample(os.listdir(path_1), 5)\n",
        "# filenames_2 = random.sample(os.listdir(path_2), 5)\n",
        "\n",
        "full_filenames_1 = [os.path.join(path_1, fname) for fname in filenames_1]\n",
        "full_filenames_2 = [os.path.join(path_2, fname) for fname in filenames_2]\n",
        "\n",
        "filenames = full_filenames_1 + full_filenames_2\n",
        "images_list = [Image.open(fname) for fname in filenames]\n",
        "\n",
        "ipyplot.plot_images(images_list,show_url=False)"
      ],
      "metadata": {
        "id": "6WY6JsL3ecRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las dimensiones de las imagenes"
      ],
      "metadata": {
        "id": "DPRHSPxi7PjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from seaborn import heatmap\n",
        "import numpy as np\n",
        "\n",
        "path = '/content/cnn_perros_gatos/train'\n",
        "\n",
        "widths = []\n",
        "heights = []\n",
        "\n",
        "for folder in os.listdir(path):\n",
        "    folder_path = os.path.join(path, folder)\n",
        "    for image in os.listdir(folder_path):\n",
        "        image_path = os.path.join(folder_path, image)\n",
        "        img = Image.open(image_path)\n",
        "        widths.append(img.width)\n",
        "        heights.append(img.height)\n",
        "\n",
        "min_width = min(widths)\n",
        "max_width = max(widths)\n",
        "min_height = min(heights)\n",
        "max_height = max(heights)\n",
        "\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle('Dimensiones de las imágenes')\n",
        "plt.hist(widths, bins=20, alpha=0.5, label='Width')\n",
        "plt.hist(heights, bins=20, alpha=0.5, label='Height')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist2d(widths, heights, bins=(20, 20), cmap='Blues')\n",
        "plt.xticks(range(min_width, max_width+1, 100))\n",
        "plt.yticks(range(min_height, max_height+1, 100))\n",
        "plt.colorbar()\n",
        "plt.xlabel('Width')\n",
        "plt.ylabel('Height')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NLyOc0fq7UDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 ¿Qué retos presentaría este dataset para una MLP como las que hemos definido y usado?"
      ],
      "metadata": {
        "id": "fpHSQQQa3Jzo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4aC0_mXDbXn"
      },
      "source": [
        "Definimos los directorios de entrenamiento, validación y prueba para usaralos en el resto de la notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJGFJS-qNLJc"
      },
      "outputs": [],
      "source": [
        "train_dir = 'cnn_perros_gatos/train'\n",
        "validation_dir = 'cnn_perros_gatos/validation'\n",
        "test_dir = 'cnn_perros_gatos/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además, probaremos con dos imágenes externas al dataset"
      ],
      "metadata": {
        "id": "dKDa0X5j2Gp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1eSWPCWL-mc4ekrjbh5BN25IKlNZfAECF\n",
        "!gdown 1OEUZgYKM_brFUwjNi1RmMCOLzctdimYK"
      ],
      "metadata": {
        "id": "e1KRZqps2GQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Zxc_HG1MuG"
      },
      "source": [
        "# Preprocesamiento de datos\n",
        "\n",
        "\n",
        "Los datos deben formatearse en tensores de punto flotante preprocesados adecuadamente antes de que se introduzcan en la red. En este momento, nuestros datos se encuentran almacenados como archivos JPEG, por lo que los pasos para que puedan ser introducidos en nuestra red son:\n",
        "\n",
        "* Leer los archivos de imagen.\n",
        "\n",
        "* Decodificar el contenido JPEG a cuadrículas de píxeles RBG.\n",
        "\n",
        "* Convertirlos en tensores de punto flotante.\n",
        "\n",
        "* Volver a escalar los valores de píxeles (entre 0 y 255) al intervalo $[0, 1]$ (las redes neuronales prefieren tratar con valores de entrada pequeños).\n",
        "\n",
        "Afortunadamente, Keras tiene herramientas para encargarse de estos pasos automáticamente. Keras tiene un módulo con herramientas de ayuda para procesamiento de imágenes, ubicado en **keras.preprocessing.image**. En particular, contiene la clase **ImageDataGenerator**, que permite configurar rápidamente los generadores de Python que pueden convertir automáticamente los archivos de imagen en disco en *batches* de tensores preprocesados."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una función para obtener los generadores de entranamiento, validación y prueba especificando las rutas de las carpetas y el tamaño de imagen"
      ],
      "metadata": {
        "id": "vGtIhJBQ5MFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LAZtTFHvkvZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\n",
        "from tensorflow.keras.applications.resnet import preprocess_input as resnet_preprocess\n",
        "\n",
        "def get_generators(train_dir,\n",
        "                  validation_dir,\n",
        "                  test_dir,\n",
        "                  img_size,\n",
        "                  preprocessing_mode='rescaling',\n",
        "                  model_type=None,\n",
        "                  augmentation_params=None):\n",
        "    \"\"\"\n",
        "    Crea generadores de datos para entrenamiento, validación y test.\n",
        "\n",
        "    Args:\n",
        "        train_dir (str): Directorio de entrenamiento\n",
        "        validation_dir (str): Directorio de validación\n",
        "        test_dir (str): Directorio de test\n",
        "        img_size (tuple): Tamaño de las imágenes (height, width)\n",
        "        preprocessing_mode (str): Tipo de preprocesamiento ('rescaling', 'model_specific', 'augmenting')\n",
        "        model_type (str): Tipo de modelo para preprocesamiento específico ('efficientnet', 'resnet', etc.)\n",
        "        augmentation_params (dict): Parámetros de aumento de datos personalizados\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_generator, validation_generator, test_generator)\n",
        "    \"\"\"\n",
        "    # Configuración base para test y validación (siempre sin aumento de datos)\n",
        "    test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Configuración para entrenamiento\n",
        "    if preprocessing_mode == 'rescaling':\n",
        "        train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    elif preprocessing_mode == 'model_specific':\n",
        "        if model_type == 'efficientnet':\n",
        "            train_datagen = ImageDataGenerator(preprocessing_function=effnet_preprocess)\n",
        "            test_val_datagen = ImageDataGenerator(preprocessing_function=effnet_preprocess)\n",
        "        elif model_type == 'resnet':\n",
        "            train_datagen = ImageDataGenerator(preprocessing_function=resnet_preprocess)\n",
        "            test_val_datagen = ImageDataGenerator(preprocessing_function=resnet_preprocess)\n",
        "    elif preprocessing_mode == 'augmenting':\n",
        "        if augmentation_params:\n",
        "            # Usar parámetros personalizados si se proporcionan\n",
        "            train_datagen = ImageDataGenerator(**augmentation_params)\n",
        "        else:\n",
        "            # Configuración por defecto para aumento de datos\n",
        "            train_datagen = ImageDataGenerator(\n",
        "                rescale=1./255,\n",
        "                rotation_range=40,\n",
        "                width_shift_range=0.2,\n",
        "                height_shift_range=0.2,\n",
        "                shear_range=0.2,\n",
        "                zoom_range=0.2,\n",
        "                horizontal_flip=True,\n",
        "                fill_mode='nearest'\n",
        "            )\n",
        "\n",
        "    # Crear generadores\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    validation_generator = test_val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_generator = test_val_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator, test_generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (16,16)\n",
        "\n",
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size,\n",
        "                                                                       preprocessing_mode='rescaling',\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "hCg7fCe-FpB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_labels = train_generator.classes\n",
        "\n",
        "np.unique(train_labels,return_counts=True)"
      ],
      "metadata": {
        "id": "L72uBeyNquNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_generator.class_indices)\n",
        "print(validation_generator.class_indices)\n",
        "print(test_generator.class_indices)"
      ],
      "metadata": {
        "id": "-IuMG8LjraNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHKu7G_y4fPC"
      },
      "source": [
        "\n",
        "* Vamos a revisar la salida de uno de estos generadores: produce batches de imágenes de 150 x 150 RGB (con la forma (20, 150, 150, 3)) y etiquetas binarias (con la forma (20,)). 20 es el número de muestras en cada batch (el tamaño del batch).\n",
        "* Como el generador genera estos batches de forma indefinida (i.e. recorre sin fin las imágenes presentes en la carpeta que se le indicó), se necesita romper el loop de iteración en algún punto. A continuación podemos ver cómo es cada corrida (batch/step) que proporciona el generador."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = next(train_generator)\n",
        "print(\"Labels in batch:\", y_train)\n",
        "print(\"Shape:\",x_train.shape)\n",
        "print(\"Number of class 0:\", sum(y_train==0))\n",
        "print(\"Number of class 1:\", sum(y_train==1))\n",
        "\n",
        "x_test, y_test = next(test_generator)\n",
        "print(\"Labels in batch:\", y_test)\n",
        "print(\"Shape:\",x_test.shape)\n",
        "print(\"Number of class 0:\", sum(y_test==0))\n",
        "print(\"Number of class 1:\", sum(y_test==1))\n",
        "\n",
        "x_val, y_val = next(validation_generator)\n",
        "print(\"Labels in batch:\", y_val)\n",
        "print(\"Shape:\",x_val.shape)\n",
        "print(\"Number of class 0:\", sum(y_val==0))\n",
        "print(\"Number of class 1:\", sum(y_val==1))"
      ],
      "metadata": {
        "id": "ilu7aAs_8gmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LevQH8ih4ek_"
      },
      "source": [
        "* Vamos a proceder a entrenar nuestro modelo con los datos usando el generador. Debido a que los datos se generan infinitamente, el generador necesita saber cuántas muestras extraer antes de declarar una época finalizada. Esta es la función del argumento **steps_per_epoch**\n",
        "\n",
        "* En este caso, **steps_per_epoch** corresponde al número de batches que requiere el generador para leer el conjunto de datos completo. Sólo después de haber solicitado este número de batches, el proceso de ajuste de nuestro modelo pasará a la siguiente época. **steps_per_epoch** corresponde a el número de pasos de descenso del gradiente. En nuestro caso, cada batch tiene un tamaño de 20 muestras, por lo que tomará 100 pasos (batches) hasta que cubramos las 2,000 muestras de nuestra base de datos.\n",
        "\n",
        "* Como siempre, uno puede pasar un argumento llamado **validation_data**. Es importante destacar que este argumento puede ser un generador de datos en sí mismo, pero también podría ser una tupla de arreglos Numpy. Si se pasa un generador como **validation_data**, entonces se espera que este generador produzca batches de datos de validación sin fin, y por lo tanto también se debe especificar el argumento **validation_steps**, que le dice al proceso cuántos batches debe extraer del generador de validación para su evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definamos funciones para graficar las curvas de entrenamiento y mostrar el rendimiento en el conjunto de prueba"
      ],
      "metadata": {
        "id": "1tegpJcW27f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "from seaborn import heatmap\n",
        "\n",
        "def plot_training_curves(history):\n",
        "    plt.figure(figsize=(11,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.title(\"Validation and Training Loss\",fontsize=14)\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='validation')\n",
        "    plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.title(\"Validation and Training Accuracy\",fontsize=14)\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='validation')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate(model,X,y,classes_names):\n",
        "    if len(classes_names) > 2:\n",
        "        y_pred_proba = model.predict(X)\n",
        "        y_pred = np.argmax(y_pred_proba,axis=1)\n",
        "    elif len(classes_names) == 2:\n",
        "        y_pred_proba = model.predict(X)\n",
        "        y_pred = np.where(y_pred_proba > 0.5, 1, 0).reshape(-1)\n",
        "    print(f\"Accuracy: {accuracy_score(y,y_pred)}\")\n",
        "    print(f\"F1 Score: {f1_score(y,y_pred,average='macro')}\")\n",
        "    cm = confusion_matrix(y_pred=y_pred,y_true=y)\n",
        "    plt.figure()\n",
        "    heatmap(cm,\n",
        "            fmt='g',\n",
        "            annot=True,\n",
        "            xticklabels=classes_names,\n",
        "            yticklabels=classes_names,\n",
        "            cmap='Blues'\n",
        "            )\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0gGtLZcL26Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 0: MLP"
      ],
      "metadata": {
        "id": "R4qSm78z1VEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size=(16,16),\n",
        "                                                                       preprocessing_mode='rescaling',\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "DYxeOl5p6W1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Input, Dropout\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "input_shape = (16,16,3)\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=input_shape),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               patience=5,\n",
        "                               restore_best_weights=True\n",
        "                               )\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.2,\n",
        "                              patience=4,\n",
        "                              min_lr=1e-5)"
      ],
      "metadata": {
        "id": "CpZDYny91YDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps = train_generator.samples // train_generator.batch_size\n",
        "val_steps = validation_generator.samples // validation_generator.batch_size\n",
        "\n",
        "print(f\"Número de pasos de entrenamiento: {train_steps}\")\n",
        "print(f\"Número de pasos de validación: {val_steps}\")\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=train_steps,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=val_steps,\n",
        "      callbacks=[early_stopping, reduce_lr]\n",
        "      )"
      ],
      "metadata": {
        "id": "pHdBGYgZ3UL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(history)"
      ],
      "metadata": {
        "id": "T35UQVGV2fnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "classes_names = list(train_generator.class_indices.keys())\n",
        "evaluate(model,test_generator,test_generator.classes,classes_names)"
      ],
      "metadata": {
        "id": "MYGXyIeA2jFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "3ZZWrQ5S6tRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phDgmrWsWLO6"
      },
      "source": [
        "# Modelo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZFIorONDhG9"
      },
      "source": [
        "Definamos ahora un modelo con arquitectura CNN. Usaremos las capas [`Conv2D`](https://keras.io/api/layers/convolution_layers/convolution2d/) para las operaciones de convolución y [`MaxPooling2D`](https://keras.io/api/layers/pooling_layers/max_pooling2d/) para el pooling.\n",
        "\n",
        "Pero antes, obtengamos los generadores de imágenes con un tamaño mayor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (150,150)\n",
        "\n",
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size, # Aumentamos el tamaño de imágenes\n",
        "                                                                       preprocessing_mode='rescaling', # Sólo re-escalamos\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None # Sin aumento de datos\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "dkx_J9nLFwfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1bzFBdsAq40yN95k2pA5X2OGsXf-40v0t\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "\n",
        "**Importante**: Observa la elección de los hiperparámetros `padding=\"same\"` y `strides=1`. Esta elección asegura que las salidas de cada capa convolucional tenga las mismas dimensiones que las entradas."
      ],
      "metadata": {
        "id": "DoKzp6cRF2t4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yySANB-NNr4w"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Input\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(150, 150, 3)),\n",
        "    Conv2D(32, 3, activation='relu',\n",
        "                           input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, 3, activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, 3, activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSL8Y3n7rLtL"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbnp2bbL9jes"
      },
      "source": [
        "* NOTA que comenzamos con imágenes de tamaño 150 x 150 (una elección de tamaño arbitraria) y terminamos con mapas de características de tamaño 7 x 7 justo antes de la capa de *flatten*.\n",
        "* En realidad las imágenes de entrada tienen tamaños diversos (desconocidos), pero afortunadamente Keras nos puede ayudar a pre-procesarlas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA43G8SqEv2f"
      },
      "source": [
        "Compilamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=1e-4)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vrU-K9eKLcDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "505HhEEzMn8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tarda alrededor de 3 minutos\n",
        "\n"
      ],
      "metadata": {
        "id": "blAr9k68wLoN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdjhzcgzwQ-T"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOeKcGvvWLPB"
      },
      "source": [
        "## Rendimiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtq39jKpFV2g"
      },
      "source": [
        "Guardamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFnMW2EbyyqD"
      },
      "outputs": [],
      "source": [
        "model.save('cnn_perros_gatos_model_1.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFYSGmOGZsF"
      },
      "source": [
        "Grafiquemos las curvas de aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt6kRFIYzjPb"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting**... ¡Tenemos muy pocos ejemplos!"
      ],
      "metadata": {
        "id": "_GnVvFeiJVsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "VmNuNQb-IC6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos el desempeño del modelo a detalle"
      ],
      "metadata": {
        "id": "kM0_LAVp3MnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "classes_names = list(train_generator.class_indices.keys())\n",
        "evaluate(model,test_generator,\n",
        "         test_generator.classes,\n",
        "         classes_names)"
      ],
      "metadata": {
        "id": "-VgCUZcg3MLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 2: Aumento de datos"
      ],
      "metadata": {
        "id": "y4GxjIoaMv_t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXzBZmaj_V2_"
      },
      "source": [
        "## Aumento de Datos\n",
        "\n",
        "* El efecto de sobreajuste ocurre cuando se tienen muy pocas muestras de las que aprender, lo que nos impide entrenar un modelo capaz de generalizar a nuevos datos. Si tuviesemos datos infinitos, nuestro modelo estaría expuesto a todos los aspectos posibles de la distribución de datos en cuestión y nunca se sobreajustaría nuestro modelo.\n",
        "\n",
        "* El aumento de datos adopta el enfoque de generar más datos de entrenamiento a partir de muestras de entrenamiento existentes, al \"aumentar\" las muestras a través de una serie de transformaciones aleatorias que producen imágenes de apariencia creíble. El objetivo es que durante el tiempo de entrenamiento, nuestro modelo nunca vea exactamente la misma imagen dos veces. Esto ayuda a que el modelo se exponga a más aspectos de los datos y generalice mejor.\n",
        "\n",
        "* En Keras, esto se puede hacer configurando una serie de transformaciones aleatorias que se realizarán en las imágenes leídas por nuestra instancia de ImageDataGenerator.\n",
        "\n",
        "* Vamos a comenzar por aumentar una imagen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_qyda81Q81"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U48tZWbBB2TM"
      },
      "source": [
        "Las opciones anteriores son solo algunas de las opciones disponibles.\n",
        "\n",
        "* **rotation_range** es un valor en grados (0-180), un rango dentro del cual girar las imágenes de forma aleatoria.\n",
        "\n",
        "* **width_shift** y **height_shift** son rangos expresados como una fracción del ancho o altura total de la imagen, dentro de los cuales se pueden trasladar vertical u horizontalmente de forma aleatoria a las imágenes.\n",
        "\n",
        "* **shear_range** aplica aleatoriamente transformaciones de corte.\n",
        "\n",
        "* **zoom_range** aplica acercamientos aleatorios dentro de las imágenes.\n",
        "\n",
        "* **horizontal_flip** Voltea de forma aleatoria la mitad en las imágenes horizontalmente.\n",
        "\n",
        "* **fill_mode** es la estrategia utilizada para rellenar píxeles creados, que pueden aparecer después de una rotación o un cambio de ancho / altura."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos 25 imágenes modificadas a partir de una misma imagen"
      ],
      "metadata": {
        "id": "XgJ_5x2rtyM4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbyqp-q61cbr"
      },
      "outputs": [],
      "source": [
        "from keras import utils\n",
        "import random\n",
        "\n",
        "train_cats_dir = '/content/cnn_perros_gatos/train/cats' # El directorio donde están las imágenes de gatos de entrenamiento\n",
        "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
        "img_path = random.sample(fnames, 1)[0] # Escogemos una imagen al azar para aplicar el \"aumentado de datos\"\n",
        "img = utils.load_img(img_path, target_size=(150, 150)) # Leemos la imagen y la redimensionamos.\n",
        "x = utils.img_to_array(img) # Leemos la imagen y la redimensionamos.\n",
        "x = x.reshape((1,) + x.shape) # Redimensionamos el arreglo a (1, 150, 150, 3)\n",
        "\n",
        "'''\n",
        "El comando .flow () genera batches de imágenes transformadas aleatoriamente\n",
        "Con el \"for\" de abajo estaremos en un loop indefinidamente,\n",
        "Necesitamos 'romper' el loop en algún momento\n",
        "'''\n",
        "\n",
        "fig, axs = plt.subplots(5,5,figsize=(10,10))\n",
        "k = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    i = k//5\n",
        "    j = k%5\n",
        "    axs[i,j].imshow(utils.array_to_img(batch[0]))\n",
        "    axs[i,j].axis('off')\n",
        "    k += 1\n",
        "    if k == 25:\n",
        "        break\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teFUV-JSFKZH"
      },
      "source": [
        "* Si entrenamos una nueva red neuronal utilizando esta configuración de aumento de datos, nuestra red nunca verá dos veces la misma entrada, pues a cada nueva imagen se le aplica transformaciones aleatorias dentro de ciertos rangos.\n",
        "\n",
        "* Sin embargo, las entradas que ves están aún muy interrelacionadas, ya que provienen de un pequeño número de imágenes originales: **no podemos producir nueva información, sólo podemos mezclar la información existente**.\n",
        "\n",
        "* Dado que esto podría no ser suficiente para librarnos del sobreajuste. Para mitigarlo aún más, también agregaremos una capa de Dropout a nuestro modelo, justo antes de la etapa del clasificador densamente conectado (fully-connected)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los nuevos generadores de imágenes"
      ],
      "metadata": {
        "id": "YO3rm6nGoyRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size=(150,150),\n",
        "                                                                       preprocessing_mode='augmenting',\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None)"
      ],
      "metadata": {
        "id": "isCG_ejdoyAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ5RY_qYGlln"
      },
      "source": [
        "Definimos la misma red CNN, con dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYgaCOd2R4HU"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Input\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(150, 150, 3)),\n",
        "    Conv2D(32, 3, activation='relu', name='Convolution1'),\n",
        "    MaxPooling2D(name='MaxPooling1'),\n",
        "    Conv2D(64, 3, activation='relu',name='Convolution2'),\n",
        "    MaxPooling2D(name='MaxPooling2'),\n",
        "    Conv2D(128, 3, activation='relu',name='Convolution3'),\n",
        "    MaxPooling2D(name='MaxPooling3'),\n",
        "    Conv2D(128, 3, activation='relu',name='Convolution4'),\n",
        "    MaxPooling2D(name='MaxPooling4'),\n",
        "    Flatten(name='Flatten'),\n",
        "    Dropout(0.1,name='DropOut'), # 0.5\n",
        "    Dense(512, activation='relu',name='Densa'),\n",
        "    Dense(1, activation='sigmoid',name='Salida')\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop\n",
        "\n",
        "opt = RMSprop(learning_rate=1e-4)"
      ],
      "metadata": {
        "id": "5hjNpXpZevmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6bwra-5TfZ3"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "-BL7MuurM3p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenemos el modelo.\n",
        "\n",
        "**Observación**: Para mejores resultados entrenar durante 100 épocas (tarda alrededor de 30 minutos). Por cuestiones de tiempo, entrenamos con 30 épocas (tarda alrededor de 8 minutos). Con ambos obtendremos un poco más de 80% de accuracy."
      ],
      "metadata": {
        "id": "fRqn2i_1wUZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "                train_generator,\n",
        "                steps_per_epoch=100,\n",
        "                epochs=30,\n",
        "                validation_data=validation_generator,\n",
        "                validation_steps=50,\n",
        "                verbose=1)"
      ],
      "metadata": {
        "id": "idfaLKGcwUDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podríamos guardar el modelo"
      ],
      "metadata": {
        "id": "tq2kAHx0px-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5VV-0J0UTGN"
      },
      "outputs": [],
      "source": [
        "model.save('cnn_perros_gatos_model2_30_epochs.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CHs_DRqGwKk"
      },
      "source": [
        "Veamos las curvas de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pYHbXOlUXPx"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos el desempeño"
      ],
      "metadata": {
        "id": "wR66A_ZY2wrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "L4A9vsXhIAh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "evaluate(model,test_generator,\n",
        "         test_generator.classes,\n",
        "         classes_names)"
      ],
      "metadata": {
        "id": "slKSl9tP3hWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_-OWUThLg-f"
      },
      "source": [
        "## ¿Qué pasa si entrenamos con más épocas y todo el dataset?\n",
        "\n",
        "A continuación se muestras las gráficas de entrenamiento y la matriz de confusión con un modelo más grande, entrenado durante cerca de 50 épocas, usando todo el conjunto de entrenamiento completo.\n",
        "\n",
        "El accuracy en el conjunto de prueba fue de 87%.\n",
        "\n",
        "Podemos descargar este modelo ya entrenado de Google Drive\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1l4zeHmnDvsxhrHlMH0xgzT9BaYfa2oMs\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1eNMacyz1KA0WAwJW3fn1ktfm5_MrpJem\" width=\"600\" />\n",
        "</p>"
      ],
      "metadata": {
        "id": "2iwh6EJPmK06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 13L5oCFXIgw22FR8irsuwNHvnzekA7bGd"
      ],
      "metadata": {
        "id": "gjHZ07i3IQsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfXwuYwPda0Q"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('/content/cnn_perros_gatos_improved.keras')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'cnn_perros_gatos/train'\n",
        "validation_dir = 'cnn_perros_gatos/validation'\n",
        "test_dir = 'cnn_perros_gatos/test'\n",
        "\n",
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size=(150,150),\n",
        "                                                                       preprocessing_mode='rescaling',\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None)"
      ],
      "metadata": {
        "id": "XmQievMvW5C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "classes_names = list(train_generator.class_indices.keys())\n",
        "evaluate(model,test_generator,\n",
        "         test_generator.classes,\n",
        "         classes_names)"
      ],
      "metadata": {
        "id": "41XwoYZhX-fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "T7_ZAXDLZHhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 ¿Qué imágenes son las que está confundiendo el modelo?"
      ],
      "metadata": {
        "id": "2TeMs-ihBlfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "y_pred_proba = model.predict(test_generator)\n",
        "y_pred = np.where(y_pred_proba > 0.5, 1, 0).reshape(-1)\n",
        "\n",
        "false_positives_idxs = np.where((y_pred == 1) & (test_generator.classes == 0))[0]\n",
        "false_negatives_idxs = np.where((y_pred == 0) & (test_generator.classes == 1))[0]\n",
        "\n",
        "print(f\"False positives: {false_positives_idxs.shape[0]}\")\n",
        "print(f\"False negatives: {false_negatives_idxs.shape[0]}\")"
      ],
      "metadata": {
        "id": "AQowWN3PBqrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Diccionario con el nombre de cada clase\n",
        "test_generator.class_indices\n",
        "idxs_to_classes = {v:k for k,v in test_generator.class_indices.items()}\n",
        "\n",
        "# Escogemos un ejemplo de falso positivo y uno de falso negativo\n",
        "fp_idx = np.random.choice(false_positives_idxs,size=1)\n",
        "fn_idx = np.random.choice(false_negatives_idxs,size=1)\n",
        "\n",
        "# Obtenemos los nombres de archivo de esos índices\n",
        "test_generator.reset()\n",
        "fp_filename = test_generator.filenames[fp_idx[0]]\n",
        "fn_filename = test_generator.filenames[fn_idx[0]]\n",
        "\n",
        "# Leemos las imágenes\n",
        "fp_img = imageio.v2.imread(os.path.join(test_dir,fp_filename))\n",
        "fn_img = imageio.v2.imread(os.path.join(test_dir,fn_filename))\n",
        "\n",
        "# Mostramos las imágenes\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
        "axs[0].imshow(fp_img)\n",
        "axs[0].set_title(f'Prediction: {idxs_to_classes[y_pred[fp_idx][0]]}')\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(fn_img)\n",
        "axs[1].set_title(f'Prediction: {idxs_to_classes[y_pred[fn_idx][0]]}')\n",
        "axs[1].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hRQx0iY7CbZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imageio.v2 import imread\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "dog_path = 'dc_dog.jpg'\n",
        "cat_path = 'dc_cat.jpg'\n",
        "\n",
        "dog_img = imread(dog_path)\n",
        "cat_img = imread(cat_path)\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(dog_img)\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(cat_img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UPE-d98S2eGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Función para cargar y preprocesar una imagen\n",
        "def load_and_preprocess_image(img_path, target_size=(150, 150)):\n",
        "    # Cargar imagen y redimensionar a 150x150\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    # Convertir a array numpy\n",
        "    img_array = image.img_to_array(img)\n",
        "    # Normalizar valores de píxeles al rango [0,1]\n",
        "    img_array = img_array / 255.0\n",
        "    return img_array"
      ],
      "metadata": {
        "id": "c0yldXWh3v7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_img = load_and_preprocess_image(dog_path)\n",
        "cat_img = load_and_preprocess_image(cat_path)\n",
        "\n",
        "# Crear el tensor combinando ambas imágenes\n",
        "images_tensor = np.stack([dog_img, cat_img], axis=0)\n",
        "\n",
        "print(images_tensor.shape)\n",
        "\n",
        "preds = model.predict(images_tensor)\n",
        "print(f\"Salida de la red:\\t{preds.reshape(-1,)}\\n\")\n",
        "\n",
        "predictions = np.where(preds > 0.5, 1, 0).reshape(-1)\n",
        "\n",
        "idxs_to_classes = {v:k for k,v in test_generator.class_indices.items()}\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(dog_img)\n",
        "plt.axis('off')\n",
        "plt.title(f'Prediction: {idxs_to_classes[predictions[0]]}')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(cat_img)\n",
        "plt.axis('off')\n",
        "plt.title(f'Prediction: {idxs_to_classes[predictions[1]]}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JKcE6e8x3HsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡ Usando la red para obtener features de las imágenes: Embeddings\n",
        "\n",
        "En esta parte de la notebook ilustraremos cómo la parte convolucional de las redes CNN se puede ver como un método de extracción de features. Es decir, podemos ver al bloque convolucional como un método que convierte cada imagen en un vector, de una *buena* manera.\n",
        "\n",
        "Para esto, usaremos el modelo CNN pre-entrenado con el dataset complejo."
      ],
      "metadata": {
        "id": "yOz_Ei4QRBUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 13L5oCFXIgw22FR8irsuwNHvnzekA7bGd"
      ],
      "metadata": {
        "id": "8CKova_FRJSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('/content/cnn_perros_gatos_improved.keras')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-NSN4D3aRX2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos acceder a las distintas capas"
      ],
      "metadata": {
        "id": "N7HFEo2vMbXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.layers)"
      ],
      "metadata": {
        "id": "lAxKi6niMIl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, definimos un modelo de Keras que será el mismo modelo pre-entrenado pero sin la capa de salida.\n",
        "\n",
        "Para esto, usamos la clase `Model` de Keras.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1lbLQ7D1_QElq_iTscpQ_rjXPWJ2uPke3\" width=\"600\" />\n",
        "</p>"
      ],
      "metadata": {
        "id": "eQ7102MiJafN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "\n",
        "first_layer = model.layers[0] # Capa de entrada del modelo original\n",
        "\n",
        "features_layer = model.layers[20] # Si quieremos la salida justo antes de la capa de salida\n",
        "# features_layer = model.layers[15]  # Si queremos la salida de la parte convolucional\n",
        "\n",
        "# Creamos el modelo especificando la(s) entrada(s) y salida(s)\n",
        "features_model = Model(inputs=first_layer.input, outputs=features_layer.output)"
      ],
      "metadata": {
        "id": "5xI21kaGMek0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los generadores"
      ],
      "metadata": {
        "id": "R69zghhQYlEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size=(150,150),\n",
        "                                                                       preprocessing_mode='rescaling',\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "XaEMbB2nYk0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como necesitamos obtener predicciones sobre el conjunto de prueba, definamos la siguiente función:"
      ],
      "metadata": {
        "id": "WLi-f-VqAuDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm  # Opcional: para barra de progreso\n",
        "\n",
        "def get_embeddings_with_shuffled_generator(model, generator):\n",
        "    \"\"\"\n",
        "    Obtiene embeddings y etiquetas REALES de un generador (incluso con shuffle=True).\n",
        "\n",
        "    Args:\n",
        "        model: Modelo de Keras que devuelve embeddings (sin capa softmax).\n",
        "        generator: Generador de imágenes (DirectoryIterator).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (y_true, embeddings) - etiquetas y embeddings alineados.\n",
        "    \"\"\"\n",
        "    generator.reset()  # Reinicia el generador\n",
        "    y_true = []\n",
        "    embeddings = []\n",
        "\n",
        "    # Iterar sobre TODOS los batches del generador\n",
        "    for _ in tqdm(range(len(generator))):\n",
        "        x_batch, y_batch = next(generator)\n",
        "        y_true.extend(y_batch)\n",
        "        batch_embeddings = model.predict(x_batch, verbose=0)\n",
        "        embeddings.extend(batch_embeddings)\n",
        "\n",
        "    # Convertir a numpy array\n",
        "    y_true = np.array(y_true)\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    return y_true, embeddings"
      ],
      "metadata": {
        "id": "SCId_H7MAtTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos las features para el conjunto de entrenamiento y prueba, pasandolas por este nuevo modelo `features_model`"
      ],
      "metadata": {
        "id": "EwNy8xXsL_d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train, train_features = get_embeddings_with_shuffled_generator(features_model, train_generator)"
      ],
      "metadata": {
        "id": "ob3bOkl-CXv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "y_test = test_generator.classes\n",
        "\n",
        "test_generator.reset()\n",
        "test_features = features_model.predict(test_generator)\n",
        "print(test_features.shape)"
      ],
      "metadata": {
        "id": "FaG7JPTZR2Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de este punto, ya podemos usar estas features como features para cualquier método de Machine Learning (clásico o profundo)."
      ],
      "metadata": {
        "id": "ubaRPGwPMIdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "clfs = [SVC(),\n",
        "        DecisionTreeClassifier(max_depth=10),\n",
        "        RandomForestClassifier(n_estimators=50,max_depth=10),\n",
        "        KNeighborsClassifier()]\n",
        "names = [x.__class__.__name__ for x in clfs]\n",
        "\n",
        "for clf, name in zip(clfs, names):\n",
        "    clf = Pipeline([('scaler', StandardScaler()), (name, clf)])\n",
        "    clf.fit(train_features, train_generator.classes)\n",
        "    y_pred_train = clf.predict(train_features)\n",
        "    y_pred_test = clf.predict(test_features)\n",
        "    print(f'{name} - Train Accuracy: {accuracy_score(y_true=y_train, y_pred=y_pred_train)}')\n",
        "    print(f'{name} - Test Accuracy: {accuracy_score(y_true=y_test, y_pred=y_pred_test)}')\n"
      ],
      "metadata": {
        "id": "jCwfSUboSJrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Tenemos mejores métricas que con la red MLP. Esto nos dice que la parte convolucional de la red es un buen método para extraer features."
      ],
      "metadata": {
        "id": "4SaUjlhwIr_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "train_pca = pca.fit_transform(train_features)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(train_pca[:,0], train_pca[:,1], c=train_generator.classes)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OdUBiTTdWhqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "train_tsne = tsne.fit_transform(train_features)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(train_tsne[:,0], train_tsne[:,1], c=train_generator.classes)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PCilb7k-V-b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando un modelo pre-entrado especializado"
      ],
      "metadata": {
        "id": "Oqo_bBqCFZRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EfficientNetB0** es una red neuronal convolucional pre-entrenada en el conjunto de datos ImageNet, que puede clasificar imágenes en 1000 categorías de objetos. Es una de las variantes de la familia EfficientNet, conocida por su eficiencia en términos de parámetros y cálculos, logrando un buen equilibrio entre precisión y tamaño.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1y5dsxf3gUaaVaodEcwkuovw-zgrIDuXI\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "[Documentación](https://keras.io/api/applications/efficientnet/)"
      ],
      "metadata": {
        "id": "zWU6r3qUhV6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'cnn_perros_gatos/train'\n",
        "validation_dir = 'cnn_perros_gatos/validation'\n",
        "test_dir = 'cnn_perros_gatos/test'\n",
        "\n",
        "generators = get_generators(train_dir, validation_dir, test_dir,\n",
        "                            img_size=(224,224),\n",
        "                            preprocessing_mode='model_specific',\n",
        "                            model_type='efficientnet',\n",
        "                            augmentation_params=None\n",
        "                            )\n",
        "\n",
        "train_generator_en, validation_generator_en, test_generator_en = generators"
      ],
      "metadata": {
        "id": "itT-K_y6sxGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el modelo preentrenado (sin la capa superior 'softmax')\n",
        "model_en = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "y_train, train_features_EN = get_embeddings_with_shuffled_generator(model_en, train_generator_en)\n",
        "print(\"Train embedding shape:\", train_features_EN.shape)\n",
        "\n",
        "test_generator_en.reset()\n",
        "y_test = test_generator_en.classes\n",
        "test_features_EN = model_en.predict(test_generator_en)\n",
        "print(\"Test embedding shape:\", test_features_EN.shape)"
      ],
      "metadata": {
        "id": "Ny1CAbqzM6jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "clfs = [SVC(),\n",
        "        DecisionTreeClassifier(max_depth=5),\n",
        "        RandomForestClassifier(n_estimators=30,max_depth=5),\n",
        "        KNeighborsClassifier()]\n",
        "names = [x.__class__.__name__ for x in clfs]\n",
        "\n",
        "for clf, name in zip(clfs, names):\n",
        "    clf = Pipeline([('scaler', StandardScaler()),\n",
        "                    ('pca', PCA(n_components=100)),\n",
        "                    (name, clf)])\n",
        "    clf.fit(train_features_EN, y_train)\n",
        "    y_pred_train = clf.predict(train_features_EN)\n",
        "    y_pred_test = clf.predict(test_features_EN)\n",
        "    print(f'{name} - Train Accuracy: {accuracy_score(y_true=y_train, y_pred=y_pred_train)}')\n",
        "    print(f'{name} - Test Accuracy: {accuracy_score(y_true=y_test, y_pred=y_pred_test)}')"
      ],
      "metadata": {
        "id": "zukQUAONUQWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "train_pca = pca.fit_transform(train_features_EN)\n",
        "test_pca = pca.transform(test_features_EN)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(train_pca[:,0], train_pca[:,1], c=y_train)\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(test_pca[:,0], test_pca[:,1], c=y_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NGBiIYehF4B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ml = Pipeline([('scaler', StandardScaler()),\n",
        "                    ('pca', PCA(n_components=100)),\n",
        "                    ('clf', SVC())])\n",
        "\n",
        "model_ml.fit(train_features_EN, y_train)\n",
        "\n",
        "evaluate(model_ml, test_features_EN, y_test, ['cat','dog'])"
      ],
      "metadata": {
        "id": "0FvnWEm0KlQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos algunas de las clasificaciones incorrectas"
      ],
      "metadata": {
        "id": "YAWIJTl3uGNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imageio.v2 import imread\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "y_test_pred = model_ml.predict(test_features_EN)\n",
        "\n",
        "mistakes_idxs = np.where(y_test_pred != y_test)[0]\n",
        "\n",
        "idxs = np.random.choice(mistakes_idxs,size=3)\n",
        "\n",
        "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
        "for i,idx in enumerate(idxs):\n",
        "    img = imread(os.path.join(test_dir,test_generator_en.filenames[idx]))\n",
        "    axs[i].imshow(img)\n",
        "    axs[i].set_title(f'Prediction: {int(y_test_pred[idx])}')\n",
        "    axs[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ty_cqO5Js3s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKDZA5MnVnt3"
      },
      "source": [
        "# 🔽 Visualizando los mapas de características de una red neuronal convolucional\n",
        "\n",
        "Algunos señalan que los modelos de aprendizaje profundo funcionan como \"cajas negras\", pues aprenden representaciones que son difíciles de extraer y presentar de una forma legible para el ser humano.\n",
        "\n",
        "Si bien esto es parcialmente cierto para algunos tipos de modelos de aprendizaje profundo, definitivamente no lo es para las redes convolucionales (*CNN*). Las representaciones aprendidas por las redes convolucionales son altamente susceptibles de visualización, en gran parte porque son representaciones de conceptos visuales. Desde 2013, se ha desarrollado una amplia gama de técnicas para visualizar e interpretar estas representaciones. No exploraremos todas ellas, pero mencionaremos tres de las más accesibles y útiles:\n",
        "\n",
        "\n",
        "\n",
        "*   **Visualización de las salidas intermedias de una *CNN*  (\"activaciones intermedias\")**. Este método es útil para entender cómo las capas sucesivas de una red convolucional transforman su entrada y para obtener una noción de la función de los filtros individuales en una red convolucional .\n",
        "\n",
        "*   **Visualización de los  filtros en una CNN**. Este método es útil para entender con precisión a qué patrón o concepto visual es receptivo cada filtro en una red convolucional.\n",
        "\n",
        "*   **Visualización de los mapas de calor de activación por clase en una imagen**. Este método es útil para entender qué parte de una imagen se identificó como perteneciente a una clase determinada y, por lo tanto, permite localizar objetos en imágenes.\n",
        "\n",
        "En este ejercicio, abordaremos únicamente el primer método, la visualización de las activaciones intermedias o mapas de características. Para ello, usaremos la CNN que entrenamos anteriormente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qczIqCMEzRcb"
      },
      "source": [
        "Seleccionaremos una imagen de entrada, puede ser cualquier imagen del **conjunto de test**. Por ser del conjunto de test, no forma parte de las imágenes sobre las que se entrenó la red.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2CcwGp7dcMD"
      },
      "outputs": [],
      "source": [
        "from keras import utils\n",
        "import numpy as np\n",
        "\n",
        "# ----- Para el conjunto de datos completo ----\n",
        "# img_path = 'cnn_perros_gatos/test/cats/cat.147.jpg'   # Una imágen de un gato\n",
        "# img_path = 'cnn_perros_gatos/test/dogs/dog.1517.jpg'  # Una imágen de un perro\n",
        "\n",
        "# ----- Para el conjunto de datos reducido ----\n",
        "img_path = '/content/cnn_perros_gatos/test/cats/cat.10128.jpg'\n",
        "# img_path = '/content/cnn_perros_gatos/test/dogs/dog.10086.jpg'\n",
        "\n",
        "# ----- Preprocesamos la imagen en un tensor 4D\n",
        "\n",
        "img = utils.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = utils.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "\n",
        "# ----- Debemos recordar que el modelo fue entrenado con imagenes de entrada preprocesadas de la siguiente manera:\n",
        "img_tensor /= 255.\n",
        "\n",
        "# Debemos ver que su forma es de (1, 150, 150, 3)\n",
        "print(img_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjD5E3W05kjX"
      },
      "source": [
        "Mostramos la imágen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRaoI0vSdfcF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# # Para usar alguna de las imagenes reales\n",
        "# img_tensor = dog_img.reshape((1,150,150,3))\n",
        "# img_tensor = cat_img.reshape((1,150,150,3))\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.axis('Off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH5ebeoL56FH"
      },
      "source": [
        "* Para extraer los mapas de características que queremos visualizar, crearemos un modelo de Keras que toma lotes ó *batches* de imágenes como entrada y genera las activaciones de todas las capas de convolución y *pooling*.\n",
        "* Para ello, utilizaremos la clase de Keras **Model**, que ya vimos anteriormente. Un **model** se instancia mediante dos argumentos: un tensor de entrada (o lista de tensores de entrada) y un tensor de salida (o lista de tensores de salida). La clase resultante es un modelo de Keras, igual que los modelos secuenciales (Sequential models) que ya estudiamos, que mapea las entradas especificadas a las salidas especificadas. Lo que distingue a la clase **Model** es que permite modelos con múltiples salidas, a diferencia de **Sequential**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 13L5oCFXIgw22FR8irsuwNHvnzekA7bGd"
      ],
      "metadata": {
        "id": "71HBU6mGuN6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('/content/cnn_perros_gatos_improved.keras')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Gis-AZQ5uSCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos la forma de la salida de la primer capa"
      ],
      "metadata": {
        "id": "AbmmPjXpuXNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].output"
      ],
      "metadata": {
        "id": "A7VGUdgDnmYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las formas y nombres de las capas del modelo. Además, identifiquemos los índices de las capas convolucionales"
      ],
      "metadata": {
        "id": "0-Y9MDqjuaoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layers_idxs = []\n",
        "conv_layers_names = []\n",
        "\n",
        "for idx, layer in enumerate(model.layers):\n",
        "    print(f\"{idx}\\t{layer.name}\")\n",
        "    if layer.name.startswith('conv'):\n",
        "        conv_layers_idxs.append(idx)\n",
        "        conv_layers_names.append(layer.name)\n",
        "\n",
        "print(conv_layers_idxs)"
      ],
      "metadata": {
        "id": "1ZByn1BRPTk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creemos un modelo tipo `Model` con tantas salidas como capas convolucionales tenemos."
      ],
      "metadata": {
        "id": "UaWvakTXu2J_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiDTipp_diyQ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "\n",
        "# Extraemos las salidas de las 8 capas superiores:\n",
        "layer_outputs = [model.layers[j].output for j in conv_layers_idxs]\n",
        "\n",
        "first_layer = model.layers[0]\n",
        "\n",
        "# Creamos un modelo que devolverá estas salidas, dada la entrada al modelo:\n",
        "activation_model = Model(inputs=first_layer.input, outputs=layer_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB4r9KkU9IEy"
      },
      "source": [
        "* Cuando se introduce una imagen como entrada a la red, este modelo devuelve los valores de las activaciones de las capas del modelo original. Hasta antes de esta sección del ejercicio, el modelo que se presentó sólo tenía exactamente una entrada y una salida. Ahora estamos introduciendo el concepto de un modelo con múltiples salidas.\n",
        "\n",
        "* En el caso general, un modelo podría tener cualquier número de entradas y salidas. Este último modelo tiene una entrada y varias salidas, una salida por capa de convolución. Aunque, cada capa de convolución tiene varias dimensiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwuC7FEWdl8y"
      },
      "outputs": [],
      "source": [
        "# Esto devolverá una lista de arreglos de Numpy: Un arreglo por capa de activación\n",
        "activations = activation_model.predict(img_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Número de salidas del modelo: {len(activations)}\")\n",
        "for j,activation in enumerate(activations):\n",
        "    print(f\"Forma de la activación {j}: {activation.shape}\")"
      ],
      "metadata": {
        "id": "lJ-LjFp1o7lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparar con el modelo original"
      ],
      "metadata": {
        "id": "3aWHZ-ZQvtPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "vSr7WYU6vv91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z460Fyq3TWJm"
      },
      "source": [
        "Es un mapa de características con una dimensión de 148 x 148 con 32 canales o profundidad.\n",
        "\n",
        "Vamos a visualizar uno de estos canales de ese mapa de características."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAMBCCJxdqj1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "first_layer_activation = activations[0]\n",
        "\n",
        "_, w, h, ch = first_layer_activation.shape\n",
        "print(f\"La salida son {ch} imágenes de {w}x{h}\")\n",
        "print(f\"Shape: {first_layer_activation.shape}\")\n",
        "\n",
        "num_canal = 18\n",
        "\n",
        "plt.matshow(first_layer_activation[0, :, :, num_canal], cmap='plasma')\n",
        "plt.axis('Off')\n",
        "plt.suptitle(f\"Canal {num_canal}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhiUH3GhX29w"
      },
      "source": [
        "Finalmente, vamos a desplegar un gráfico completo de todas las activaciones en la red. En otras palabras, vamos a extraer y mostrar cada canal presente en cada uno de los mapas de características. Apilaremos los resultados secuencialmente, con los canales colocados uno junto al otro.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA8QlWTZNVyI"
      },
      "source": [
        "Primero, guardamos los nombres de las capas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU3C1qdgdyQs"
      },
      "outputs": [],
      "source": [
        "# Recordemos los nombres de las capas convolucionales del modelo\n",
        "print(conv_layers_names)\n",
        "\n",
        "# Especificamos cuántas imagenes por cada renglón\n",
        "images_per_row = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI1_CVYgd2QW"
      },
      "outputs": [],
      "source": [
        "for k,(layer_name, layer_activation) in enumerate(zip(conv_layers_names, activations)):\n",
        "    # Este es el número de canales presentes en un mapa de características\n",
        "    n_features = layer_activation.shape[-1]\n",
        "\n",
        "    # El mapa de características tiene la forma: (1, size, size, n_features)\n",
        "    size = layer_activation.shape[1]\n",
        "\n",
        "    # Vamos a colocar los canales de activación en esta matriz\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size)) # Aquí vamos a poner toda la imagen de salida de la capa\n",
        "\n",
        "    # Colocaremos cada mapa en esta gran malla horizontal\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
        "\n",
        "            '''\n",
        "            Este proceso toma la salida de la capa convolucional\n",
        "            (que pueden tener cualquier rango de valores) y las\n",
        "            transforma en una imagen visualizable con buen contraste\n",
        "            y en el formato estándar de píxeles.\n",
        "            '''\n",
        "            channel_image -= channel_image.mean() # Centrado en cero\n",
        "            channel_image /= channel_image.std() # Normalización por desviación estándar\n",
        "            channel_image *= 64 # Escalado de contraste\n",
        "            channel_image += 128 # Desplazamiento a rango positivo\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8') # Limitación al rango válido\n",
        "            display_grid[col * size : (col + 1) * size,\n",
        "                         row * size : (row + 1) * size] = channel_image # Llenamos la parte correspondiente de la imagen\n",
        "\n",
        "    # Mostramos los mapas en la malla\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.axis('Off')\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "    plt.savefig(f\"mascara-{k+1}.png\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh6Ip2qTYete"
      },
      "source": [
        "# Observaciones\n",
        "\n",
        "Del gráfico de mapas de características podemos notar lo siguiente:\n",
        "\n",
        "* La primera capa de la red actúa como una colección de varios detectores de borde. En esa etapa, las activaciones aún retienen casi toda la información presente en la imagen inicial.\n",
        "\n",
        "* A medida que avanzamos en profundidad, las activaciones se vuelven cada vez más abstractas y menos interpretables visualmente. Se comienzan a codificar conceptos de nivel superior como \"oreja de gato\" u \"ojo de gato\". Las representaciones superiores llevan cada vez menos información sobre el contenido visual de la imagen, y cada vez más información relacionada con la clase de la imagen.\n",
        "\n",
        "* La escasez de activaciones aumenta con la profundidad de la red: en la primera capa, todos los filtros se activan mediante la imagen de entrada, pero en las siguientes capas, más y más canales de activación están en blanco. Esto significa que el patrón codificado por el filtro no se encuentra en la imagen de entrada.\n",
        "\n",
        "# Comentarios Finales\n",
        "\n",
        "Acabamos de evidenciar un hecho muy importante de las representaciones aprendidas por las redes neuronales profundas: las características extraídas por una capa se vuelven cada vez más abstractas con la profundidad de la red.\n",
        "\n",
        "Las activaciones de las capas superiores contienen cada vez menos información sobre la entrada específica que se está viendo y más información sobre el objetivo (en el caso de este ejemplo, la clase de la imagen: gato o perro). Una red neuronal profunda actúa efectivamente como un *pipeline* (tubería) que destila la información, con datos en crudo que entran (en nuestro caso, imágenes RBG) y se transforman repetidamente de tal forma que la información irrelevante es filtrada (por ejemplo, la apariencia visual específica de la imagen) mientras que la información útil es magnificada y refinada (por ejemplo, la clase de la imagen).\n",
        "\n",
        "Esto es análogo a la forma en que los humanos y los animales perciben el mundo: después de observar una escena durante unos segundos, un humano puede recordar qué objetos abstractos estaban presentes en él (por ejemplo, una bicicleta, un árbol) pero muchas veces no puede recordar la apariencia específica de estos objetos.\n",
        "\n",
        "El cerebro ha aprendido a abstraer completamente la información visual, a transformarla en conceptos visuales de alto nivel mientras filtra por completo los detalles visuales irrelevantes, haciendo que sea tremendamente difícil recordar cómo se ven exactamente las cosas a nuestro alrededor.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5R3eylJLMUNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
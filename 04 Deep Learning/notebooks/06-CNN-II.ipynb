{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TgLUQd5WLOm"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/notebooks/06-CNN-II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFc78aDaWLOo"
      },
      "source": [
        "<h1>Clasificaci칩n con Redes Neuronales Convolucionales</h1>\n",
        "\n",
        "<h3>Parte II: Generadores, Embeddings y Modelos Pre-entrenados\n",
        "\n",
        "En esta notebook usaremos una red neuronal convolucional (CNN) para clasificar el dataset *cats vs dogs* de kaggle. Observaremos, adem치s, el efecto del dropout y analizaremos la informaci칩n de las capas ocultas para ganar intuici칩n sobre el funcionamiento interno de este tipo de redes.\n",
        "\n",
        "Adem치s, usaremos modelos pre-entrenados y con estos obtendremos embeddings para clasificar las im치genes. Para esto, usaremos la clase `Model` de Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq-bgFf-WLOq"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN71Ul1JWLOx"
      },
      "source": [
        "Verifiquemos que el entorno de ejecuci칩n en Colab sea GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln6imMSY8vLe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('GPU presente en: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EjrtHWCWLOv"
      },
      "source": [
        "# [El dataset Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG3DcGuhdyaT"
      },
      "source": [
        "El conjunto de datos de Dogs vs Cats fue publicado por Kaggle como parte de una competencia de visi칩n computacional a fines de 2013, cuando las CNNs no eran muy comunes.\n",
        "\n",
        "Se puede descargar el dataset original en: https://www.kaggle.com/c/dogs-vs-cats/data.\n",
        "\n",
        "Esta notebook se puede usar con dos conjuntos de datos:\n",
        "\n",
        "* Usaremos el conjunto de datos original de entrenamiento, dado que contiene las etiquetas de las clases. Este conjunto contiene 25,000 im치genes de perros y gatos (12,500 de cada clase) y tiene un tama침o de 543 MB. Ya se encuentra dividido en *train*, *validation* y *test*. [Download](https://drive.google.com/file/d/1Q3xOfn2Up9uIOLviS66oYH_oFFK-IGpW/view?usp=sharing)\n",
        "\n",
        "* Usaremos un conjunto reducido de datos, el cual contiene 1000 im치genes de cada clase para entrenamiento, 500 para validaci칩n y 500 para prueba. Todos los datos se sacaron del conjunto de entrenamiento original. [Download](https://drive.google.com/file/d/1Ce3u8dwYYriLkz5OpcGn72xIQENIHZX5/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C4obFfa2rjL"
      },
      "source": [
        "Copiaremos el dataset desde un v칤nculo de Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsjdNtihfL8n"
      },
      "outputs": [],
      "source": [
        "!pip install -qq gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC-8WGEgidmb"
      },
      "source": [
        "Descargamos el dataset desde Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LDaaZ3udhLI"
      },
      "outputs": [],
      "source": [
        "# ----- Versi칩n completa -----\n",
        "# !gdown --id 1Q3xOfn2Up9uIOLviS66oYH_oFFK-IGpW\n",
        "\n",
        "# ----- Copia de la versi칩n completa -----\n",
        "# !gdown 1hchhNQ_3WNncaXVD3kX58EIppcYFt-E2\n",
        "\n",
        "# ----- Versi칩n reducida -----\n",
        "!gdown 1Ce3u8dwYYriLkz5OpcGn72xIQENIHZX5\n",
        "\n",
        "# ----- Copia de la versi칩n reducida -----\n",
        "# !gdown 1NK9LvrVwsEQM0UHkFHq_GYCF2fjGrwAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXbCkAPa2w66"
      },
      "source": [
        "Descomprimimos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G3WswyR8vho"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# file_name = '/content/cnn_perros_gatos.zip'\n",
        "# file_name = '/content/cnn_perros_gatos-copia.zip'\n",
        "file_name = '/content/cnn_perros_gatos-small.zip'\n",
        "# file_name = '/content/cnn_perros_gatos-small-copia.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as myzip:\n",
        "    myzip.extractall()\n",
        "    print('Listo')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "游댯 Exploremos la ruta de archivos del dataset"
      ],
      "metadata": {
        "id": "WZrlRcMpqa7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el balance de clases"
      ],
      "metadata": {
        "id": "FmPCSI8xqZbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path = '/content/cnn_perros_gatos/train'\n",
        "\n",
        "num_train_dogs = len(os.listdir(path + '/dogs'))\n",
        "num_train_cats = len(os.listdir(path + '/cats'))\n",
        "\n",
        "print(f'N칰mero de im치genes de entrenamiento de perros: {num_train_dogs}')\n",
        "print(f'N칰mero de im치genes de entrenamiento de gatos: {num_train_cats}')\n",
        "\n",
        "ratio = num_train_dogs / (num_train_dogs + num_train_cats)\n",
        "\n",
        "plt.figure()\n",
        "plt.suptitle(f'N칰mero de im치genes por clase\\nRatio:{round(ratio,3)}')\n",
        "plt.bar(['Perros', 'Gatos'], [num_train_dogs, num_train_cats])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DckIM6u_vgb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFqw4JUe-P2O"
      },
      "source": [
        "Exploramos las carpetas de entrenamiento, validaci칩n y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMypQXip8vzN"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "print('Para entrenamiento:')\n",
        "\n",
        "train_dogs = 'cnn_perros_gatos/train/dogs'\n",
        "print(f'\\t{len(os.listdir(train_dogs))} Perros.')\n",
        "train_cats = 'cnn_perros_gatos/train/cats'\n",
        "print(f'\\t{len(os.listdir(train_cats))} Gatos.')\n",
        "\n",
        "print('\\nPara validaci칩n:')\n",
        "validation_dogs = 'cnn_perros_gatos/validation/dogs'\n",
        "print(f'\\t{len(os.listdir(validation_dogs))} Perros.')\n",
        "validation_cats = 'cnn_perros_gatos/validation/cats'\n",
        "print(f'\\t{len(os.listdir(validation_cats))} Gatos.')\n",
        "\n",
        "print('\\nPara prueba:')\n",
        "test_dogs = 'cnn_perros_gatos/test/dogs'\n",
        "print(f'\\t{len(os.listdir(test_dogs))} Perros.')\n",
        "test_cats = 'cnn_perros_gatos/test/cats'\n",
        "print(f'\\t{len(os.listdir(test_cats))} Gatos.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos algunas im치genes del dataset, como podemos ver:\n",
        "\n",
        "* Son archivos jpeg\n",
        "* Tienen diferentes tama침os"
      ],
      "metadata": {
        "id": "TtPRJtxIhLJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq ipyplot"
      ],
      "metadata": {
        "id": "0EVbHiCae8nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import ipyplot\n",
        "import random, os\n",
        "import numpy as np\n",
        "\n",
        "path_1 = '/content/cnn_perros_gatos/train/cats'\n",
        "path_2 = '/content/cnn_perros_gatos/train/dogs'\n",
        "\n",
        "filenames_1 = np.random.choice(os.listdir(path_1), 5, replace=False)\n",
        "filenames_2 = np.random.choice(os.listdir(path_2), 5, replace=False)\n",
        "# random.sample(os.listdir(path_1), 5)\n",
        "# filenames_2 = random.sample(os.listdir(path_2), 5)\n",
        "\n",
        "full_filenames_1 = [os.path.join(path_1, fname) for fname in filenames_1]\n",
        "full_filenames_2 = [os.path.join(path_2, fname) for fname in filenames_2]\n",
        "\n",
        "filenames = full_filenames_1 + full_filenames_2\n",
        "images_list = [Image.open(fname) for fname in filenames]\n",
        "\n",
        "ipyplot.plot_images(images_list,show_url=False)"
      ],
      "metadata": {
        "id": "6WY6JsL3ecRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las dimensiones de las imagenes"
      ],
      "metadata": {
        "id": "DPRHSPxi7PjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from seaborn import heatmap\n",
        "import numpy as np\n",
        "\n",
        "path = '/content/cnn_perros_gatos/train'\n",
        "\n",
        "widths = []\n",
        "heights = []\n",
        "\n",
        "for folder in os.listdir(path):\n",
        "    folder_path = os.path.join(path, folder)\n",
        "    for image in os.listdir(folder_path):\n",
        "        image_path = os.path.join(folder_path, image)\n",
        "        img = Image.open(image_path)\n",
        "        widths.append(img.width)\n",
        "        heights.append(img.height)\n",
        "\n",
        "min_width = min(widths)\n",
        "max_width = max(widths)\n",
        "min_height = min(heights)\n",
        "max_height = max(heights)\n",
        "\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle('Dimensiones de las im치genes')\n",
        "plt.hist(widths, bins=20, alpha=0.5, label='Width')\n",
        "plt.hist(heights, bins=20, alpha=0.5, label='Height')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist2d(widths, heights, bins=(20, 20), cmap='Blues')\n",
        "plt.xticks(range(min_width, max_width+1, 100))\n",
        "plt.yticks(range(min_height, max_height+1, 100))\n",
        "plt.colorbar()\n",
        "plt.xlabel('Width')\n",
        "plt.ylabel('Height')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NLyOc0fq7UDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "游댯 쯈u칠 retos presentar칤a este dataset para una MLP como las que hemos definido y usado?"
      ],
      "metadata": {
        "id": "fpHSQQQa3Jzo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4aC0_mXDbXn"
      },
      "source": [
        "Definimos los directorios de entrenamiento, validaci칩n y prueba para usaralos en el resto de la notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJGFJS-qNLJc"
      },
      "outputs": [],
      "source": [
        "train_dir = 'cnn_perros_gatos/train'\n",
        "validation_dir = 'cnn_perros_gatos/validation'\n",
        "test_dir = 'cnn_perros_gatos/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adem치s, probaremos con dos im치genes externas al dataset"
      ],
      "metadata": {
        "id": "dKDa0X5j2Gp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1eSWPCWL-mc4ekrjbh5BN25IKlNZfAECF\n",
        "!gdown 1OEUZgYKM_brFUwjNi1RmMCOLzctdimYK"
      ],
      "metadata": {
        "id": "e1KRZqps2GQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Zxc_HG1MuG"
      },
      "source": [
        "# Preprocesamiento de datos\n",
        "\n",
        "\n",
        "Los datos deben formatearse en tensores de punto flotante preprocesados adecuadamente antes de que se introduzcan en la red. En este momento, nuestros datos se encuentran almacenados como archivos JPEG, por lo que los pasos para que puedan ser introducidos en nuestra red son:\n",
        "\n",
        "* Leer los archivos de imagen.\n",
        "\n",
        "* Decodificar el contenido JPEG a cuadr칤culas de p칤xeles RBG.\n",
        "\n",
        "* Convertirlos en tensores de punto flotante.\n",
        "\n",
        "* Volver a escalar los valores de p칤xeles (entre 0 y 255) al intervalo $[0, 1]$ (las redes neuronales prefieren tratar con valores de entrada peque침os).\n",
        "\n",
        "Afortunadamente, Keras tiene herramientas para encargarse de estos pasos autom치ticamente. Keras tiene un m칩dulo con herramientas de ayuda para procesamiento de im치genes, ubicado en **keras.preprocessing.image**. En particular, contiene la clase **ImageDataGenerator**, que permite configurar r치pidamente los generadores de Python que pueden convertir autom치ticamente los archivos de imagen en disco en *batches* de tensores preprocesados."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una funci칩n para obtener los generadores de entranamiento, validaci칩n y prueba especificando las rutas de las carpetas y el tama침o de imagen"
      ],
      "metadata": {
        "id": "vGtIhJBQ5MFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LAZtTFHvkvZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\n",
        "from tensorflow.keras.applications.resnet import preprocess_input as resnet_preprocess\n",
        "\n",
        "def get_generators(train_dir,\n",
        "                  validation_dir,\n",
        "                  test_dir,\n",
        "                  img_size,\n",
        "                  preprocessing_mode='rescaling',\n",
        "                  model_type=None,\n",
        "                  augmentation_params=None):\n",
        "    \"\"\"\n",
        "    Crea generadores de datos para entrenamiento, validaci칩n y test.\n",
        "\n",
        "    Args:\n",
        "        train_dir (str): Directorio de entrenamiento\n",
        "        validation_dir (str): Directorio de validaci칩n\n",
        "        test_dir (str): Directorio de test\n",
        "        img_size (tuple): Tama침o de las im치genes (height, width)\n",
        "        preprocessing_mode (str): Tipo de preprocesamiento ('rescaling', 'model_specific', 'augmenting')\n",
        "        model_type (str): Tipo de modelo para preprocesamiento espec칤fico ('efficientnet', 'resnet', etc.)\n",
        "        augmentation_params (dict): Par치metros de aumento de datos personalizados\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_generator, validation_generator, test_generator)\n",
        "    \"\"\"\n",
        "    # Configuraci칩n base para test y validaci칩n (siempre sin aumento de datos)\n",
        "    test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Configuraci칩n para entrenamiento\n",
        "    if preprocessing_mode == 'rescaling':\n",
        "        train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    elif preprocessing_mode == 'model_specific':\n",
        "        if model_type == 'efficientnet':\n",
        "            train_datagen = ImageDataGenerator(preprocessing_function=effnet_preprocess)\n",
        "            test_val_datagen = ImageDataGenerator(preprocessing_function=effnet_preprocess)\n",
        "        elif model_type == 'resnet':\n",
        "            train_datagen = ImageDataGenerator(preprocessing_function=resnet_preprocess)\n",
        "            test_val_datagen = ImageDataGenerator(preprocessing_function=resnet_preprocess)\n",
        "    elif preprocessing_mode == 'augmenting':\n",
        "        if augmentation_params:\n",
        "            # Usar par치metros personalizados si se proporcionan\n",
        "            train_datagen = ImageDataGenerator(**augmentation_params)\n",
        "        else:\n",
        "            # Configuraci칩n por defecto para aumento de datos\n",
        "            train_datagen = ImageDataGenerator(\n",
        "                rescale=1./255,\n",
        "                rotation_range=40,\n",
        "                width_shift_range=0.2,\n",
        "                height_shift_range=0.2,\n",
        "                shear_range=0.2,\n",
        "                zoom_range=0.2,\n",
        "                horizontal_flip=True,\n",
        "                fill_mode='nearest'\n",
        "            )\n",
        "\n",
        "    # Crear generadores\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    validation_generator = test_val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_generator = test_val_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator, test_generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (16,16)\n",
        "\n",
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size,\n",
        "                                                                       preprocessing_mode='rescaling',\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "hCg7fCe-FpB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_labels = train_generator.classes\n",
        "\n",
        "np.unique(train_labels,return_counts=True)"
      ],
      "metadata": {
        "id": "L72uBeyNquNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_generator.class_indices)\n",
        "print(validation_generator.class_indices)\n",
        "print(test_generator.class_indices)"
      ],
      "metadata": {
        "id": "-IuMG8LjraNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHKu7G_y4fPC"
      },
      "source": [
        "\n",
        "* Vamos a revisar la salida de uno de estos generadores: produce batches de im치genes de 150 x 150 RGB (con la forma (20, 150, 150, 3)) y etiquetas binarias (con la forma (20,)). 20 es el n칰mero de muestras en cada batch (el tama침o del batch).\n",
        "* Como el generador genera estos batches de forma indefinida (i.e. recorre sin fin las im치genes presentes en la carpeta que se le indic칩), se necesita romper el loop de iteraci칩n en alg칰n punto. A continuaci칩n podemos ver c칩mo es cada corrida (batch/step) que proporciona el generador."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = next(train_generator)\n",
        "print(\"Labels in batch:\", y_train)\n",
        "print(\"Shape:\",x_train.shape)\n",
        "print(\"Number of class 0:\", sum(y_train==0))\n",
        "print(\"Number of class 1:\", sum(y_train==1))\n",
        "\n",
        "x_test, y_test = next(test_generator)\n",
        "print(\"Labels in batch:\", y_test)\n",
        "print(\"Shape:\",x_test.shape)\n",
        "print(\"Number of class 0:\", sum(y_test==0))\n",
        "print(\"Number of class 1:\", sum(y_test==1))\n",
        "\n",
        "x_val, y_val = next(validation_generator)\n",
        "print(\"Labels in batch:\", y_val)\n",
        "print(\"Shape:\",x_val.shape)\n",
        "print(\"Number of class 0:\", sum(y_val==0))\n",
        "print(\"Number of class 1:\", sum(y_val==1))"
      ],
      "metadata": {
        "id": "ilu7aAs_8gmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LevQH8ih4ek_"
      },
      "source": [
        "* Vamos a proceder a entrenar nuestro modelo con los datos usando el generador. Debido a que los datos se generan infinitamente, el generador necesita saber cu치ntas muestras extraer antes de declarar una 칠poca finalizada. Esta es la funci칩n del argumento **steps_per_epoch**\n",
        "\n",
        "* En este caso, **steps_per_epoch** corresponde al n칰mero de batches que requiere el generador para leer el conjunto de datos completo. S칩lo despu칠s de haber solicitado este n칰mero de batches, el proceso de ajuste de nuestro modelo pasar치 a la siguiente 칠poca. **steps_per_epoch** corresponde a el n칰mero de pasos de descenso del gradiente. En nuestro caso, cada batch tiene un tama침o de 20 muestras, por lo que tomar치 100 pasos (batches) hasta que cubramos las 2,000 muestras de nuestra base de datos.\n",
        "\n",
        "* Como siempre, uno puede pasar un argumento llamado **validation_data**. Es importante destacar que este argumento puede ser un generador de datos en s칤 mismo, pero tambi칠n podr칤a ser una tupla de arreglos Numpy. Si se pasa un generador como **validation_data**, entonces se espera que este generador produzca batches de datos de validaci칩n sin fin, y por lo tanto tambi칠n se debe especificar el argumento **validation_steps**, que le dice al proceso cu치ntos batches debe extraer del generador de validaci칩n para su evaluaci칩n."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definamos funciones para graficar las curvas de entrenamiento y mostrar el rendimiento en el conjunto de prueba"
      ],
      "metadata": {
        "id": "1tegpJcW27f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "from seaborn import heatmap\n",
        "\n",
        "def plot_training_curves(history):\n",
        "    plt.figure(figsize=(11,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.title(\"Validation and Training Loss\",fontsize=14)\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='validation')\n",
        "    plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.title(\"Validation and Training Accuracy\",fontsize=14)\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='validation')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate(model,X,y,classes_names):\n",
        "    if len(classes_names) > 2:\n",
        "        y_pred_proba = model.predict(X)\n",
        "        y_pred = np.argmax(y_pred_proba,axis=1)\n",
        "    elif len(classes_names) == 2:\n",
        "        y_pred_proba = model.predict(X)\n",
        "        y_pred = np.where(y_pred_proba > 0.5, 1, 0).reshape(-1)\n",
        "    print(f\"Accuracy: {accuracy_score(y,y_pred)}\")\n",
        "    print(f\"F1 Score: {f1_score(y,y_pred,average='macro')}\")\n",
        "    cm = confusion_matrix(y_pred=y_pred,y_true=y)\n",
        "    plt.figure()\n",
        "    heatmap(cm,\n",
        "            fmt='g',\n",
        "            annot=True,\n",
        "            xticklabels=classes_names,\n",
        "            yticklabels=classes_names,\n",
        "            cmap='Blues'\n",
        "            )\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0gGtLZcL26Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 0: MLP"
      ],
      "metadata": {
        "id": "R4qSm78z1VEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size=(16,16),\n",
        "                                                                       preprocessing_mode='rescaling',\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "DYxeOl5p6W1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Input, Dropout\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "input_shape = (16,16,3)\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=input_shape),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               patience=5,\n",
        "                               restore_best_weights=True\n",
        "                               )\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.2,\n",
        "                              patience=4,\n",
        "                              min_lr=1e-5)"
      ],
      "metadata": {
        "id": "CpZDYny91YDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps = train_generator.samples // train_generator.batch_size\n",
        "val_steps = validation_generator.samples // validation_generator.batch_size\n",
        "\n",
        "print(f\"N칰mero de pasos de entrenamiento: {train_steps}\")\n",
        "print(f\"N칰mero de pasos de validaci칩n: {val_steps}\")\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=train_steps,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=val_steps,\n",
        "      callbacks=[early_stopping, reduce_lr]\n",
        "      )"
      ],
      "metadata": {
        "id": "pHdBGYgZ3UL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(history)"
      ],
      "metadata": {
        "id": "T35UQVGV2fnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "classes_names = list(train_generator.class_indices.keys())\n",
        "evaluate(model,test_generator,test_generator.classes,classes_names)"
      ],
      "metadata": {
        "id": "MYGXyIeA2jFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "3ZZWrQ5S6tRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phDgmrWsWLO6"
      },
      "source": [
        "# Modelo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZFIorONDhG9"
      },
      "source": [
        "Definamos ahora un modelo con arquitectura CNN. Usaremos las capas [`Conv2D`](https://keras.io/api/layers/convolution_layers/convolution2d/) para las operaciones de convoluci칩n y [`MaxPooling2D`](https://keras.io/api/layers/pooling_layers/max_pooling2d/) para el pooling.\n",
        "\n",
        "Pero antes, obtengamos los generadores de im치genes con un tama침o mayor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (150,150)\n",
        "\n",
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size, # Aumentamos el tama침o de im치genes\n",
        "                                                                       preprocessing_mode='rescaling', # S칩lo re-escalamos\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None # Sin aumento de datos\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "dkx_J9nLFwfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1bzFBdsAq40yN95k2pA5X2OGsXf-40v0t\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "\n",
        "**Importante**: Observa la elecci칩n de los hiperpar치metros `padding=\"same\"` y `strides=1`. Esta elecci칩n asegura que las salidas de cada capa convolucional tenga las mismas dimensiones que las entradas."
      ],
      "metadata": {
        "id": "DoKzp6cRF2t4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yySANB-NNr4w"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Input\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(150, 150, 3)),\n",
        "    Conv2D(32, 3, activation='relu',\n",
        "                           input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, 3, activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, 3, activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSL8Y3n7rLtL"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbnp2bbL9jes"
      },
      "source": [
        "* NOTA que comenzamos con im치genes de tama침o 150 x 150 (una elecci칩n de tama침o arbitraria) y terminamos con mapas de caracter칤sticas de tama침o 7 x 7 justo antes de la capa de *flatten*.\n",
        "* En realidad las im치genes de entrada tienen tama침os diversos (desconocidos), pero afortunadamente Keras nos puede ayudar a pre-procesarlas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA43G8SqEv2f"
      },
      "source": [
        "Compilamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=1e-4)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vrU-K9eKLcDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "505HhEEzMn8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tarda alrededor de 3 minutos\n",
        "\n"
      ],
      "metadata": {
        "id": "blAr9k68wLoN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdjhzcgzwQ-T"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOeKcGvvWLPB"
      },
      "source": [
        "## Rendimiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtq39jKpFV2g"
      },
      "source": [
        "Guardamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFnMW2EbyyqD"
      },
      "outputs": [],
      "source": [
        "model.save('cnn_perros_gatos_model_1.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFYSGmOGZsF"
      },
      "source": [
        "Grafiquemos las curvas de aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt6kRFIYzjPb"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting**... 춰Tenemos muy pocos ejemplos!"
      ],
      "metadata": {
        "id": "_GnVvFeiJVsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "VmNuNQb-IC6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos el desempe침o del modelo a detalle"
      ],
      "metadata": {
        "id": "kM0_LAVp3MnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "classes_names = list(train_generator.class_indices.keys())\n",
        "evaluate(model,test_generator,\n",
        "         test_generator.classes,\n",
        "         classes_names)"
      ],
      "metadata": {
        "id": "-VgCUZcg3MLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 2: Aumento de datos"
      ],
      "metadata": {
        "id": "y4GxjIoaMv_t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXzBZmaj_V2_"
      },
      "source": [
        "## Aumento de Datos\n",
        "\n",
        "* El efecto de sobreajuste ocurre cuando se tienen muy pocas muestras de las que aprender, lo que nos impide entrenar un modelo capaz de generalizar a nuevos datos. Si tuviesemos datos infinitos, nuestro modelo estar칤a expuesto a todos los aspectos posibles de la distribuci칩n de datos en cuesti칩n y nunca se sobreajustar칤a nuestro modelo.\n",
        "\n",
        "* El aumento de datos adopta el enfoque de generar m치s datos de entrenamiento a partir de muestras de entrenamiento existentes, al \"aumentar\" las muestras a trav칠s de una serie de transformaciones aleatorias que producen im치genes de apariencia cre칤ble. El objetivo es que durante el tiempo de entrenamiento, nuestro modelo nunca vea exactamente la misma imagen dos veces. Esto ayuda a que el modelo se exponga a m치s aspectos de los datos y generalice mejor.\n",
        "\n",
        "* En Keras, esto se puede hacer configurando una serie de transformaciones aleatorias que se realizar치n en las im치genes le칤das por nuestra instancia de ImageDataGenerator.\n",
        "\n",
        "* Vamos a comenzar por aumentar una imagen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_qyda81Q81"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U48tZWbBB2TM"
      },
      "source": [
        "Las opciones anteriores son solo algunas de las opciones disponibles.\n",
        "\n",
        "* **rotation_range** es un valor en grados (0-180), un rango dentro del cual girar las im치genes de forma aleatoria.\n",
        "\n",
        "* **width_shift** y **height_shift** son rangos expresados como una fracci칩n del ancho o altura total de la imagen, dentro de los cuales se pueden trasladar vertical u horizontalmente de forma aleatoria a las im치genes.\n",
        "\n",
        "* **shear_range** aplica aleatoriamente transformaciones de corte.\n",
        "\n",
        "* **zoom_range** aplica acercamientos aleatorios dentro de las im치genes.\n",
        "\n",
        "* **horizontal_flip** Voltea de forma aleatoria la mitad en las im치genes horizontalmente.\n",
        "\n",
        "* **fill_mode** es la estrategia utilizada para rellenar p칤xeles creados, que pueden aparecer despu칠s de una rotaci칩n o un cambio de ancho / altura."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos 25 im치genes modificadas a partir de una misma imagen"
      ],
      "metadata": {
        "id": "XgJ_5x2rtyM4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbyqp-q61cbr"
      },
      "outputs": [],
      "source": [
        "from keras import utils\n",
        "import random\n",
        "\n",
        "train_cats_dir = '/content/cnn_perros_gatos/train/cats' # El directorio donde est치n las im치genes de gatos de entrenamiento\n",
        "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
        "img_path = random.sample(fnames, 1)[0] # Escogemos una imagen al azar para aplicar el \"aumentado de datos\"\n",
        "img = utils.load_img(img_path, target_size=(150, 150)) # Leemos la imagen y la redimensionamos.\n",
        "x = utils.img_to_array(img) # Leemos la imagen y la redimensionamos.\n",
        "x = x.reshape((1,) + x.shape) # Redimensionamos el arreglo a (1, 150, 150, 3)\n",
        "\n",
        "'''\n",
        "El comando .flow () genera batches de im치genes transformadas aleatoriamente\n",
        "Con el \"for\" de abajo estaremos en un loop indefinidamente,\n",
        "Necesitamos 'romper' el loop en alg칰n momento\n",
        "'''\n",
        "\n",
        "fig, axs = plt.subplots(5,5,figsize=(10,10))\n",
        "k = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    i = k//5\n",
        "    j = k%5\n",
        "    axs[i,j].imshow(utils.array_to_img(batch[0]))\n",
        "    axs[i,j].axis('off')\n",
        "    k += 1\n",
        "    if k == 25:\n",
        "        break\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teFUV-JSFKZH"
      },
      "source": [
        "* Si entrenamos una nueva red neuronal utilizando esta configuraci칩n de aumento de datos, nuestra red nunca ver치 dos veces la misma entrada, pues a cada nueva imagen se le aplica transformaciones aleatorias dentro de ciertos rangos.\n",
        "\n",
        "* Sin embargo, las entradas que ves est치n a칰n muy interrelacionadas, ya que provienen de un peque침o n칰mero de im치genes originales: **no podemos producir nueva informaci칩n, s칩lo podemos mezclar la informaci칩n existente**.\n",
        "\n",
        "* Dado que esto podr칤a no ser suficiente para librarnos del sobreajuste. Para mitigarlo a칰n m치s, tambi칠n agregaremos una capa de Dropout a nuestro modelo, justo antes de la etapa del clasificador densamente conectado (fully-connected)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los nuevos generadores de im치genes"
      ],
      "metadata": {
        "id": "YO3rm6nGoyRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size=(150,150),\n",
        "                                                                       preprocessing_mode='augmenting',\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None)"
      ],
      "metadata": {
        "id": "isCG_ejdoyAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ5RY_qYGlln"
      },
      "source": [
        "Definimos la misma red CNN, con dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYgaCOd2R4HU"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Input\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(150, 150, 3)),\n",
        "    Conv2D(32, 3, activation='relu', name='Convolution1'),\n",
        "    MaxPooling2D(name='MaxPooling1'),\n",
        "    Conv2D(64, 3, activation='relu',name='Convolution2'),\n",
        "    MaxPooling2D(name='MaxPooling2'),\n",
        "    Conv2D(128, 3, activation='relu',name='Convolution3'),\n",
        "    MaxPooling2D(name='MaxPooling3'),\n",
        "    Conv2D(128, 3, activation='relu',name='Convolution4'),\n",
        "    MaxPooling2D(name='MaxPooling4'),\n",
        "    Flatten(name='Flatten'),\n",
        "    Dropout(0.1,name='DropOut'), # 0.5\n",
        "    Dense(512, activation='relu',name='Densa'),\n",
        "    Dense(1, activation='sigmoid',name='Salida')\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop\n",
        "\n",
        "opt = RMSprop(learning_rate=1e-4)"
      ],
      "metadata": {
        "id": "5hjNpXpZevmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6bwra-5TfZ3"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "-BL7MuurM3p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenemos el modelo.\n",
        "\n",
        "**Observaci칩n**: Para mejores resultados entrenar durante 100 칠pocas (tarda alrededor de 30 minutos). Por cuestiones de tiempo, entrenamos con 30 칠pocas (tarda alrededor de 8 minutos). Con ambos obtendremos un poco m치s de 80% de accuracy."
      ],
      "metadata": {
        "id": "fRqn2i_1wUZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "                train_generator,\n",
        "                steps_per_epoch=100,\n",
        "                epochs=30,\n",
        "                validation_data=validation_generator,\n",
        "                validation_steps=50,\n",
        "                verbose=1)"
      ],
      "metadata": {
        "id": "idfaLKGcwUDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podr칤amos guardar el modelo"
      ],
      "metadata": {
        "id": "tq2kAHx0px-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5VV-0J0UTGN"
      },
      "outputs": [],
      "source": [
        "model.save('cnn_perros_gatos_model2_30_epochs.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CHs_DRqGwKk"
      },
      "source": [
        "Veamos las curvas de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pYHbXOlUXPx"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos el desempe침o"
      ],
      "metadata": {
        "id": "wR66A_ZY2wrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "L4A9vsXhIAh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "evaluate(model,test_generator,\n",
        "         test_generator.classes,\n",
        "         classes_names)"
      ],
      "metadata": {
        "id": "slKSl9tP3hWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_-OWUThLg-f"
      },
      "source": [
        "## 쯈u칠 pasa si entrenamos con m치s 칠pocas y todo el dataset?\n",
        "\n",
        "A continuaci칩n se muestras las gr치ficas de entrenamiento y la matriz de confusi칩n con un modelo m치s grande, entrenado durante cerca de 50 칠pocas, usando todo el conjunto de entrenamiento completo.\n",
        "\n",
        "El accuracy en el conjunto de prueba fue de 87%.\n",
        "\n",
        "Podemos descargar este modelo ya entrenado de Google Drive\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1l4zeHmnDvsxhrHlMH0xgzT9BaYfa2oMs\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1eNMacyz1KA0WAwJW3fn1ktfm5_MrpJem\" width=\"600\" />\n",
        "</p>"
      ],
      "metadata": {
        "id": "2iwh6EJPmK06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 13L5oCFXIgw22FR8irsuwNHvnzekA7bGd"
      ],
      "metadata": {
        "id": "gjHZ07i3IQsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfXwuYwPda0Q"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('/content/cnn_perros_gatos_improved.keras')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'cnn_perros_gatos/train'\n",
        "validation_dir = 'cnn_perros_gatos/validation'\n",
        "test_dir = 'cnn_perros_gatos/test'\n",
        "\n",
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size=(150,150),\n",
        "                                                                       preprocessing_mode='rescaling',\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None)"
      ],
      "metadata": {
        "id": "XmQievMvW5C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "classes_names = list(train_generator.class_indices.keys())\n",
        "evaluate(model,test_generator,\n",
        "         test_generator.classes,\n",
        "         classes_names)"
      ],
      "metadata": {
        "id": "41XwoYZhX-fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "T7_ZAXDLZHhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "游댯 쯈u칠 im치genes son las que est치 confundiendo el modelo?"
      ],
      "metadata": {
        "id": "2TeMs-ihBlfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "y_pred_proba = model.predict(test_generator)\n",
        "y_pred = np.where(y_pred_proba > 0.5, 1, 0).reshape(-1)\n",
        "\n",
        "false_positives_idxs = np.where((y_pred == 1) & (test_generator.classes == 0))[0]\n",
        "false_negatives_idxs = np.where((y_pred == 0) & (test_generator.classes == 1))[0]\n",
        "\n",
        "print(f\"False positives: {false_positives_idxs.shape[0]}\")\n",
        "print(f\"False negatives: {false_negatives_idxs.shape[0]}\")"
      ],
      "metadata": {
        "id": "AQowWN3PBqrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Diccionario con el nombre de cada clase\n",
        "test_generator.class_indices\n",
        "idxs_to_classes = {v:k for k,v in test_generator.class_indices.items()}\n",
        "\n",
        "# Escogemos un ejemplo de falso positivo y uno de falso negativo\n",
        "fp_idx = np.random.choice(false_positives_idxs,size=1)\n",
        "fn_idx = np.random.choice(false_negatives_idxs,size=1)\n",
        "\n",
        "# Obtenemos los nombres de archivo de esos 칤ndices\n",
        "test_generator.reset()\n",
        "fp_filename = test_generator.filenames[fp_idx[0]]\n",
        "fn_filename = test_generator.filenames[fn_idx[0]]\n",
        "\n",
        "# Leemos las im치genes\n",
        "fp_img = imageio.v2.imread(os.path.join(test_dir,fp_filename))\n",
        "fn_img = imageio.v2.imread(os.path.join(test_dir,fn_filename))\n",
        "\n",
        "# Mostramos las im치genes\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
        "axs[0].imshow(fp_img)\n",
        "axs[0].set_title(f'Prediction: {idxs_to_classes[y_pred[fp_idx][0]]}')\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(fn_img)\n",
        "axs[1].set_title(f'Prediction: {idxs_to_classes[y_pred[fn_idx][0]]}')\n",
        "axs[1].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hRQx0iY7CbZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imageio.v2 import imread\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "dog_path = 'dc_dog.jpg'\n",
        "cat_path = 'dc_cat.jpg'\n",
        "\n",
        "dog_img = imread(dog_path)\n",
        "cat_img = imread(cat_path)\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(dog_img)\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(cat_img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UPE-d98S2eGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Funci칩n para cargar y preprocesar una imagen\n",
        "def load_and_preprocess_image(img_path, target_size=(150, 150)):\n",
        "    # Cargar imagen y redimensionar a 150x150\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    # Convertir a array numpy\n",
        "    img_array = image.img_to_array(img)\n",
        "    # Normalizar valores de p칤xeles al rango [0,1]\n",
        "    img_array = img_array / 255.0\n",
        "    return img_array"
      ],
      "metadata": {
        "id": "c0yldXWh3v7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_img = load_and_preprocess_image(dog_path)\n",
        "cat_img = load_and_preprocess_image(cat_path)\n",
        "\n",
        "# Crear el tensor combinando ambas im치genes\n",
        "images_tensor = np.stack([dog_img, cat_img], axis=0)\n",
        "\n",
        "print(images_tensor.shape)\n",
        "\n",
        "preds = model.predict(images_tensor)\n",
        "print(f\"Salida de la red:\\t{preds.reshape(-1,)}\\n\")\n",
        "\n",
        "predictions = np.where(preds > 0.5, 1, 0).reshape(-1)\n",
        "\n",
        "idxs_to_classes = {v:k for k,v in test_generator.class_indices.items()}\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(dog_img)\n",
        "plt.axis('off')\n",
        "plt.title(f'Prediction: {idxs_to_classes[predictions[0]]}')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(cat_img)\n",
        "plt.axis('off')\n",
        "plt.title(f'Prediction: {idxs_to_classes[predictions[1]]}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JKcE6e8x3HsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 丘 Usando la red para obtener features de las im치genes: Embeddings\n",
        "\n",
        "En esta parte de la notebook ilustraremos c칩mo la parte convolucional de las redes CNN se puede ver como un m칠todo de extracci칩n de features. Es decir, podemos ver al bloque convolucional como un m칠todo que convierte cada imagen en un vector, de una *buena* manera.\n",
        "\n",
        "Para esto, usaremos el modelo CNN pre-entrenado con el dataset complejo."
      ],
      "metadata": {
        "id": "yOz_Ei4QRBUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 13L5oCFXIgw22FR8irsuwNHvnzekA7bGd"
      ],
      "metadata": {
        "id": "8CKova_FRJSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('/content/cnn_perros_gatos_improved.keras')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-NSN4D3aRX2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos acceder a las distintas capas"
      ],
      "metadata": {
        "id": "N7HFEo2vMbXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.layers)"
      ],
      "metadata": {
        "id": "lAxKi6niMIl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, definimos un modelo de Keras que ser치 el mismo modelo pre-entrenado pero sin la capa de salida.\n",
        "\n",
        "Para esto, usamos la clase `Model` de Keras.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1lbLQ7D1_QElq_iTscpQ_rjXPWJ2uPke3\" width=\"600\" />\n",
        "</p>"
      ],
      "metadata": {
        "id": "eQ7102MiJafN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "\n",
        "first_layer = model.layers[0] # Capa de entrada del modelo original\n",
        "\n",
        "features_layer = model.layers[20] # Si quieremos la salida justo antes de la capa de salida\n",
        "# features_layer = model.layers[15]  # Si queremos la salida de la parte convolucional\n",
        "\n",
        "# Creamos el modelo especificando la(s) entrada(s) y salida(s)\n",
        "features_model = Model(inputs=first_layer.input, outputs=features_layer.output)"
      ],
      "metadata": {
        "id": "5xI21kaGMek0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los generadores"
      ],
      "metadata": {
        "id": "R69zghhQYlEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, validation_generator, test_generator = get_generators(train_dir,\n",
        "                                                                       validation_dir,\n",
        "                                                                       test_dir,\n",
        "                                                                       img_size=(150,150),\n",
        "                                                                       preprocessing_mode='rescaling',\n",
        "                                                                       model_type=None,\n",
        "                                                                       augmentation_params=None\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "XaEMbB2nYk0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como necesitamos obtener predicciones sobre el conjunto de prueba, definamos la siguiente funci칩n:"
      ],
      "metadata": {
        "id": "WLi-f-VqAuDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm  # Opcional: para barra de progreso\n",
        "\n",
        "def get_embeddings_with_shuffled_generator(model, generator):\n",
        "    \"\"\"\n",
        "    Obtiene embeddings y etiquetas REALES de un generador (incluso con shuffle=True).\n",
        "\n",
        "    Args:\n",
        "        model: Modelo de Keras que devuelve embeddings (sin capa softmax).\n",
        "        generator: Generador de im치genes (DirectoryIterator).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (y_true, embeddings) - etiquetas y embeddings alineados.\n",
        "    \"\"\"\n",
        "    generator.reset()  # Reinicia el generador\n",
        "    y_true = []\n",
        "    embeddings = []\n",
        "\n",
        "    # Iterar sobre TODOS los batches del generador\n",
        "    for _ in tqdm(range(len(generator))):\n",
        "        x_batch, y_batch = next(generator)\n",
        "        y_true.extend(y_batch)\n",
        "        batch_embeddings = model.predict(x_batch, verbose=0)\n",
        "        embeddings.extend(batch_embeddings)\n",
        "\n",
        "    # Convertir a numpy array\n",
        "    y_true = np.array(y_true)\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    return y_true, embeddings"
      ],
      "metadata": {
        "id": "SCId_H7MAtTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos las features para el conjunto de entrenamiento y prueba, pasandolas por este nuevo modelo `features_model`"
      ],
      "metadata": {
        "id": "EwNy8xXsL_d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train, train_features = get_embeddings_with_shuffled_generator(features_model, train_generator)"
      ],
      "metadata": {
        "id": "ob3bOkl-CXv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "y_test = test_generator.classes\n",
        "\n",
        "test_generator.reset()\n",
        "test_features = features_model.predict(test_generator)\n",
        "print(test_features.shape)"
      ],
      "metadata": {
        "id": "FaG7JPTZR2Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de este punto, ya podemos usar estas features como features para cualquier m칠todo de Machine Learning (cl치sico o profundo)."
      ],
      "metadata": {
        "id": "ubaRPGwPMIdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "clfs = [SVC(),\n",
        "        DecisionTreeClassifier(max_depth=10),\n",
        "        RandomForestClassifier(n_estimators=50,max_depth=10),\n",
        "        KNeighborsClassifier()]\n",
        "names = [x.__class__.__name__ for x in clfs]\n",
        "\n",
        "for clf, name in zip(clfs, names):\n",
        "    clf = Pipeline([('scaler', StandardScaler()), (name, clf)])\n",
        "    clf.fit(train_features, train_generator.classes)\n",
        "    y_pred_train = clf.predict(train_features)\n",
        "    y_pred_test = clf.predict(test_features)\n",
        "    print(f'{name} - Train Accuracy: {accuracy_score(y_true=y_train, y_pred=y_pred_train)}')\n",
        "    print(f'{name} - Test Accuracy: {accuracy_score(y_true=y_test, y_pred=y_pred_test)}')\n"
      ],
      "metadata": {
        "id": "jCwfSUboSJrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "游댯 Tenemos mejores m칠tricas que con la red MLP. Esto nos dice que la parte convolucional de la red es un buen m칠todo para extraer features."
      ],
      "metadata": {
        "id": "4SaUjlhwIr_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "train_pca = pca.fit_transform(train_features)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(train_pca[:,0], train_pca[:,1], c=train_generator.classes)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OdUBiTTdWhqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "train_tsne = tsne.fit_transform(train_features)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(train_tsne[:,0], train_tsne[:,1], c=train_generator.classes)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PCilb7k-V-b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando un modelo pre-entrado especializado"
      ],
      "metadata": {
        "id": "Oqo_bBqCFZRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EfficientNetB0** es una red neuronal convolucional pre-entrenada en el conjunto de datos ImageNet, que puede clasificar im치genes en 1000 categor칤as de objetos. Es una de las variantes de la familia EfficientNet, conocida por su eficiencia en t칠rminos de par치metros y c치lculos, logrando un buen equilibrio entre precisi칩n y tama침o.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1y5dsxf3gUaaVaodEcwkuovw-zgrIDuXI\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "[Documentaci칩n](https://keras.io/api/applications/efficientnet/)"
      ],
      "metadata": {
        "id": "zWU6r3qUhV6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'cnn_perros_gatos/train'\n",
        "validation_dir = 'cnn_perros_gatos/validation'\n",
        "test_dir = 'cnn_perros_gatos/test'\n",
        "\n",
        "generators = get_generators(train_dir, validation_dir, test_dir,\n",
        "                            img_size=(224,224),\n",
        "                            preprocessing_mode='model_specific',\n",
        "                            model_type='efficientnet',\n",
        "                            augmentation_params=None\n",
        "                            )\n",
        "\n",
        "train_generator_en, validation_generator_en, test_generator_en = generators"
      ],
      "metadata": {
        "id": "itT-K_y6sxGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el modelo preentrenado (sin la capa superior 'softmax')\n",
        "model_en = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "y_train, train_features_EN = get_embeddings_with_shuffled_generator(model_en, train_generator_en)\n",
        "print(\"Train embedding shape:\", train_features_EN.shape)\n",
        "\n",
        "test_generator_en.reset()\n",
        "y_test = test_generator_en.classes\n",
        "test_features_EN = model_en.predict(test_generator_en)\n",
        "print(\"Test embedding shape:\", test_features_EN.shape)"
      ],
      "metadata": {
        "id": "Ny1CAbqzM6jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "clfs = [SVC(),\n",
        "        DecisionTreeClassifier(max_depth=5),\n",
        "        RandomForestClassifier(n_estimators=30,max_depth=5),\n",
        "        KNeighborsClassifier()]\n",
        "names = [x.__class__.__name__ for x in clfs]\n",
        "\n",
        "for clf, name in zip(clfs, names):\n",
        "    clf = Pipeline([('scaler', StandardScaler()),\n",
        "                    ('pca', PCA(n_components=100)),\n",
        "                    (name, clf)])\n",
        "    clf.fit(train_features_EN, y_train)\n",
        "    y_pred_train = clf.predict(train_features_EN)\n",
        "    y_pred_test = clf.predict(test_features_EN)\n",
        "    print(f'{name} - Train Accuracy: {accuracy_score(y_true=y_train, y_pred=y_pred_train)}')\n",
        "    print(f'{name} - Test Accuracy: {accuracy_score(y_true=y_test, y_pred=y_pred_test)}')"
      ],
      "metadata": {
        "id": "zukQUAONUQWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "train_pca = pca.fit_transform(train_features_EN)\n",
        "test_pca = pca.transform(test_features_EN)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(train_pca[:,0], train_pca[:,1], c=y_train)\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(test_pca[:,0], test_pca[:,1], c=y_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NGBiIYehF4B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ml = Pipeline([('scaler', StandardScaler()),\n",
        "                    ('pca', PCA(n_components=100)),\n",
        "                    ('clf', SVC())])\n",
        "\n",
        "model_ml.fit(train_features_EN, y_train)\n",
        "\n",
        "evaluate(model_ml, test_features_EN, y_test, ['cat','dog'])"
      ],
      "metadata": {
        "id": "0FvnWEm0KlQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos algunas de las clasificaciones incorrectas"
      ],
      "metadata": {
        "id": "YAWIJTl3uGNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imageio.v2 import imread\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "y_test_pred = model_ml.predict(test_features_EN)\n",
        "\n",
        "mistakes_idxs = np.where(y_test_pred != y_test)[0]\n",
        "\n",
        "idxs = np.random.choice(mistakes_idxs,size=3)\n",
        "\n",
        "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
        "for i,idx in enumerate(idxs):\n",
        "    img = imread(os.path.join(test_dir,test_generator_en.filenames[idx]))\n",
        "    axs[i].imshow(img)\n",
        "    axs[i].set_title(f'Prediction: {int(y_test_pred[idx])}')\n",
        "    axs[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ty_cqO5Js3s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKDZA5MnVnt3"
      },
      "source": [
        "# 游댷 Visualizando los mapas de caracter칤sticas de una red neuronal convolucional\n",
        "\n",
        "Algunos se침alan que los modelos de aprendizaje profundo funcionan como \"cajas negras\", pues aprenden representaciones que son dif칤ciles de extraer y presentar de una forma legible para el ser humano.\n",
        "\n",
        "Si bien esto es parcialmente cierto para algunos tipos de modelos de aprendizaje profundo, definitivamente no lo es para las redes convolucionales (*CNN*). Las representaciones aprendidas por las redes convolucionales son altamente susceptibles de visualizaci칩n, en gran parte porque son representaciones de conceptos visuales. Desde 2013, se ha desarrollado una amplia gama de t칠cnicas para visualizar e interpretar estas representaciones. No exploraremos todas ellas, pero mencionaremos tres de las m치s accesibles y 칰tiles:\n",
        "\n",
        "\n",
        "\n",
        "*   **Visualizaci칩n de las salidas intermedias de una *CNN*  (\"activaciones intermedias\")**. Este m칠todo es 칰til para entender c칩mo las capas sucesivas de una red convolucional transforman su entrada y para obtener una noci칩n de la funci칩n de los filtros individuales en una red convolucional .\n",
        "\n",
        "*   **Visualizaci칩n de los  filtros en una CNN**. Este m칠todo es 칰til para entender con precisi칩n a qu칠 patr칩n o concepto visual es receptivo cada filtro en una red convolucional.\n",
        "\n",
        "*   **Visualizaci칩n de los mapas de calor de activaci칩n por clase en una imagen**. Este m칠todo es 칰til para entender qu칠 parte de una imagen se identific칩 como perteneciente a una clase determinada y, por lo tanto, permite localizar objetos en im치genes.\n",
        "\n",
        "En este ejercicio, abordaremos 칰nicamente el primer m칠todo, la visualizaci칩n de las activaciones intermedias o mapas de caracter칤sticas. Para ello, usaremos la CNN que entrenamos anteriormente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qczIqCMEzRcb"
      },
      "source": [
        "Seleccionaremos una imagen de entrada, puede ser cualquier imagen del **conjunto de test**. Por ser del conjunto de test, no forma parte de las im치genes sobre las que se entren칩 la red.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2CcwGp7dcMD"
      },
      "outputs": [],
      "source": [
        "from keras import utils\n",
        "import numpy as np\n",
        "\n",
        "# ----- Para el conjunto de datos completo ----\n",
        "# img_path = 'cnn_perros_gatos/test/cats/cat.147.jpg'   # Una im치gen de un gato\n",
        "# img_path = 'cnn_perros_gatos/test/dogs/dog.1517.jpg'  # Una im치gen de un perro\n",
        "\n",
        "# ----- Para el conjunto de datos reducido ----\n",
        "img_path = '/content/cnn_perros_gatos/test/cats/cat.10128.jpg'\n",
        "# img_path = '/content/cnn_perros_gatos/test/dogs/dog.10086.jpg'\n",
        "\n",
        "# ----- Preprocesamos la imagen en un tensor 4D\n",
        "\n",
        "img = utils.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = utils.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "\n",
        "# ----- Debemos recordar que el modelo fue entrenado con imagenes de entrada preprocesadas de la siguiente manera:\n",
        "img_tensor /= 255.\n",
        "\n",
        "# Debemos ver que su forma es de (1, 150, 150, 3)\n",
        "print(img_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjD5E3W05kjX"
      },
      "source": [
        "Mostramos la im치gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRaoI0vSdfcF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# # Para usar alguna de las imagenes reales\n",
        "# img_tensor = dog_img.reshape((1,150,150,3))\n",
        "# img_tensor = cat_img.reshape((1,150,150,3))\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.axis('Off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH5ebeoL56FH"
      },
      "source": [
        "* Para extraer los mapas de caracter칤sticas que queremos visualizar, crearemos un modelo de Keras que toma lotes 칩 *batches* de im치genes como entrada y genera las activaciones de todas las capas de convoluci칩n y *pooling*.\n",
        "* Para ello, utilizaremos la clase de Keras **Model**, que ya vimos anteriormente. Un **model** se instancia mediante dos argumentos: un tensor de entrada (o lista de tensores de entrada) y un tensor de salida (o lista de tensores de salida). La clase resultante es un modelo de Keras, igual que los modelos secuenciales (Sequential models) que ya estudiamos, que mapea las entradas especificadas a las salidas especificadas. Lo que distingue a la clase **Model** es que permite modelos con m칰ltiples salidas, a diferencia de **Sequential**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 13L5oCFXIgw22FR8irsuwNHvnzekA7bGd"
      ],
      "metadata": {
        "id": "71HBU6mGuN6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('/content/cnn_perros_gatos_improved.keras')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Gis-AZQ5uSCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos la forma de la salida de la primer capa"
      ],
      "metadata": {
        "id": "AbmmPjXpuXNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].output"
      ],
      "metadata": {
        "id": "A7VGUdgDnmYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las formas y nombres de las capas del modelo. Adem치s, identifiquemos los 칤ndices de las capas convolucionales"
      ],
      "metadata": {
        "id": "0-Y9MDqjuaoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layers_idxs = []\n",
        "conv_layers_names = []\n",
        "\n",
        "for idx, layer in enumerate(model.layers):\n",
        "    print(f\"{idx}\\t{layer.name}\")\n",
        "    if layer.name.startswith('conv'):\n",
        "        conv_layers_idxs.append(idx)\n",
        "        conv_layers_names.append(layer.name)\n",
        "\n",
        "print(conv_layers_idxs)"
      ],
      "metadata": {
        "id": "1ZByn1BRPTk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creemos un modelo tipo `Model` con tantas salidas como capas convolucionales tenemos."
      ],
      "metadata": {
        "id": "UaWvakTXu2J_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiDTipp_diyQ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "\n",
        "# Extraemos las salidas de las 8 capas superiores:\n",
        "layer_outputs = [model.layers[j].output for j in conv_layers_idxs]\n",
        "\n",
        "first_layer = model.layers[0]\n",
        "\n",
        "# Creamos un modelo que devolver치 estas salidas, dada la entrada al modelo:\n",
        "activation_model = Model(inputs=first_layer.input, outputs=layer_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB4r9KkU9IEy"
      },
      "source": [
        "* Cuando se introduce una imagen como entrada a la red, este modelo devuelve los valores de las activaciones de las capas del modelo original. Hasta antes de esta secci칩n del ejercicio, el modelo que se present칩 s칩lo ten칤a exactamente una entrada y una salida. Ahora estamos introduciendo el concepto de un modelo con m칰ltiples salidas.\n",
        "\n",
        "* En el caso general, un modelo podr칤a tener cualquier n칰mero de entradas y salidas. Este 칰ltimo modelo tiene una entrada y varias salidas, una salida por capa de convoluci칩n. Aunque, cada capa de convoluci칩n tiene varias dimensiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwuC7FEWdl8y"
      },
      "outputs": [],
      "source": [
        "# Esto devolver치 una lista de arreglos de Numpy: Un arreglo por capa de activaci칩n\n",
        "activations = activation_model.predict(img_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"N칰mero de salidas del modelo: {len(activations)}\")\n",
        "for j,activation in enumerate(activations):\n",
        "    print(f\"Forma de la activaci칩n {j}: {activation.shape}\")"
      ],
      "metadata": {
        "id": "lJ-LjFp1o7lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparar con el modelo original"
      ],
      "metadata": {
        "id": "3aWHZ-ZQvtPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "vSr7WYU6vv91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z460Fyq3TWJm"
      },
      "source": [
        "Es un mapa de caracter칤sticas con una dimensi칩n de 148 x 148 con 32 canales o profundidad.\n",
        "\n",
        "Vamos a visualizar uno de estos canales de ese mapa de caracter칤sticas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAMBCCJxdqj1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "first_layer_activation = activations[0]\n",
        "\n",
        "_, w, h, ch = first_layer_activation.shape\n",
        "print(f\"La salida son {ch} im치genes de {w}x{h}\")\n",
        "print(f\"Shape: {first_layer_activation.shape}\")\n",
        "\n",
        "num_canal = 18\n",
        "\n",
        "plt.matshow(first_layer_activation[0, :, :, num_canal], cmap='plasma')\n",
        "plt.axis('Off')\n",
        "plt.suptitle(f\"Canal {num_canal}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhiUH3GhX29w"
      },
      "source": [
        "Finalmente, vamos a desplegar un gr치fico completo de todas las activaciones en la red. En otras palabras, vamos a extraer y mostrar cada canal presente en cada uno de los mapas de caracter칤sticas. Apilaremos los resultados secuencialmente, con los canales colocados uno junto al otro.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA8QlWTZNVyI"
      },
      "source": [
        "Primero, guardamos los nombres de las capas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU3C1qdgdyQs"
      },
      "outputs": [],
      "source": [
        "# Recordemos los nombres de las capas convolucionales del modelo\n",
        "print(conv_layers_names)\n",
        "\n",
        "# Especificamos cu치ntas imagenes por cada rengl칩n\n",
        "images_per_row = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI1_CVYgd2QW"
      },
      "outputs": [],
      "source": [
        "for k,(layer_name, layer_activation) in enumerate(zip(conv_layers_names, activations)):\n",
        "    # Este es el n칰mero de canales presentes en un mapa de caracter칤sticas\n",
        "    n_features = layer_activation.shape[-1]\n",
        "\n",
        "    # El mapa de caracter칤sticas tiene la forma: (1, size, size, n_features)\n",
        "    size = layer_activation.shape[1]\n",
        "\n",
        "    # Vamos a colocar los canales de activaci칩n en esta matriz\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size)) # Aqu칤 vamos a poner toda la imagen de salida de la capa\n",
        "\n",
        "    # Colocaremos cada mapa en esta gran malla horizontal\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
        "\n",
        "            '''\n",
        "            Este proceso toma la salida de la capa convolucional\n",
        "            (que pueden tener cualquier rango de valores) y las\n",
        "            transforma en una imagen visualizable con buen contraste\n",
        "            y en el formato est치ndar de p칤xeles.\n",
        "            '''\n",
        "            channel_image -= channel_image.mean() # Centrado en cero\n",
        "            channel_image /= channel_image.std() # Normalizaci칩n por desviaci칩n est치ndar\n",
        "            channel_image *= 64 # Escalado de contraste\n",
        "            channel_image += 128 # Desplazamiento a rango positivo\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8') # Limitaci칩n al rango v치lido\n",
        "            display_grid[col * size : (col + 1) * size,\n",
        "                         row * size : (row + 1) * size] = channel_image # Llenamos la parte correspondiente de la imagen\n",
        "\n",
        "    # Mostramos los mapas en la malla\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.axis('Off')\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "    plt.savefig(f\"mascara-{k+1}.png\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh6Ip2qTYete"
      },
      "source": [
        "# Observaciones\n",
        "\n",
        "Del gr치fico de mapas de caracter칤sticas podemos notar lo siguiente:\n",
        "\n",
        "* La primera capa de la red act칰a como una colecci칩n de varios detectores de borde. En esa etapa, las activaciones a칰n retienen casi toda la informaci칩n presente en la imagen inicial.\n",
        "\n",
        "* A medida que avanzamos en profundidad, las activaciones se vuelven cada vez m치s abstractas y menos interpretables visualmente. Se comienzan a codificar conceptos de nivel superior como \"oreja de gato\" u \"ojo de gato\". Las representaciones superiores llevan cada vez menos informaci칩n sobre el contenido visual de la imagen, y cada vez m치s informaci칩n relacionada con la clase de la imagen.\n",
        "\n",
        "* La escasez de activaciones aumenta con la profundidad de la red: en la primera capa, todos los filtros se activan mediante la imagen de entrada, pero en las siguientes capas, m치s y m치s canales de activaci칩n est치n en blanco. Esto significa que el patr칩n codificado por el filtro no se encuentra en la imagen de entrada.\n",
        "\n",
        "# Comentarios Finales\n",
        "\n",
        "Acabamos de evidenciar un hecho muy importante de las representaciones aprendidas por las redes neuronales profundas: las caracter칤sticas extra칤das por una capa se vuelven cada vez m치s abstractas con la profundidad de la red.\n",
        "\n",
        "Las activaciones de las capas superiores contienen cada vez menos informaci칩n sobre la entrada espec칤fica que se est치 viendo y m치s informaci칩n sobre el objetivo (en el caso de este ejemplo, la clase de la imagen: gato o perro). Una red neuronal profunda act칰a efectivamente como un *pipeline* (tuber칤a) que destila la informaci칩n, con datos en crudo que entran (en nuestro caso, im치genes RBG) y se transforman repetidamente de tal forma que la informaci칩n irrelevante es filtrada (por ejemplo, la apariencia visual espec칤fica de la imagen) mientras que la informaci칩n 칰til es magnificada y refinada (por ejemplo, la clase de la imagen).\n",
        "\n",
        "Esto es an치logo a la forma en que los humanos y los animales perciben el mundo: despu칠s de observar una escena durante unos segundos, un humano puede recordar qu칠 objetos abstractos estaban presentes en 칠l (por ejemplo, una bicicleta, un 치rbol) pero muchas veces no puede recordar la apariencia espec칤fica de estos objetos.\n",
        "\n",
        "El cerebro ha aprendido a abstraer completamente la informaci칩n visual, a transformarla en conceptos visuales de alto nivel mientras filtra por completo los detalles visuales irrelevantes, haciendo que sea tremendamente dif칤cil recordar c칩mo se ven exactamente las cosas a nuestro alrededor.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5R3eylJLMUNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
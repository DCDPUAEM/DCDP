{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TgLUQd5WLOm"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/notebooks/05-CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFc78aDaWLOo"
      },
      "source": [
        "<h1>Clasificaci√≥n con Redes Neuronales Convolucionales</h1>\n",
        "\n",
        "En esta notebook usaremos una red neuronal convolucional (CNN) para clasificar el dataset *cats vs dogs* de kaggle. Observaremos, adem√°s, el efecto del dropout y analizaremos la informaci√≥n de las capas ocultas para ganar intuici√≥n sobre el funcionamiento interno de este tipo de redes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq-bgFf-WLOq"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN71Ul1JWLOx"
      },
      "source": [
        "Verifiquemos que el entorno de ejecuci√≥n en Colab sea GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln6imMSY8vLe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('GPU presente en: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EjrtHWCWLOv"
      },
      "source": [
        "# [El dataset Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG3DcGuhdyaT"
      },
      "source": [
        "El conjunto de datos de Dogs vs Cats fue publicado por Kaggle como parte de una competencia de visi√≥n computacional a fines de 2013, cuando las CNNs no eran muy comunes.\n",
        "\n",
        "Se puede descargar el dataset original en: https://www.kaggle.com/c/dogs-vs-cats/data.\n",
        "\n",
        "Esta notebook se puede usar con dos conjuntos de datos:\n",
        "\n",
        "* Usaremos el conjunto de datos original de entrenamiento, dado que contiene las etiquetas de las clases. Este conjunto contiene 25,000 im√°genes de perros y gatos (12,500 de cada clase) y tiene un tama√±o de 543 MB. Ya se encuentra dividido en *train*, *validation* y *test*. [Download](https://drive.google.com/file/d/1Q3xOfn2Up9uIOLviS66oYH_oFFK-IGpW/view?usp=sharing)\n",
        "\n",
        "* Usaremos un conjunto reducido de datos, el cual contiene 1000 im√°genes de cada clase para entrenamiento, 500 para validaci√≥n y 500 para prueba. Todos los datos se sacaron del conjunto de entrenamiento original. [Download](https://drive.google.com/file/d/1Ce3u8dwYYriLkz5OpcGn72xIQENIHZX5/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C4obFfa2rjL"
      },
      "source": [
        "Copiaremos el dataset desde un v√≠nculo de Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsjdNtihfL8n"
      },
      "outputs": [],
      "source": [
        "!pip install -qq gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC-8WGEgidmb"
      },
      "source": [
        "Descargamos el dataset desde Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LDaaZ3udhLI"
      },
      "outputs": [],
      "source": [
        "# ----- Versi√≥n completa -----\n",
        "# !gdown --id 1Q3xOfn2Up9uIOLviS66oYH_oFFK-IGpW\n",
        "\n",
        "# ----- Copia de la versi√≥n completa -----\n",
        "# !gdown 1hchhNQ_3WNncaXVD3kX58EIppcYFt-E2\n",
        "\n",
        "# ----- Versi√≥n reducida -----\n",
        "!gdown 1Ce3u8dwYYriLkz5OpcGn72xIQENIHZX5\n",
        "\n",
        "# ----- Copia de la versi√≥n reducida -----\n",
        "# !gdown 1NK9LvrVwsEQM0UHkFHq_GYCF2fjGrwAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXbCkAPa2w66"
      },
      "source": [
        "Descomprimimos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G3WswyR8vho"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# file_name = '/content/cnn_perros_gatos.zip'\n",
        "# file_name = '/content/cnn_perros_gatos-copia.zip'\n",
        "file_name = '/content/cnn_perros_gatos-small.zip'\n",
        "# file_name = '/content/cnn_perros_gatos-small-copia.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as myzip:\n",
        "    myzip.extractall()\n",
        "    print('Listo')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos algunas im√°genes del dataset, como podemos ver:\n",
        "\n",
        "* Son archivos jpeg\n",
        "* Tienen diferentes tama√±os"
      ],
      "metadata": {
        "id": "TtPRJtxIhLJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq ipyplot"
      ],
      "metadata": {
        "id": "0EVbHiCae8nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import ipyplot\n",
        "import random, os\n",
        "\n",
        "path_1 = '/content/cnn_perros_gatos/train/cats'\n",
        "path_2 = '/content/cnn_perros_gatos/train/dogs'\n",
        "\n",
        "filenames_1 = random.sample(os.listdir(path_1), 5)\n",
        "filenames_2 = random.sample(os.listdir(path_2), 5)\n",
        "\n",
        "full_filenames_1 = [os.path.join(path_1, fname) for fname in filenames_1]\n",
        "full_filenames_2 = [os.path.join(path_2, fname) for fname in filenames_2]\n",
        "\n",
        "filenames = full_filenames_1 + full_filenames_2\n",
        "images_list = [Image.open(fname) for fname in filenames]\n",
        "\n",
        "ipyplot.plot_images(images_list,show_url=False,)"
      ],
      "metadata": {
        "id": "6WY6JsL3ecRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ ¬øQu√© retos presentar√≠a este dataset para una MLP?"
      ],
      "metadata": {
        "id": "fpHSQQQa3Jzo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFqw4JUe-P2O"
      },
      "source": [
        "Exploramos las carpetas de entrenamiento, validaci√≥n y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMypQXip8vzN"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "train_dogs = 'cnn_perros_gatos/train/dogs'\n",
        "print('Para entrenamiento:')\n",
        "print(f'\\t{len(os.listdir(train_dogs))} Perros.')\n",
        "train_cats = 'cnn_perros_gatos/train/cats'\n",
        "print(f'\\t{len(os.listdir(train_cats))} Gatos.')\n",
        "print('\\nPara validaci√≥n:')\n",
        "validation_dogs = 'cnn_perros_gatos/validation/dogs'\n",
        "print(f'\\t{len(os.listdir(validation_dogs))} Perros.')\n",
        "validation_cats = 'cnn_perros_gatos/validation/cats'\n",
        "print(f'\\t{len(os.listdir(validation_cats))} Gatos.')\n",
        "print('\\nPara prueba:')\n",
        "test_dogs = 'cnn_perros_gatos/test/dogs'\n",
        "print(f'\\t{len(os.listdir(test_dogs))} Perros.')\n",
        "test_cats = 'cnn_perros_gatos/test/cats'\n",
        "print(f'\\t{len(os.listdir(test_cats))} Gatos.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4aC0_mXDbXn"
      },
      "source": [
        "Definimos los directorios de entrenamiento, validaci√≥n y prueba para usaralos en el resto de la notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJGFJS-qNLJc"
      },
      "outputs": [],
      "source": [
        "train_dir = 'cnn_perros_gatos/train'\n",
        "validation_dir = 'cnn_perros_gatos/validation'\n",
        "test_dir = 'cnn_perros_gatos/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phDgmrWsWLO6"
      },
      "source": [
        "# Modelo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZFIorONDhG9"
      },
      "source": [
        "Definimos un primer modelo de CNN. Usaremos las capas `Conv2D` para las operaciones de convoluci√≥n y `MaxPooling2D` para el pooling.\n",
        "\n",
        "* https://keras.io/api/layers/convolution_layers/convolution2d/\n",
        "* https://keras.io/api/layers/pooling_layers/max_pooling2d/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yySANB-NNr4w"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, 3, activation='relu',\n",
        "                           input_shape=(150, 150, 3)),  # ¬øPor qu√© 150x150?\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, 3, activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, 3, activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSL8Y3n7rLtL"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbnp2bbL9jes"
      },
      "source": [
        "* NOTA que comenzamos con im√°genes de tama√±o 150 x 150 (una elecci√≥n de tama√±o arbitraria) y terminamos con mapas de caracter√≠sticas de tama√±o 7 x 7 justo antes de la capa de *flatten*.\n",
        "* En realidad las im√°genes de entrada tienen tama√±os diversos (desconocidos), pero afortunadamente Keras nos puede ayudar a pre-procesarlas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA43G8SqEv2f"
      },
      "source": [
        "Compilamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop\n",
        "\n",
        "opt = RMSprop(learning_rate=1e-4)"
      ],
      "metadata": {
        "id": "vrU-K9eKLcDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94vCX8FgrPgU"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Zxc_HG1MuG"
      },
      "source": [
        "## Preprocesamiento de datos\n",
        "\n",
        "\n",
        "Los datos deben formatearse en tensores de punto flotante preprocesados adecuadamente antes de que se introduzcan en la red. En este momento, nuestros datos se encuentran almacenados como archivos JPEG, por lo que los pasos para que puedan ser introducidos en nuestra red son:\n",
        "\n",
        "* Leer los archivos de imagen.\n",
        "\n",
        "* Decodificar el contenido JPEG a cuadr√≠culas de p√≠xeles RBG.\n",
        "\n",
        "* Convertirlos en tensores de punto flotante.\n",
        "\n",
        "* Volver a escalar los valores de p√≠xeles (entre 0 y 255) al intervalo $[0, 1]$ (las redes neuronales prefieren tratar con valores de entrada peque√±os).\n",
        "\n",
        "Afortunadamente, Keras tiene herramientas para encargarse de estos pasos autom√°ticamente. Keras tiene un m√≥dulo con herramientas de ayuda para procesamiento de im√°genes, ubicado en **keras.preprocessing.image**. En particular, contiene la clase **ImageDataGenerator**, que permite configurar r√°pidamente los generadores de Python que pueden convertir autom√°ticamente los archivos de imagen en disco en *batches* de tensores preprocesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LAZtTFHvkvZ"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Todas las im√°genes ser√°n reescaladas por 1./255.\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,              # Directorio donde buscar√° las imagenes\n",
        "        target_size=(150, 150), # Todas las im√°genes se redimensionar√°n a 150 x 150\n",
        "        batch_size=20,\n",
        "        class_mode='binary'     # Es una tarea de clasificaci√≥n binaria\n",
        "        )\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHKu7G_y4fPC"
      },
      "source": [
        "* Vamos a revisar la salida de uno de estos generadores: produce batches de im√°genes de 150 x 150 RGB (con la forma (20, 150, 150, 3)) y etiquetas binarias (con la forma (20,)). 20 es el n√∫mero de muestras en cada batch (el tama√±o del batch).\n",
        "* Como el generador genera estos batches de forma indefinida (i.e. recorre sin fin las im√°genes presentes en la carpeta que se le indic√≥), se necesita romper el loop de iteraci√≥n en alg√∫n punto. A continuaci√≥n podemos ver c√≥mo es cada corrida (batch/step) que proporciona el generador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51xbh9LMv0yQ"
      },
      "outputs": [],
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print(f'Dimensiones del batch de im√°genes: {data_batch.shape}')\n",
        "    print(f'Dimensiones del batch de las etiquetas: {labels_batch.shape}')\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LevQH8ih4ek_"
      },
      "source": [
        "* Vamos a proceder a entrenar nuestro modelo con los datos usando el generador. Debido a que los datos se generan infinitamente, el generador necesita saber cu√°ntas muestras extraer antes de declarar una √©poca finalizada. Esta es la funci√≥n del argumento **steps_per_epoch**\n",
        "\n",
        "* En este caso, **steps_per_epoch** corresponde al n√∫mero de batches que requiere el generador para leer el conjunto de datos completo. S√≥lo despu√©s de haber solicitado este n√∫mero de batches, el proceso de ajuste de nuestro modelo pasar√° a la siguiente √©poca. **steps_per_epoch** corresponde a el n√∫mero de pasos de descenso del gradiente. En nuestro caso, cada batch tiene un tama√±o de 20 muestras, por lo que tomar√° 100 pasos (batches) hasta que cubramos las 2,000 muestras de nuestra base de datos.\n",
        "\n",
        "* Como siempre, uno puede pasar un argumento llamado **validation_data**. Es importante destacar que este argumento puede ser un generador de datos en s√≠ mismo, pero tambi√©n podr√≠a ser una tupla de arreglos Numpy. Si se pasa un generador como **validation_data**, entonces se espera que este generador produzca batches de datos de validaci√≥n sin fin, y por lo tanto tambi√©n se debe especificar el argumento **validation_steps**, que le dice al proceso cu√°ntos batches debe extraer del generador de validaci√≥n para su evaluaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "505HhEEzMn8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tarda alrededor de 3 minutos\n",
        "\n"
      ],
      "metadata": {
        "id": "blAr9k68wLoN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdjhzcgzwQ-T"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOeKcGvvWLPB"
      },
      "source": [
        "## Rendimiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtq39jKpFV2g"
      },
      "source": [
        "Guardamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFnMW2EbyyqD"
      },
      "outputs": [],
      "source": [
        "model.save('cnn_perros_gatos_model1.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFYSGmOGZsF"
      },
      "source": [
        "Grafiquemos las curvas de aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt6kRFIYzjPb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting**... ¬°Tenemos muy pocos ejemplos!"
      ],
      "metadata": {
        "id": "_GnVvFeiJVsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "VmNuNQb-IC6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 2"
      ],
      "metadata": {
        "id": "y4GxjIoaMv_t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXzBZmaj_V2_"
      },
      "source": [
        "## Aumento de Datos\n",
        "\n",
        "* El efecto de sobreajuste ocurre cuando se tienen muy pocas muestras de las que aprender, lo que nos impide entrenar un modelo capaz de generalizar a nuevos datos. Si tuviesemos datos infinitos, nuestro modelo estar√≠a expuesto a todos los aspectos posibles de la distribuci√≥n de datos en cuesti√≥n y nunca se sobreajustar√≠a nuestro modelo.\n",
        "\n",
        "* El aumento de datos adopta el enfoque de generar m√°s datos de entrenamiento a partir de muestras de entrenamiento existentes, al \"aumentar\" las muestras a trav√©s de una serie de transformaciones aleatorias que producen im√°genes de apariencia cre√≠ble. El objetivo es que durante el tiempo de entrenamiento, nuestro modelo nunca vea exactamente la misma imagen dos veces. Esto ayuda a que el modelo se exponga a m√°s aspectos de los datos y generalice mejor.\n",
        "\n",
        "* En Keras, esto se puede hacer configurando una serie de transformaciones aleatorias que se realizar√°n en las im√°genes le√≠das por nuestra instancia de ImageDataGenerator.\n",
        "\n",
        "* Vamos a comenzar por aumentar una imagen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_qyda81Q81"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U48tZWbBB2TM"
      },
      "source": [
        "Las opciones anteriores son solo algunas de las opciones disponibles.\n",
        "\n",
        "* **rotation_range** es un valor en grados (0-180), un rango dentro del cual girar las im√°genes de forma aleatoria.\n",
        "\n",
        "* **width_shift** y **height_shift** son rangos expresados como una fracci√≥n del ancho o altura total de la imagen, dentro de los cuales se pueden trasladar vertical u horizontalmente de forma aleatoria a las im√°genes.\n",
        "\n",
        "* **shear_range** aplica aleatoriamente transformaciones de corte.\n",
        "\n",
        "* **zoom_range** aplica acercamientos aleatorios dentro de las im√°genes.\n",
        "\n",
        "* **horizontal_flip** Voltea de forma aleatoria la mitad en las im√°genes horizontalmente.\n",
        "\n",
        "* **fill_mode** es la estrategia utilizada para rellenar p√≠xeles creados, que pueden aparecer despu√©s de una rotaci√≥n o un cambio de ancho / altura."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos 25 im√°genes modificadas a partir de una misma imagen"
      ],
      "metadata": {
        "id": "XgJ_5x2rtyM4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbyqp-q61cbr"
      },
      "outputs": [],
      "source": [
        "from keras import utils\n",
        "import random\n",
        "\n",
        "train_cats_dir = '/content/cnn_perros_gatos/train/cats' # El directorio donde est√°n las im√°genes de gatos de entrenamiento\n",
        "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
        "img_path = random.sample(fnames, 1)[0] # Escogemos una imagen al azar para aplicar el \"aumentado de datos\"\n",
        "img = utils.load_img(img_path, target_size=(150, 150)) # Leemos la imagen y la redimensionamos.\n",
        "x = utils.img_to_array(img) # Leemos la imagen y la redimensionamos.\n",
        "x = x.reshape((1,) + x.shape) # Redimensionamos el arreglo a (1, 150, 150, 3)\n",
        "\n",
        "'''\n",
        "El comando .flow () genera batches de im√°genes transformadas aleatoriamente\n",
        "Con el \"for\" de abajo estaremos en un loop indefinidamente,\n",
        "Necesitamos 'romper' el loop en alg√∫n momento\n",
        "'''\n",
        "\n",
        "fig, axs = plt.subplots(5,5,figsize=(10,10))\n",
        "k = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    i = k//5\n",
        "    j = k%5\n",
        "    axs[i,j].imshow(utils.array_to_img(batch[0]))\n",
        "    axs[i,j].axis('off')\n",
        "    k += 1\n",
        "    if k == 25:\n",
        "        break\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teFUV-JSFKZH"
      },
      "source": [
        "* Si entrenamos una nueva red neuronal utilizando esta configuraci√≥n de aumento de datos, nuestra red nunca ver√° dos veces la misma entrada, pues a cada nueva imagen se le aplica transformaciones aleatorias dentro de ciertos rangos.\n",
        "\n",
        "* Sin embargo, las entradas que ves est√°n a√∫n muy interrelacionadas, ya que provienen de un peque√±o n√∫mero de im√°genes originales: **no podemos producir nueva informaci√≥n, s√≥lo podemos mezclar la informaci√≥n existente**.\n",
        "\n",
        "* Dado que esto podr√≠a no ser suficiente para librarnos del sobreajuste. Para mitigarlo a√∫n m√°s, tambi√©n agregaremos una capa de Dropout a nuestro modelo, justo antes de la etapa del clasificador densamente conectado (fully-connected)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ5RY_qYGlln"
      },
      "source": [
        "Definimos la misma red CNN, con dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYgaCOd2R4HU"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, 3, activation='relu', input_shape=(150, 150, 3),name='Convolution1'),\n",
        "    MaxPooling2D(name='MaxPooling1'),\n",
        "    Conv2D(64, 3, activation='relu',name='Convolution2'),\n",
        "    MaxPooling2D(name='MaxPooling2'),\n",
        "    Conv2D(128, 3, activation='relu',name='Convolution3'),\n",
        "    MaxPooling2D(name='MaxPooling3'),\n",
        "    Conv2D(128, 3, activation='relu',name='Convolution4'),\n",
        "    MaxPooling2D(name='MaxPooling4'),\n",
        "    Flatten(name='Flatten'),\n",
        "    Dropout(0.1,name='DropOut'), # 0.5\n",
        "    Dense(512, activation='relu',name='Densa'),\n",
        "    Dense(1, activation='sigmoid',name='Salida')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbgrTc9UV5i-"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop\n",
        "\n",
        "opt = RMSprop(learning_rate=1e-4)"
      ],
      "metadata": {
        "id": "5hjNpXpZevmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6bwra-5TfZ3"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "-BL7MuurM3p2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHiNZWb2GqmS"
      },
      "source": [
        "Definimos los nuevos generadores de im√°genes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hc8gia_TwWn"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "# Debemos notar que los datos del conjunto validaci√≥n y prueba no son sometidos\n",
        "# al aumentado de datos\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # Esta es la carpeta de origen\n",
        "        train_dir,\n",
        "        # Todas las im√°genes se redimensionar√°n a 150 x 150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenemos el modelo.\n",
        "\n",
        "**Observaci√≥n**: Para mejores resultados entrenar durante 100 √©pocas (tarda alrededor de 30 minutos). Por cuestiones de tiempo, entrenamos con 30 √©pocas (tarda alrededor de 8 minutos)."
      ],
      "metadata": {
        "id": "fRqn2i_1wUZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "                train_generator,\n",
        "                steps_per_epoch=100,\n",
        "                epochs=30,\n",
        "                validation_data=validation_generator,\n",
        "                validation_steps=50,\n",
        "                verbose=2)"
      ],
      "metadata": {
        "id": "idfaLKGcwUDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5VV-0J0UTGN"
      },
      "outputs": [],
      "source": [
        "# Descomentar la siguiente linea si entrenaste con epochs = 100\n",
        "model.save('cnn_perros_gatos_model2.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CHs_DRqGwKk"
      },
      "source": [
        "Veamos las curvas de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pYHbXOlUXPx"
      },
      "outputs": [],
      "source": [
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(validation_generator)"
      ],
      "metadata": {
        "id": "L4A9vsXhIAh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_-OWUThLg-f"
      },
      "source": [
        "## ¬øQu√© pasar√≠a si entrenas con 100 √©pocas?\n",
        "\n",
        "Con 100 √©pocas se obtendr√≠an los siguientes resultados:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1tqFh1ETyhtOE3ttAIkjk8cqFy-e-SLR_)\n",
        "\n",
        "El accuracy fue de 80.1%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos descargar este modelo ya entrenado de Google Drive"
      ],
      "metadata": {
        "id": "RVHUfTj5Ibcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° Podemos seguirlo usando o leerlo desde un archivo"
      ],
      "metadata": {
        "id": "COssYGVAI3mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1m5kfBviSBa6dI9wEwBpowwlBJ1EUdds0"
      ],
      "metadata": {
        "id": "gjHZ07i3IQsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfXwuYwPda0Q"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('cnn_perros_gatos_extra_imgs_100_epocas.h5')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKDZA5MnVnt3"
      },
      "source": [
        "## ‚ûñ Visualizando los mapas de caracter√≠sticas de una red neuronal convolucional\n",
        "\n",
        "Algunos se√±alan que los modelos de aprendizaje profundo funcionan como \"cajas negras\", pues aprenden representaciones que son dif√≠ciles de extraer y presentar de una forma legible para el ser humano.\n",
        "\n",
        "Si bien esto es parcialmente cierto para algunos tipos de modelos de aprendizaje profundo, definitivamente no lo es para las redes convolucionales (*CNN*). Las representaciones aprendidas por las redes convolucionales son altamente susceptibles de visualizaci√≥n, en gran parte porque son representaciones de conceptos visuales. Desde 2013, se ha desarrollado una amplia gama de t√©cnicas para visualizar e interpretar estas representaciones. No exploraremos todas ellas, pero mencionaremos tres de las m√°s accesibles y √∫tiles:\n",
        "\n",
        "\n",
        "\n",
        "*   **Visualizaci√≥n de las salidas intermedias de una *CNN*  (\"activaciones intermedias\")**. Este m√©todo es √∫til para entender c√≥mo las capas sucesivas de una red convolucional transforman su entrada y para obtener una noci√≥n de la funci√≥n de los filtros individuales en una red convolucional .\n",
        "\n",
        "*   **Visualizaci√≥n de los  filtros en una CNN**. Este m√©todo es √∫til para entender con precisi√≥n a qu√© patr√≥n o concepto visual es receptivo cada filtro en una red convolucional.\n",
        "\n",
        "*   **Visualizaci√≥n de los mapas de calor de activaci√≥n por clase en una imagen**. Este m√©todo es √∫til para entender qu√© parte de una imagen se identific√≥ como perteneciente a una clase determinada y, por lo tanto, permite localizar objetos en im√°genes.\n",
        "\n",
        "En este ejercicio, abordaremos √∫nicamente el primer m√©todo, la visualizaci√≥n de las activaciones intermedias o mapas de caracter√≠sticas. Para ello, usaremos la CNN que entrenamos anteriormente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qczIqCMEzRcb"
      },
      "source": [
        "Seleccionaremos una imagen de entrada, puede ser cualquier imagen del **conjunto de test**. Por ser del conjunto de test, no forma parte de las im√°genes sobre las que se entren√≥ la red.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2CcwGp7dcMD"
      },
      "outputs": [],
      "source": [
        "from keras import utils\n",
        "import numpy as np\n",
        "\n",
        "# ----- Para el conjunto de datos completo ----\n",
        "# img_path = 'cnn_perros_gatos/test/cats/cat.147.jpg'   # Una im√°gen de un gato\n",
        "# img_path = 'cnn_perros_gatos/test/dogs/dog.1517.jpg'  # Una im√°gen de un perro\n",
        "\n",
        "# ----- Para el conjunto de datos reducido ----\n",
        "img_path = '/content/cnn_perros_gatos/test/cats/cat.10128.jpg'\n",
        "# img_path = '/content/cnn_perros_gatos/test/dogs/dog.10086.jpg'\n",
        "\n",
        "# ----- Preprocesamos la imagen en un tensor 4D\n",
        "\n",
        "img = utils.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = utils.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "\n",
        "# ----- Debemos recordar que el modelo fue entrenado con imagenes de entrada preprocesadas de la siguiente manera:\n",
        "img_tensor /= 255.\n",
        "\n",
        "# Debemos ver que su forma es de (1, 150, 150, 3)\n",
        "print(img_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjD5E3W05kjX"
      },
      "source": [
        "Mostramos la im√°gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRaoI0vSdfcF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.axis('Off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH5ebeoL56FH"
      },
      "source": [
        "* Para extraer los mapas de caracter√≠sticas que queremos visualizar, crearemos un modelo de Keras que toma lotes √≥ *batches* de im√°genes como entrada y genera las activaciones de todas las capas de convoluci√≥n y *pooling*.\n",
        "* Para ello, utilizaremos la clase de Keras **Model**, que ya vimos anteriormente. Un **model** se instancia mediante dos argumentos: un tensor de entrada (o lista de tensores de entrada) y un tensor de salida (o lista de tensores de salida). La clase resultante es un modelo de Keras, igual que los modelos secuenciales (Sequential models) que ya estudiamos, que mapea las entradas especificadas a las salidas especificadas. Lo que distingue a la clase **Model** es que permite modelos con m√∫ltiples salidas, a diferencia de **Sequential**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].output"
      ],
      "metadata": {
        "id": "A7VGUdgDnmYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiDTipp_diyQ"
      },
      "outputs": [],
      "source": [
        "# Extraemos las salidas de las 8 capas superiores:\n",
        "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
        "# Creamos un modelo que devolver√° estas salidas, dada la entrada al modelo:\n",
        "activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB4r9KkU9IEy"
      },
      "source": [
        "* Cuando se introduce una imagen como entrada a la red, este modelo devuelve los valores de las activaciones de las capas del modelo original. Hasta antes de esta secci√≥n del ejercicio, el modelo que se present√≥ s√≥lo ten√≠a exactamente una entrada y una salida. Ahora estamos introduciendo el concepto de un modelo con m√∫ltiples salidas.\n",
        "\n",
        "* En el caso general, un modelo podr√≠a tener cualquier n√∫mero de entradas y salidas. Este √∫ltimo modelo tiene una entrada y 8 salidas, una salida por capa de activaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwuC7FEWdl8y"
      },
      "outputs": [],
      "source": [
        "# Esto devolver√° una lista de arreglos de Numpy: Un arreglo por capa de activaci√≥n\n",
        "activations = activation_model.predict(img_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activations[1].shape"
      ],
      "metadata": {
        "id": "lJ-LjFp1o7lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4--_UfiCP1O"
      },
      "source": [
        "Por ejemplo, vamos a imprimir las dimensiones  del mapa de caracter√≠sticas/activaciones de la primer capa convolucional para la imagen del gato:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnYfbrz1doar"
      },
      "outputs": [],
      "source": [
        "first_layer_activation = activations[0]\n",
        "print(first_layer_activation.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z460Fyq3TWJm"
      },
      "source": [
        "Es un mapa de caracter√≠sticas con una dimensi√≥n de 148 x 148 con 32 canales o profundidad.\n",
        "\n",
        "Vamos a visualizar el 3er canal de ese mapa de caracter√≠sticas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAMBCCJxdqj1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.matshow(first_layer_activation[0, :, :, 28], cmap='plasma')\n",
        "plt.axis('Off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ICdttbWbPW"
      },
      "source": [
        "Probemos alg√∫n otro canal, pero debemos tener en cuenta que los canales que tenga uno pueden variar de los que tiene cualquier otro, ya que los filtros que en espec√≠fico aprende la red en las capas convolucionales no son determin√≠sticos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1pjytgcdu_B"
      },
      "outputs": [],
      "source": [
        "plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\n",
        "plt.axis('Off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhiUH3GhX29w"
      },
      "source": [
        "Finalmente, vamos a desplegar un gr√°fico completo de todas las activaciones en la red. En otras palabras, vamos a extraer y mostrar cada canal presente en cada uno de nuestros 8 mapas de caracter√≠sticas. Apilaremos los resultados en un gran tensor de imagen, con los canales colocados uno junto al otro.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA8QlWTZNVyI"
      },
      "source": [
        "Primero, guardamos los nombres de las capas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU3C1qdgdyQs"
      },
      "outputs": [],
      "source": [
        "layer_names = []\n",
        "for layer in model.layers[:8]:\n",
        "    layer_names.append(layer.name)\n",
        "\n",
        "images_per_row = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI1_CVYgd2QW"
      },
      "outputs": [],
      "source": [
        "\n",
        "for k,(layer_name, layer_activation) in enumerate(zip(layer_names, activations)):\n",
        "    # Este es el n√∫mero de caracter√≠sticas presentes en un mapa de caracter√≠sticas\n",
        "    n_features = layer_activation.shape[-1]\n",
        "\n",
        "    # El mapa de caracter√≠sticas tiene la forma: (1, size, size, n_features)\n",
        "    size = layer_activation.shape[1]\n",
        "\n",
        "    # Vamos a colocar los canales de activaci√≥n en esta matriz\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "\n",
        "    # Colocaremos cada mapa en esta gran malla horizontal\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
        "\n",
        "            # Procesaremos el mapa de caracter√≠sticas para que sea visualmente agradable\n",
        "            channel_image -= channel_image.mean()\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "            display_grid[col * size : (col + 1) * size,\n",
        "                         row * size : (row + 1) * size] = channel_image\n",
        "\n",
        "    # Mostramos los mapas en la malla\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.axis('Off')\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "    plt.savefig(f\"mascara-{k+1}.png\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh6Ip2qTYete"
      },
      "source": [
        "# Observaciones\n",
        "\n",
        "Del gr√°fico de mapas de caracter√≠sticas podemos notar lo siguiente:\n",
        "\n",
        "* La primera capa de la red act√∫a como una colecci√≥n de varios detectores de borde. En esa etapa, las activaciones a√∫n retienen casi toda la informaci√≥n presente en la imagen inicial.\n",
        "\n",
        "* A medida que avanzamos en profundidad, las activaciones se vuelven cada vez m√°s abstractas y menos interpretables visualmente. Se comienzan a codificar conceptos de nivel superior como \"oreja de gato\" u \"ojo de gato\". Las representaciones superiores llevan cada vez menos informaci√≥n sobre el contenido visual de la imagen, y cada vez m√°s informaci√≥n relacionada con la clase de la imagen.\n",
        "\n",
        "* La escasez de activaciones aumenta con la profundidad de la red: en la primera capa, todos los filtros se activan mediante la imagen de entrada, pero en las siguientes capas, m√°s y m√°s canales de activaci√≥n est√°n en blanco. Esto significa que el patr√≥n codificado por el filtro no se encuentra en la imagen de entrada.\n",
        "\n",
        "# Comentarios Finales\n",
        "\n",
        "Acabamos de evidenciar un hecho muy importante de las representaciones aprendidas por las redes neuronales profundas: las caracter√≠sticas extra√≠das por una capa se vuelven cada vez m√°s abstractas con la profundidad de la red.\n",
        "\n",
        "Las activaciones de las capas superiores contienen cada vez menos informaci√≥n sobre la entrada espec√≠fica que se est√° viendo y m√°s informaci√≥n sobre el objetivo (en el caso de este ejemplo, la clase de la imagen: gato o perro). Una red neuronal profunda act√∫a efectivamente como un *pipeline* (tuber√≠a) que destila la informaci√≥n, con datos en crudo que entran (en nuestro caso, im√°genes RBG) y se transforman repetidamente de tal forma que la informaci√≥n irrelevante es filtrada (por ejemplo, la apariencia visual espec√≠fica de la imagen) mientras que la informaci√≥n √∫til es magnificada y refinada (por ejemplo, la clase de la imagen).\n",
        "\n",
        "Esto es an√°logo a la forma en que los humanos y los animales perciben el mundo: despu√©s de observar una escena durante unos segundos, un humano puede recordar qu√© objetos abstractos estaban presentes en √©l (por ejemplo, una bicicleta, un √°rbol) pero muchas veces no puede recordar la apariencia espec√≠fica de estos objetos.\n",
        "\n",
        "El cerebro ha aprendido a abstraer completamente la informaci√≥n visual, a transformarla en conceptos visuales de alto nivel mientras filtra por completo los detalles visuales irrelevantes, haciendo que sea tremendamente dif√≠cil recordar c√≥mo se ven exactamente las cosas a nuestro alrededor.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5R3eylJLMUNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
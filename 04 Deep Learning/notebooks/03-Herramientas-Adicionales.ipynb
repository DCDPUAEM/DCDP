{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/notebooks/03-Herramientas-Adicionales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "n-VKWu5-95LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Herramientas Adicionales</h1>\n",
        "\n",
        "El objetivo de esta notebook es mostrar algunas herramientas adicionales para mejorar el entrenamiento y/o desempe√±o de redes neuronales. En particular, veremos:\n",
        "\n",
        "* Callbacks\n",
        "    * Early Stopping\n",
        "    * Checkpoint\n",
        "* Dropout\n",
        "* Gridsearch\n",
        "\n",
        "Adem√°s, realizaremos ejemplos de clasificaci√≥n binaria."
      ],
      "metadata": {
        "id": "QnpFmzTeM5Lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recuerda la simbolog√≠a de las secciones:\n",
        "\n",
        "* üîΩ Esta secci√≥n no forma parte del proceso usual de Machine Learning. Es una exploraci√≥n did√°ctica de alg√∫n aspecto del funcionamiento del algoritmo.\n",
        "* ‚ö° Esta secci√≥n incluye t√©cnicas m√°s avanzadas destinadas a optimizar o profundizar en el uso de los algoritmos.\n",
        "* ‚≠ï Esta secci√≥n contiene un ejercicio o pr√°ctica a realizar. A√∫n si no se establece una fecha de entrega, es muy recomendable realizarla para practicar conceptos clave de cada tema."
      ],
      "metadata": {
        "id": "andCwrj_XJgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "De esta forma podemos verificar que tenemos una GPU:"
      ],
      "metadata": {
        "id": "DnX6pDkyT6vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('GPU presente en: {}'.format(tf.test.gpu_device_name()))"
      ],
      "metadata": {
        "id": "cBYEZQgDT6_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks"
      ],
      "metadata": {
        "id": "ZCB9b3aRwB91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un *callback* es un objeto que puede realizar acciones en varias etapas del entrenamiento (por ejemplo, al inicio o al final de una √©poca, antes o despu√©s de un *batch*, etc.).\n",
        "\n",
        "Puedes usar *callbacks* para:\n",
        "\n",
        "* Escribir los registros de TensorBoard despu√©s de cada lote de entrenamiento para monitorizar tus m√©tricas\n",
        "* Guardar peri√≥dicamente tu modelo en el disco\n",
        "* Hacer un *early stopping*.\n",
        "* Obtener una visi√≥n de los estados internos y las estad√≠sticas de un modelo durante el entrenamiento.\n",
        "\n",
        "Podemos consultar la lista completa de callbacks en https://keras.io/api/callbacks/"
      ],
      "metadata": {
        "id": "26r9vfsUMjM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ilustrar algunos callbacks, consideremos el siguiente ejemplo ilustrativo. Entrenaremos una red neuronal MLP para la tarea de **clasificaci√≥n binaria** en el siguiente dataset."
      ],
      "metadata": {
        "id": "7WPQLIByweBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_moons(n_samples=400, noise=0.21, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,train_size=0.8,random_state=199)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_train[:,0],X_train[:,1],c=y_train)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rJJI47H8I-hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Callbacks: `EarlyStopping`"
      ],
      "metadata": {
        "id": "6sEU7Qa-XI6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El [`EarlyStopping`](https://keras.io/api/callbacks/early_stopping/) es un *callback* que nos permite detener el entrenamiento para evitar el overfitting. El callback monitorea el entrenamiento y lo detiene en el momento en el que una m√©trica (o perdida) deja de mejorar.\n",
        "\n",
        "Algunos de los principales hiperpar√°metros son:\n",
        "\n",
        "* `monitor`: Cantidad a controlar.\n",
        "* `min_delta`: Cambio m√≠nimo en la cantidad monitorizada para calificar como mejora, es decir, un cambio absoluto inferior a min_delta, contar√° como no mejora.\n",
        "* `patience`: N√∫mero de √©pocas sin mejora tras las cuales se detendr√° el entrenamiento.\n",
        "* `mode`: Puede ser {\"auto\", \"min\", \"max\"}. En el modo \"min\", el entrenamiento se detendr√° cuando la cantidad supervisada haya dejado de disminuir; en el modo \"max\", se detendr√° cuando la cantidad supervisada haya dejado de aumentar; en el modo \"auto\", la direcci√≥n se deduce autom√°ticamente del nombre de la cantidad o m√©trica supervisada."
      ],
      "metadata": {
        "id": "tmZ7gGrWHtDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos y entrenamos una red neuronal. Observa el n√∫mero de neuronas en la capa de salida y la funci√≥n de activaci√≥n.\n",
        "\n",
        "Su desempe√±o tendr√° un accuracy mucho menor en el conjunto de prueba. Observa las curvas de aprendizaje, **es un caso claro de overfitting**.\n",
        "\n",
        "*El tiempo de ejecuci√≥n es de alrededor de 2 minutos*"
      ],
      "metadata": {
        "id": "oHXLYWRbKFZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "#----- Definimos el modelo -------\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#----- Entremamos el modelo ------\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1000, verbose=0)"
      ],
      "metadata": {
        "id": "TOr9VSuxFwqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluamos el modelo\n",
        "_, train_acc = model.evaluate(X_train, y_train, verbose=0) # No nos importa guardar el loss en una variable\n",
        "_, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f'Train accuracy: {round(train_acc,3)}. Validation accuracy : {round(val_acc,3)}')\n",
        "\n",
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-Ie9v0cKKyhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, usemos el callback para detener el entrenamiento en el momento adecuado"
      ],
      "metadata": {
        "id": "29gnf-X7LlHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "T_9wTBSpEPkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el *callback*. Es una clase por lo que tenemos que inicializarla con hiperpar√°metros y obtenemos un objeto."
      ],
      "metadata": {
        "id": "zz_H6xXhN3oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)"
      ],
      "metadata": {
        "id": "kF-9IFJQL57P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos usando el *callback*. Observa en cu√°ntas √©pocas detiene el entrenamiento."
      ],
      "metadata": {
        "id": "xojm6PU0N7Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                    epochs=1000, verbose=0,\n",
        "                    callbacks=[es])\n",
        "\n",
        "\n",
        "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "_, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "print('Train accuracy: %.3f. Validation accuracy : %.3f' % (train_acc, val_acc))"
      ],
      "metadata": {
        "id": "3vI2CwoeL8RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ Observa que el accuracy en la validaci√≥n mejor√≥ y en el entrenamiento empeor√≥ un poco, ¬øqu√© interpretaci√≥n le damos a esto?"
      ],
      "metadata": {
        "id": "HsyItI4iM23S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficamos el entrenamiento"
      ],
      "metadata": {
        "id": "ssQIT6_smEeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ScSHqVD1mHHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Callbacks: `ModelCheckpoint`"
      ],
      "metadata": {
        "id": "Qigch9fzoRyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este callback sirve para guardar el modelo en el momento en que comenz√≥ el overfitting y se comenz√≥ a perder accuracy en el conjunto de validaci√≥n. Algunos par√°metros importantes:\n",
        "\n",
        "* `save_best_only`: if save_best_only=True, it only saves when the model is considered the \"best\" and the latest best model according to the quantity monitored will not be overwritten. If filepath doesn't contain formatting options like {epoch} then filepath will be overwritten by each new better model.\n",
        "* `mode`: one of {'auto', 'min', 'max'}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc, this should be max, for val_loss this should be min, etc. In auto mode, the mode is set to max if the quantities monitored are 'acc' or start with 'fmeasure' and are set to min for the rest of the quantities.\n",
        "* `save_weights_only`: if True, then only the model's weights will be saved (model.save_weights(filepath)), else the full model is saved (model.save(filepath))."
      ],
      "metadata": {
        "id": "y-DDMrLJrqh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el modelo"
      ],
      "metadata": {
        "id": "h37m4BMTwO30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vjhLpU4LwPIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el callback"
      ],
      "metadata": {
        "id": "YZ_Y_5VJvw6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath = 'best_model.hdf5'\n",
        "checkpoint_best = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "callbacks = [checkpoint_best]"
      ],
      "metadata": {
        "id": "1cXjQXTAoRin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ö°**Por ahora no lo ejecutemos.** Tambi√©n podemos guardar varios modelos, con informaci√≥n sobre la √©poca y loss"
      ],
      "metadata": {
        "id": "dAtvrBRF4FFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filepath = 'my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n",
        "\n",
        "# checkpoint_all = ModelCheckpoint(filepath=filepath,\n",
        "#                              monitor='val_loss',\n",
        "#                              verbose=1,\n",
        "#                              save_best_only=True,\n",
        "#                              mode='min')\n",
        "# callbacks = [checkpoint_all]"
      ],
      "metadata": {
        "id": "FzLFoLRV4EdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos el modelo usando el callback definido previamente. Observar que, en este caso, realizar√° el entrenamiento con todas las √©pocas y s√≥lo guardar√° el m√≥delo cuando alcance un nuevo m√≠nimo en la perdida de la validaci√≥n."
      ],
      "metadata": {
        "id": "3CWLOZvww6Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100,\n",
        "                  callbacks=callbacks)"
      ],
      "metadata": {
        "id": "tKv2utyNw8KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pu1U0WGpxfhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° Leemos el modelo y realizamos predicciones con √©l"
      ],
      "metadata": {
        "id": "YHvS-OsKnICP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leemos y evaluamos usando el modelo guardado del callback anterior.\n",
        "\n",
        "**En esta parte, adem√°s, evaluamos *externamente* las m√©tricas usuales de rendimiento.**"
      ],
      "metadata": {
        "id": "Rv6mTKnWyMZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model_reloaded = load_model(filepath)\n",
        "y_pred = model_reloaded.predict(X_val)"
      ],
      "metadata": {
        "id": "rzGaSIGwxgPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "print(f\"Binary Cross Entropy: {log_loss(y_val,y_pred)}\")"
      ],
      "metadata": {
        "id": "WCr6Piojy3Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa la forma que tienen las predicciones, son probabilidades de pertenecer a la clase positiva (la clase 1). Recuerda que la √∫ltima capa tiene una activaci√≥n sigmoide que est√° en un rango $(0,1)$."
      ],
      "metadata": {
        "id": "ykj5crr3qT9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred.shape)\n",
        "y_pred[:5]"
      ],
      "metadata": {
        "id": "yh-FF8vXqZC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convirtamoslas a predicciones de clases para fin de evaluar tambi√©n usando las m√©tricas de rendimiento de clasificaci√≥n de scikit-learn (precision, accuracy, etc)."
      ],
      "metadata": {
        "id": "irP6wMziqkoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_pred_clases = np.where(y_pred>=0.5,1,0).flatten()\n",
        "print(y_pred_clases.shape)\n",
        "y_pred_clases[:5]"
      ],
      "metadata": {
        "id": "uWsZHoK6qkKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora s√≠, podemos evaluar. Recordar que, para el `roc_auc_score` necesitamos las probabilidades de pertenecer a la clase."
      ],
      "metadata": {
        "id": "hQT7mhtFrsxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, roc_auc_score\n",
        "\n",
        "print(f\"Precision Score: {precision_score(y_val,y_pred_clases)}\")\n",
        "print(f\"F1 Score: {f1_score(y_val,y_pred_clases)}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_val,y_pred)}\")"
      ],
      "metadata": {
        "id": "mP_lQh-QrscO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambi√©n podemos recuperar la entropia binaria cruzada (la funci√≥n de perdida) a partir del m√©todo `evaluate` del modelo."
      ],
      "metadata": {
        "id": "H7SdjGzi0yMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = model_reloaded.evaluate(X_val,y_val)\n",
        "\n",
        "print(f\"Validation loss and validation accuracy: {evaluation}\")"
      ],
      "metadata": {
        "id": "LRQKeJT40OxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout"
      ],
      "metadata": {
        "id": "ZOtxgHiqXHPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es f√°cil que las redes neuronales de aprendizaje profundo se sobreajusten r√°pidamente a un conjunto de datos de entrenamiento con pocos ejemplos.\n",
        "\n",
        "Se sabe que los conjuntos de redes neuronales con diferentes configuraciones de modelos reducen el sobreajuste, pero requieren el gasto computacional adicional de entrenar y mantener m√∫ltiples modelos.\n",
        "\n",
        "Se puede utilizar un √∫nico modelo para simular que se dispone de un gran n√∫mero de arquitecturas de red diferentes mediante la eliminaci√≥n aleatoria de nodos durante el entrenamiento. Esto se denomina *dropout* y ofrece un m√©todo de regularizaci√≥n muy barato desde el punto de vista computacional y notablemente eficaz para reducir el sobreajuste y mejorar el error de generalizaci√≥n en redes neuronales profundas de todo tipo.\n",
        "\n",
        "**Esta estrategia no siempre mejora el rendimiento de la red y hay opiniones divididas en cuanto a su eficacia. Sin embargo, es una t√©cnica cl√°sica del deep learning.**\n",
        "\n",
        "\n",
        "<img align=\"center\" width=\"50%\" src=\"https://github.com/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/img/dropout.png?raw=1\"/>"
      ],
      "metadata": {
        "id": "dMR-ZBJE5tvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos el dataset de [diabetes PIMA](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database) que usamos en el m√≥dulo pasado. Usamos este dataset por su tama√±o peque√±o.\n",
        "\n",
        "En este ejercicio observaremos como el dropout puede ayudar a prevenir el overfitting, aunque podr√≠a no necesariamente mejore la p√©rdida."
      ],
      "metadata": {
        "id": "YH4_e1kJ7-FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/04%20Deep%20Learning/data/diabetes.csv\"\n",
        "\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "df"
      ],
      "metadata": {
        "id": "QXffoLDE4jmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:,:8].values\n",
        "y = df.iloc[:,8].values\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ],
      "metadata": {
        "id": "MmztPKyFAeFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.875,random_state=89)\n",
        "\n",
        "print(f\"Train size: {X_train.shape[0]}\")\n",
        "print(f\"Test size: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "id": "NFc-eO5CCbWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escalaremos los datos para mejorar el rendimiento de la red. Esto, debido a la variedad en los rangos de las variables."
      ],
      "metadata": {
        "id": "dJVZ79ni8KWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "BL5iKI-P8Si3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos la imputaci√≥n de valores faltantes"
      ],
      "metadata": {
        "id": "VohRPwhvW81m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idxs_to_impute = [1,2,3,4,5]"
      ],
      "metadata": {
        "id": "_IqeIODVuZqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(missing_values=0, strategy='mean')\n",
        "X_train[:,idxs_to_impute] = imputer.fit_transform(X_train[:,idxs_to_impute])\n",
        "X_test[:,idxs_to_impute] = imputer.transform(X_test[:,idxs_to_impute])"
      ],
      "metadata": {
        "id": "3bVIyL8Esv73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_imputado = pd.DataFrame(X_train)\n",
        "df_imputado.describe()"
      ],
      "metadata": {
        "id": "92J6ykeouyao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las escalas de valores de cada features"
      ],
      "metadata": {
        "id": "gcfR-l1ZRDm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_imputado.plot.hist(subplots=True, legend=True)"
      ],
      "metadata": {
        "id": "KUt-sMfiTtXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=4, ncols=2,figsize=(16,8))\n",
        "for col, ax in zip(df_imputado.columns, axs.flatten()):\n",
        "    df_imputado[col].plot.hist(ax=ax)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "u38qIoUCRDb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-escalamos los valores para ponerlos en la misma escala"
      ],
      "metadata": {
        "id": "8Ep19A6mQ_O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scl = StandardScaler()\n",
        "X_train = scl.fit_transform(X_train)\n",
        "X_test = scl.transform(X_test)"
      ],
      "metadata": {
        "id": "LbJgqYjKroxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siendo un modelo sencillo, requerimos una arquitectura sencilla. La siguiente es una buena alternativa. No la probaremos por ahora:"
      ],
      "metadata": {
        "id": "v3PLuxJCVnTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# model = Sequential()\n",
        "\n",
        "# model.add(Dense(8, input_dim=8, activation='relu'))\n",
        "# model.add(Dense(15, activation='relu'))\n",
        "# model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "XEu4UKNYVsug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sin dropout"
      ],
      "metadata": {
        "id": "wu0Lac-5DgUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(8, input_dim=8, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "rtgHb6RGAhpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_split=0.1, epochs=50,verbose=0)"
      ],
      "metadata": {
        "id": "4yAs1Ee6Ay69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train accuracy: %.3f. Test accuracy : %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jKqs4R-5A0RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "qtOA6XvnvPjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando dropout: Efecto en el overfitting"
      ],
      "metadata": {
        "id": "UqOTwOtDoYKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos la misma arquitectura general de la red. A√±adimos dos capas de dropout, las tasas de dropout fueron seleccionadas con gridsearch."
      ],
      "metadata": {
        "id": "4xURtdsJ8bLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=8, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_do = build_model()"
      ],
      "metadata": {
        "id": "6tUFgJGHDCgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_do.fit(X_train, y_train, validation_split=0.1, epochs=50,verbose=0)"
      ],
      "metadata": {
        "id": "MDeJx-kLD7_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model_do.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_acc = model_do.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train accuracy: %.3f. Test accuracy : %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QvQDal7EEDIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_do.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "TBYXtr7RvflZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aumentemos el n√∫mero de √©pocas, **observa el efecto del hiperpar√°metro `patience`**"
      ],
      "metadata": {
        "id": "EsSL_9TB0JVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "model_do_2 = build_model()\n",
        "\n",
        "es = EarlyStopping(patience=3)\n",
        "\n",
        "history = model_do_2.fit(X_train, y_train, validation_split=0.1, epochs=200,verbose=0,\n",
        "                       callbacks=[es])"
      ],
      "metadata": {
        "id": "vNNy8eXUyuqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- evaluamos el modelo ----\n",
        "_, train_acc = model_do_2.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_acc = model_do_2.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train accuracy: %.3f. Test accuracy : %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "igHgm1x01jDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö°‚ö° Gridsearch"
      ],
      "metadata": {
        "id": "8WBFeTAfS9RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci√≥n se muestra c√≥mo realizar un gridsearch para obtener los mejores par√°metros de una red neuronal, es decir, los que producen las mejores m√©tricas. Estos par√°metros pueden ser el n√∫mero de neuronas, la tasa de dropout, las √©pocas, etc.\n",
        "\n",
        "Para poder usar el gridsearch de scikit-learn es necesario *traducir* el m√≥delo de red neuronal a un clasificador de scikit-learn. Esto lo hacemos con la clase `KerasClassifier`.\n",
        "\n",
        "**‚ö†‚ùóWarning**: Si se especifican un gran n√∫mero de par√°metros en la busqueda, esta puede tardar mucho y pueden ser penalizados en el uso de GPU en Colab. Usar con cuidado."
      ],
      "metadata": {
        "id": "xSHg3GJNsWJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seguimos con el dataset de [diabetes](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database). Resumamos el preprocesamiento en una sola celda:"
      ],
      "metadata": {
        "id": "qAA4F5s6vV5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI4YT7B23uEJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/04%20Deep%20Learning/data/diabetes.csv\"\n",
        "\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "\n",
        "X = df.iloc[:,:8].values\n",
        "y = df.iloc[:,8].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.875,random_state=89)\n",
        "\n",
        "idxs_to_impute = [1,2,3,4,5]\n",
        "imputer = SimpleImputer(missing_values=0, strategy='mean')\n",
        "X_train[:,idxs_to_impute] = imputer.fit_transform(X_train[:,idxs_to_impute])\n",
        "X_test[:,idxs_to_impute] = imputer.transform(X_test[:,idxs_to_impute])\n",
        "\n",
        "scl = StandardScaler()\n",
        "X_train = scl.fit_transform(X_train)\n",
        "X_test = scl.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es necesario crear una funci√≥n que cree el modelo, esta debe depender de los par√°metros sobre los que se quiere realizar la busqueda. Es necesario crear el modelo, compilarlo y regresarlo ya compilado.\n",
        "\n",
        "Con la finalidad de no usar muchos recursos, problemas con un modelo muy sencillo, en el cual variaremos:\n",
        "\n",
        "* El n√∫mero de neuronas en la capa oculta.\n",
        "* La activaci√≥n de la capa oculta."
      ],
      "metadata": {
        "id": "mFJFifE4t4t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def create_model(n_neurons,activation):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(n_neurons, input_dim=8, activation=activation))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "nX1esrAZ5Hpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un modelo de clasificador de scikit-learn usando la API de Keras. Esta empaqueta el m√≥delo de keras como un estimador de scikit-learn.\n",
        "\n",
        "[Documentaci√≥n](https://adriangb.com/scikeras/stable/quickstart.html#training-a-model)"
      ],
      "metadata": {
        "id": "2VVc6awqtpyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras[tensorflow]"
      ],
      "metadata": {
        "id": "lh1lXEhR3IMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss',patience=2)\n",
        "\n",
        "model = KerasClassifier(\n",
        "    create_model,\n",
        "    n_neurons=12,\n",
        "    activation='sigmoid',\n",
        "    epochs=50,\n",
        "    verbose=0,\n",
        "    callbacks=[es],\n",
        "    validation_split=0.15\n",
        ")"
      ],
      "metadata": {
        "id": "q63WO_az3FCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos la busqueda de par√°metros"
      ],
      "metadata": {
        "id": "I2pWJ0IUt1vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# ----- Definimos los par√°metros de la busqueda -----\n",
        "neurons = [10,15,20,30]\n",
        "activations = ['relu','sigmoid','tanh']\n",
        "param_grid = {'n_neurons':neurons,\n",
        "              'activation':activations}\n",
        "\n",
        "# ----- Definimos y realizamos el gridsearch\n",
        "gs = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = gs.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "W0xBQ5NT459A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos los mejores par√°metros"
      ],
      "metadata": {
        "id": "PsHQDKpitjir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Accuracy: {grid_result.best_score_} using parameters {grid_result.best_params_}\")"
      ],
      "metadata": {
        "id": "_b0qeZrE5jMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos acceder al mejor modelo y usarlo.\n",
        "\n",
        "**Observaci√≥n**:  El modelo, a√∫n cuando es una red neuronal, est√° presentado como un estimador de scikit-learn por lo que no tiene m√©todo `evaluate` (este es de keras), sino `score` (este es de sklearn)."
      ],
      "metadata": {
        "id": "raey9l5sCFM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = gs.best_estimator_\n",
        "\n",
        "best_model.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "NDRfS7kd4tj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚≠ï Pr√°ctica\n",
        "\n",
        "En esta pr√°ctica haremos regresi√≥n multi-output usando el dataset `mo_regression.csv` de la carpeta data del m√≥dulo en el repositorio.\n",
        "\n",
        "0. Divide el conjunto de datos en 80% entrenamiento y 20% prueba.\n",
        "1. Define dos modelos de red neuronal MLP para resolver esta tarea. La funci√≥n de perdida a usar ser√° 'MSE'. Un m√≥delo tendr√° una arquitectura *sencilla* y el otro, una arquitectura *compleja*.\n",
        "2. Prueba diferentes combinaciones de hiperpar√°metros para encontrar el mejor modelo que puedas en cuanto a desempe√±o en el conjunto de prueba, usando la m√©trica MAE."
      ],
      "metadata": {
        "id": "qATM0TY9Qznf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/04%20Deep%20Learning/data/mo_regression.csv\"\n",
        "\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "df"
      ],
      "metadata": {
        "id": "SCBD5wQLQ3AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e6OydNJkUx1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
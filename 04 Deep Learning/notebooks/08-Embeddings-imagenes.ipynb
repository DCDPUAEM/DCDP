{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4AL0lmumHRSU",
        "1ak9Bk_lHUB0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/notebooks/08-Embeddings-imagenes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "3U1EFEfauevM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Embeddings de Im치genes</h1>"
      ],
      "metadata": {
        "id": "EqqQ8nLF21H2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta notebook exploraremos dos modelos pre-entrenados de embeddings de imagenes basado en arquitecturas CNN.\n",
        "\n",
        "El primer modelo est치 basado en [CLIP](https://github.com/openai/CLIP), es una implementaci칩n que no necesita Pytorch ni Tensorflow y ya est치 especializada en la obtenci칩n de embeddings ([documentaci칩n](https://github.com/minimaxir/imgbeddings)).\n",
        "\n",
        "El segundo modelo es un autoencoder convolucional implementado y entrenado en una de las sesiones anteriores ([notebook](https://github.com/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/notebooks/06-Autoencoders.ipynb)). Leeremos el modelo guardado y entrenado de esa ocasi칩n.\n",
        "\n",
        "Con estos embeddings realizaremos dos tareas:\n",
        "\n",
        "* Obtenci칩n de vecinos m치s cercanos.\n",
        "* Extracci칩n de features para una tarea de clasificaci칩n."
      ],
      "metadata": {
        "id": "LeB3ahxhT4TY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero, instalamos el m칩dulo para el primer modulo."
      ],
      "metadata": {
        "id": "5Y7aAEOarDTl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmlGXumP1gD6"
      },
      "outputs": [],
      "source": [
        "!pip3 install -qq imgbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example"
      ],
      "metadata": {
        "id": "4AL0lmumHRSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtengamos el embedding de una imagen de prueba"
      ],
      "metadata": {
        "id": "p239Y3VnkIVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "id": "sjApgJQb1nkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4-4M4yG8Y38d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imgbeddings import imgbeddings\n",
        "\n",
        "embeddings_model = imgbeddings()"
      ],
      "metadata": {
        "id": "rOhg-PTc1xMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos el embedding de esta im치gen y mostramos sus primeras 5 componentes"
      ],
      "metadata": {
        "id": "g3gWkBLCj_dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = embeddings_model.to_embeddings(image)\n",
        "print(embedding[0][:5])\n",
        "print(embedding[0].shape)"
      ],
      "metadata": {
        "id": "mAN6W-X813Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vecinos m치s cercanos usando Fashion-MNIST"
      ],
      "metadata": {
        "id": "1ak9Bk_lHUB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una instancia del modelo de embeddings pre-entrando"
      ],
      "metadata": {
        "id": "pX7ZXMVrV8IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imgbeddings import imgbeddings\n",
        "\n",
        "embeddings_model = imgbeddings()"
      ],
      "metadata": {
        "id": "63MtFMQEHZ2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el dataset Fashion-MNIST y re-escalamos"
      ],
      "metadata": {
        "id": "T6ENUYFWWAWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')/255.0\n",
        "x_test = x_test.astype('float32')/255.0"
      ],
      "metadata": {
        "id": "v4bfl9BTf8lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generaremos los embeddings de unas las primeras 10 im치genes. Generar los embeddings de todo el conjunto de entrenamiento o prueba tarda alrededor de 3 horas, por esa raz칩n no lo haremos en esta notebook."
      ],
      "metadata": {
        "id": "2D_fCTnSWWCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "x_sample = x_train[:10]\n",
        "\n",
        "images = [Image.fromarray(x) for x in x_sample]\n",
        "embeddings_sample = embeddings_model.to_embeddings(images)\n",
        "embeddings_sample.shape"
      ],
      "metadata": {
        "id": "jqoAaweMWs_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargaremos los embeddings de todo el conjunto de entrenamiento y prueba, fueron generados previamente de la misma manera mostrada en la celda anterior."
      ],
      "metadata": {
        "id": "Ar3RwoqUZEwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq gdown"
      ],
      "metadata": {
        "id": "R1fvCOG7-HaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- FASHION MNIST ----\n",
        "\n",
        "!gdown 13hmH7mgvGrYYyk0QoWyTGDxv2L3BtfmA # train\n",
        "!gdown 1t5_qj0YryhNKLMuAVoLlALBmvnMT8L3M # test"
      ],
      "metadata": {
        "id": "zfuSPCDwxd-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escojamos el conjunto de entrenamiento o prueba."
      ],
      "metadata": {
        "id": "dpcG7IUnZjoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# X = x_train.copy()\n",
        "# y = y_train.copy()\n",
        "# embeddings = np.load(\"Fmnist_train_imgbeddings.npy\")\n",
        "\n",
        "X = x_test.copy()\n",
        "y = y_test.copy()\n",
        "embeddings = np.load(\"Fmnist_test_imgbeddings.npy\")\n",
        "\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "I2ZvcLJ3yU5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenmos una instancia de [`NearestNeighbors`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) para encontrar de forma **eficiente** los vecinos m치s cercanos de cualquier embedding. El uso de la m칠trica `cosine` es muy t칤pico para los embeddings generados por modelos de aprendizaje profundo."
      ],
      "metadata": {
        "id": "u5hVvr5TZnYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "num_neighbors = 3\n",
        "\n",
        "nns = NearestNeighbors(metric='cosine')\n",
        "nns.fit(embeddings)"
      ],
      "metadata": {
        "id": "XjAMOFoEE5ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecciones 4 im치genes al asar del dataset y de cada una, encontremos los 3 vecinos m치s cercanos. 쯈u칠 tan buenos son los resultados?"
      ],
      "metadata": {
        "id": "PFgJiZJ5aYCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_to_check = 4\n",
        "\n",
        "idxs_to_check = np.random.choice(list(range(X.shape[0])), num_to_check)\n",
        "\n",
        "distances, NN = nns.kneighbors(embeddings[idxs_to_check,:],\n",
        "                               num_neighbors+1,\n",
        "                               return_distance=True)\n",
        "\n",
        "print(f\"칈ndices de los vecinos m치s cercanos:\\n{NN}\")\n",
        "print(f\"Distancia a los vecinos m치s cercanos:\\n{distances}\")\n",
        "\n",
        "distances = distances[:,1:]\n",
        "NN = NN[:,1:]\n",
        "\n",
        "fig, axs = plt.subplots(num_to_check,num_neighbors+1, figsize=(4*num_neighbors, 4*num_to_check))\n",
        "for k,idx in enumerate(idxs_to_check):\n",
        "    axs[0,k].imshow(X[idx])\n",
        "    axs[0,k].axis('off')\n",
        "    for i,nn_idx in enumerate(NN[k][:num_neighbors]):\n",
        "        axs[i+1,k].imshow(X[nn_idx])\n",
        "        axs[i+1,k].set_title(f\"NN {i+1},\\ndistance:{np.round(distances[k,i],3)}\")\n",
        "        axs[i+1,k].axis('off')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "qeeRgZ5xGm38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features para la clasificaci칩n Cats vs Dogs"
      ],
      "metadata": {
        "id": "kR6RHUmPkMxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En una notebook de redes CNN realizamos la tarea de clasificaci칩n del dataset *Cats vs Dogs*. Logramos obtener un accuracy del orden de 80%.\n",
        "\n",
        "Usaremos embeddings generados con un autoencoder CNN como features para entrenar un clasificador de Machine Learning *cl치sico* y ver c칩mo se compara con el clasificador CNN antes mencionado."
      ],
      "metadata": {
        "id": "-WvhP5SSasSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargemos los modelos ya entrenados del autoencoder CNN"
      ],
      "metadata": {
        "id": "FuR6q20AbSTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1adFS_RZK9b20rTKZlC4fTDuZtOEy9np1    # ENCODER\n",
        "!gdown 1-0Y_aV5XST4GnySszgnIi5AC6xoQOdiG    # DECODER\n",
        "!gdown 1Ce3u8dwYYriLkz5OpcGn72xIQENIHZX5    # DATASET CATS VS DOGS, versi칩n reducida"
      ],
      "metadata": {
        "id": "gHf1rFV3obM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "file_name = '/content/cnn_perros_gatos-small.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as myzip:\n",
        "    myzip.extractall()\n",
        "    print('Listo')"
      ],
      "metadata": {
        "id": "AF3i4Dv4pAEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los generadores de im치genes. Observa el `batch_size`."
      ],
      "metadata": {
        "id": "uj9kKTJTg_2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dir = 'cnn_perros_gatos/train'\n",
        "test_dir = 'cnn_perros_gatos/test'\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(144, 144),\n",
        "        batch_size=1,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(144, 144),\n",
        "        batch_size=1,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "id": "e04THn4TpD2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leemos los modelos encoder y decoder ya entrenados.\n",
        "\n",
        "游댯 쯇or qu칠 tenemos el WARNING que nos muestra?"
      ],
      "metadata": {
        "id": "jdBvV4pehKo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.saving import load_model\n",
        "\n",
        "CvD_encoder = load_model(\"perros_gatos_encoder_cnn_30epochs.h5\")\n",
        "CvD_decoder = load_model(\"perros_gatos_decoder_cnn_30epochs.h5\")"
      ],
      "metadata": {
        "id": "mVnRx__G2ksU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CvD_encoder.summary()"
      ],
      "metadata": {
        "id": "qBhm4zTOaUTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CvD_decoder.summary()"
      ],
      "metadata": {
        "id": "SznXGl_Vhbwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificaci칩n"
      ],
      "metadata": {
        "id": "xsim-pc0VLZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos las features y etiqueta de clase de cada imagen. Esto lo hacemos iterando sobre el generador de im치genes, recuerda que tenemos que detener la iteraci칩n manualmente, de otra forma la iteraci칩n nunca termina.\n",
        "\n",
        "En este paso es que es importante el `batch_size` definido en el paso anterior.\n",
        "\n",
        "La ejecuci칩n dura alrededor de 6 minutos."
      ],
      "metadata": {
        "id": "DhlstXNJhqiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_codes = np.zeros((2000, 512))\n",
        "y_train = np.zeros((2000,))\n",
        "\n",
        "train_tensors = np.zeros((2000, 144, 144, 3))\n",
        "test_tensors = np.zeros((1000, 144, 144, 3))\n",
        "\n",
        "test_codes = np.zeros((1000, 512))\n",
        "y_test = np.zeros((1000,))\n",
        "\n",
        "for k,(xi,yi) in enumerate(train_generator):\n",
        "    if k < 2000:\n",
        "        train_codes[k,:] = CvD_encoder.predict(xi,verbose=0)\n",
        "        y_train[k] = yi[0]\n",
        "        train_tensors[k] = xi[0]\n",
        "    else:\n",
        "        break\n",
        "\n",
        "for k,(xi,yi) in enumerate(test_generator):\n",
        "    if k < 1000:\n",
        "        test_codes[k,:] = CvD_encoder.predict(xi,verbose=0)\n",
        "        y_test[k] = yi[0]\n",
        "        test_tensors[k] = xi[0]\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(train_codes.shape)\n",
        "print(y_train.shape)\n",
        "print(test_codes.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "ToYhiqO3V7m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de este punto, el proceso es el mismo del Machine Learning *cl치sico*"
      ],
      "metadata": {
        "id": "tUmZMV5gkQ5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el rendimiento de varios algoritmos de Machine Learning"
      ],
      "metadata": {
        "id": "P65eqg5Ykcev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clfs = [SVC(), DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier(), LogisticRegression()]\n",
        "names = [\"SVC\", \"DecisionTreeClassifier\", \"RandomForestClassifier\", \"KNeighborsClassifier\", \"LogisticRegression\"]\n",
        "\n",
        "for clf,name in zip(clfs,names):\n",
        "    clf.fit(train_codes, y_train)\n",
        "    print(f\"{name}: {clf.score(test_codes, y_test)}\")"
      ],
      "metadata": {
        "id": "kFAyqgilc7xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vecinos m치s cercanos"
      ],
      "metadata": {
        "id": "oPjAUDH-c6Ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos una imagen de prueba"
      ],
      "metadata": {
        "id": "7aK-eNwvpk9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1ofrBZ-E9JO3nAfAYKN0l2B6rsUbXwOVp"
      ],
      "metadata": {
        "id": "ejerOr1qzDlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La preprocesamos"
      ],
      "metadata": {
        "id": "1jqot4qUpm4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import utils\n",
        "\n",
        "img_path = '/content/cat.png'\n",
        "test_image = utils.load_img(img_path, target_size=(144, 144))\n",
        "test_image = utils.img_to_array(test_image)\n",
        "test_image = test_image.reshape(1,144,144,3)\n",
        "test_image /= 255.\n",
        "test_image.shape"
      ],
      "metadata": {
        "id": "vxCxfmyYsRLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La mostramos"
      ],
      "metadata": {
        "id": "1918vV9jpogE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(test_image[0])\n",
        "plt.axis('Off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X2hYEdaS0Kxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos el embedding de la imagen de arriba, es decir, obtenemos su codificaci칩n latente mediante el codificador pre-entrenado."
      ],
      "metadata": {
        "id": "EhtItNaAYZoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_code = CvD_encoder.predict(test_image)\n",
        "print(test_image_code.shape)\n",
        "test_image_code = test_image_code.reshape(512,) # Hacemos el reshape adecuado\n",
        "test_image_code.shape"
      ],
      "metadata": {
        "id": "iTerF_81VLPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incializamos y entrenamos una instancia de `NearestNeighbors`, una vez m치s usando la m칠trica de la similitud coseno."
      ],
      "metadata": {
        "id": "KkZmqFC1Ynpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "num_neighbors = 3\n",
        "nns = NearestNeighbors(metric='cosine')\n",
        "nns.fit(train_codes)"
      ],
      "metadata": {
        "id": "Bs-vAzQXdixd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos las 3 im치genes m치s similares a la im치gen de muestra, junto con sus distancias en el espacio de codificaci칩n latente."
      ],
      "metadata": {
        "id": "-T2Rkp-RixnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "distances, NN = nns.kneighbors([test_image_code],\n",
        "                               num_neighbors+1,\n",
        "                               return_distance=True)\n",
        "\n",
        "distances = distances[:,:-1]\n",
        "NN = NN[:,:-1]\n",
        "\n",
        "print(f\"칈ndices de los vecinos m치s cercanos:\\n{NN}\")\n",
        "print(f\"Distancia a los vecinos m치s cercanos:\\n{distances}\")\n",
        "\n",
        "fig, axs = plt.subplots(1,num_neighbors+1, figsize=(4*num_neighbors, 4))\n",
        "axs[0].imshow(test_image[0])\n",
        "axs[0].axis('off')\n",
        "for i,nn_idx in enumerate(NN[0]):\n",
        "    axs[i+1].imshow(train_tensors[nn_idx])\n",
        "    axs[i+1].set_title(f\"NN {i+1},\\ndistance:{round(distances[0,i],2)}\")\n",
        "    axs[i+1].axis('off')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TmKEQcA2dJDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "游댯 Hay algunas im치genes no tan parecidas, 쯤u칠 factores pueden afectar el rendimiento de esta tarea?"
      ],
      "metadata": {
        "id": "AB1S6dlfjbzu"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/notebooks/08-NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "ZKki9UVLeTUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>NLP - Analisis de Sentimientos con Deep Learning<h1>\n",
        "\n",
        "En esta notebook haremos una tarea de analisis de sentimientos (clasificacion) en textos de reviews de peliculas usando Deep Learning.\n",
        "\n",
        "Usaremos varios enfoques involucrando solamente redes MLP y modelos pre-entrenados.\n",
        "\n",
        "Una de las principales dificultades de este dataset es su tamano y la limpieza del texto."
      ],
      "metadata": {
        "id": "VSTdKE-tsBrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBH72rTBl2ZT"
      },
      "outputs": [],
      "source": [
        "!gdown 18kGdlhOiQNS61wUK7uPbdquKL3XJrgzf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('IMDB.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "qRowOp6ysAD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una funcion para limpiar el texto, solo quitamos espacios dobles, etiquetas HTML y caracteres especiales."
      ],
      "metadata": {
        "id": "X7Tq1NH2c5Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import html\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Limpia texto de reviews manteniendo puntuación y stopwords\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Decodificar entidades HTML\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Remover etiquetas HTML y <br />\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "\n",
        "    # Remover caracteres de control y caracteres especiales problemáticos\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', ' ', text)\n",
        "\n",
        "    # Normalizar espacios en blanco múltiples\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remover espacios al inicio y final\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-_ZQTYFI7P5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in df['review'].sample(5):\n",
        "    print(text)\n",
        "    print(clean_text(text))\n",
        "    print()"
      ],
      "metadata": {
        "id": "3nAjMRK-7s30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review'] = df['review'].apply(clean_text)\n",
        "df"
      ],
      "metadata": {
        "id": "6YWmjFtP7qIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['sentiment'] = le.fit_transform(df['sentiment'])\n",
        "display(df)\n",
        "y = df['sentiment'].values"
      ],
      "metadata": {
        "id": "TBJ2PRALs6SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = list(df['review'].values)"
      ],
      "metadata": {
        "id": "Yh8Orum1taEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_docs, test_docs, y_train, y_test = train_test_split(docs, y, test_size=0.2, random_state=462)\n",
        "\n",
        "print(f'Train size: {len(train_docs)}')\n",
        "print(f'Test size: {len(test_docs)}')"
      ],
      "metadata": {
        "id": "cpH7xitFsfIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train = vectorizer.fit_transform(train_docs)\n",
        "X_test = vectorizer.transform(test_docs)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "2jEgwCOss3WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 1: MLP + TF-IDF\n",
        "\n",
        "En este primer modelo tomaremos las features TF-IDF del texto y las usaremos como entrada de una red MLP."
      ],
      "metadata": {
        "id": "neGoMPM3uw8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos reduccion de dimensionalidad para facilitar el entrenamiento de la red MLP"
      ],
      "metadata": {
        "id": "xwQIz7rKQDm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=300)\n",
        "X_train = pca.fit_transform(X_train.toarray())\n",
        "X_test = pca.transform(X_test.toarray())\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "GDtaBE_lxEN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una arquitectura MLP que recibe las features de TF-IDF"
      ],
      "metadata": {
        "id": "OF78jW-lQKH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(X_train.shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "oFwv42WvurUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min',\n",
        "                   restore_best_weights=True,\n",
        "                   verbose=1,\n",
        "                   patience=3)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[es])"
      ],
      "metadata": {
        "id": "SjQLWUmFvGio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'],label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'],label='val accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'],label='train loss')\n",
        "plt.plot(history.history['val_loss'],label='val loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n-ErG5H7vTJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "RfK5Vv4fyTIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 2: MLP + Embedding Layer + TF-IDF\n",
        "\n",
        "En este modelo usaremos las features TF-IDF como entrada a una red MLP, usaremos una capa Embedding, la cual asigna un vector denso a cada palabra. Estas representaciones vectoriales pasan a las capas densas ocultas de la red MLP.\n",
        "\n",
        "Ademas usaremos la capa GlobalAveragePooling1D, la cual toma las representaciones de todas las posiciones de la secuencia (tokens) y calcula el promedio global, reduciendo una matriz de dimensiones (batch_size, sequence_length, features) a (batch_size, features)."
      ],
      "metadata": {
        "id": "2ilD2zNzyyV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui realizamos la conversión de texto a secuencias numéricas para el procesamiento con redes neuronales.\n",
        "\n",
        "* Primero, el Tokenizer crea un vocabulario con las 10,000 palabras más frecuentes del conjunto de entrenamiento, asignando a cada palabra un índice numérico único.\n",
        "* Luego convierte cada documento de texto en una secuencia de números, donde cada número representa el índice de una palabra en el vocabulario.\n",
        "* Finalmente, pad_sequences estandariza todas las secuencias a una longitud fija de 300 elementos, truncando las secuencias más largas y rellenando con ceros las más cortas.\n",
        "\n",
        "El resultado son matrices numéricas donde cada fila representa un documento y cada columna una posición en la secuencia"
      ],
      "metadata": {
        "id": "oJgXazNiRMRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_docs)\n",
        "X_train = pad_sequences(train_sequences, maxlen=300)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_docs)\n",
        "X_test = pad_sequences(test_sequences, maxlen=300)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "JRa-0IVqyUeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input, Embedding, GlobalAveragePooling1D, BatchNormalization\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Embedding(input_dim=5000, output_dim=32),  # Embedding muy pequeño\n",
        "    GlobalAveragePooling1D(),\n",
        "\n",
        "    # MLP minimalista\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.6),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Compilación\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qdA3YWhizrP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min',\n",
        "                   restore_best_weights=True,\n",
        "                   verbose=1,\n",
        "                   patience=3)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[es])"
      ],
      "metadata": {
        "id": "cO7ZqNs30pU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'],label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'],label='val accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'],label='train loss')\n",
        "plt.plot(history.history['val_loss'],label='val loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RBo8V_zL0zCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "QZW7CvUH8_Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 3: Embeddings Modelo Pre-entrenado\n",
        "\n",
        "El enfoque de embeddings pre-entrenados con MLP consiste en utilizar modelos de lenguaje entrenados en grandes corpus de texto para convertir documentos en vectores de características numéricas de alta dimensionalidad. Estos embeddings capturan relaciones semánticas y sintácticas del lenguaje que son difíciles de obtener con técnicas tradicionales como bag-of-words o TF-IDF. Una vez obtenidos los embeddings, se alimentan a una red neuronal multicapa que aprende a mapear estas representaciones vectoriales a las clases objetivo.\n",
        "\n",
        "Este método separa la extracción de características del proceso de clasificación, permitiendo aprovechar el conocimiento lingüístico de modelos entrenados en millones de documentos mientras se mantiene la simplicidad de arquitecturas de clasificación tradicionales.\n",
        "\n",
        "all-MiniLM-L6-v2 es un modelo de transformador compacto basado en la arquitectura MiniLM que genera embeddings de texto de 384 dimensiones. Fue entrenado mediante destilación de conocimiento a partir de modelos más grandes, manteniendo un rendimiento competitivo con solo 22 millones de parámetros. El modelo utiliza 6 capas de atención y está optimizado para tareas de recuperación de información y similitud semántica, procesando secuencias de hasta 512 tokens y produciendo representaciones vectoriales que capturan el significado contextual del texto de entrada."
      ],
      "metadata": {
        "id": "gvuwwjW04xDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⌚ Tarda alrededor de 2 minutos"
      ],
      "metadata": {
        "id": "EI-JFCuORbIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Obtener embeddings\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "train_embeddings = model.encode(train_docs)\n",
        "test_embeddings = model.encode(test_docs)\n",
        "\n",
        "# Normalizar embeddings (importante para redes neuronales)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "print(f\"Embeddings shape: {train_embeddings_scaled.shape}\")"
      ],
      "metadata": {
        "id": "RdvKcsiQ4wx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP con embeddings pre-entrenados\n",
        "mlp_model = Sequential([\n",
        "        Input(shape=(train_embeddings_scaled.shape[1],)),\n",
        "        Dense(512, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "mlp_model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "mlp_model.summary()"
      ],
      "metadata": {
        "id": "jT1M8MiW5Jjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Entrenar\n",
        "history = mlp_model.fit(\n",
        "    train_embeddings_scaled, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "G2cuJwp85TBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'],label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'],label='val accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'],label='train loss')\n",
        "plt.plot(history.history['val_loss'],label='val loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tbB_aDgQ6Cas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_model.evaluate(test_embeddings_scaled, y_test)"
      ],
      "metadata": {
        "id": "YKry6-1h5XDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparamos con un clasificador de Machine Learning clasico"
      ],
      "metadata": {
        "id": "3-ajRT2oTR5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(random_state=42)\n",
        "lr.fit(train_embeddings_scaled, y_train)\n",
        "lr_acc = lr.score(test_embeddings_scaled, y_test)\n",
        "print(f\"Logistic Regression - Test Accuracy: {lr_acc:.4f}\")"
      ],
      "metadata": {
        "id": "fJTglgjc5YEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 4: Finetuning con transformers\n",
        "\n",
        "Finalemente realizaremos fine-tuning de un modelo pre-entrenado DistilBERT para clasificación de sentimientos en este dataset.\n",
        "\n",
        "El fine-tuning consiste en tomar un modelo que ya aprendió representaciones generales del lenguaje durante su pre-entrenamiento y adaptarlo a una tarea específica mediante entrenamiento adicional con datos etiquetados.\n",
        "\n",
        "* Las principales ventajas incluyen: requerir menos datos y tiempo de entrenamiento comparado con entrenar desde cero, aprovechar el conocimiento previo del modelo, y obtener mejor rendimiento en tareas específicas.\n",
        "\n",
        "* Las desventajas son: dependencia de la calidad del modelo base, riesgo de overfitting con datasets pequeños, y la necesidad de recursos computacionales (GPU) para un entrenamiento eficiente.\n",
        "\n",
        "Utilizaremos DistilBERT por ser una versión compacta de BERT que mantiene el 97% de su rendimiento pero es 40% más pequeño y rápido, ideal para demostraciones prácticas."
      ],
      "metadata": {
        "id": "SUm6pN7GU9Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "tXGpiw1hYYnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limitaremos los textos con los que probaremos el modelo"
      ],
      "metadata": {
        "id": "kidefPShaLzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "# Tokenizar datos\n",
        "inputs = tokenizer(train_docs, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Crear dataset dict\n",
        "train_dataset = {\n",
        "    'input_ids': inputs['input_ids'],\n",
        "    'attention_mask': inputs['attention_mask'],\n",
        "    'labels': torch.tensor(y_train, dtype=torch.long)\n",
        "}"
      ],
      "metadata": {
        "id": "ezO_VJJJVFzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DictDataset(Dataset):\n",
        "    def __init__(self, data_dict):\n",
        "        self.data = data_dict\n",
        "    def __len__(self):\n",
        "        return len(self.data['labels'])\n",
        "    def __getitem__(self, idx):\n",
        "        return {k: v[idx] for k, v in self.data.items()}\n",
        "\n",
        "train_dataset = DictDataset(train_dataset)"
      ],
      "metadata": {
        "id": "1aKdB7kFW7d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El entrenamiento dura alrededor de 8 minutos"
      ],
      "metadata": {
        "id": "yWPS8SSdddrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./model',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,  # Batch más pequeño\n",
        "    save_steps=10_000,\n",
        "    logging_steps=100,\n",
        "    report_to=[],  # Sin wandb\n",
        "    run_name=\"training\",\n",
        "    dataloader_pin_memory=False,  # Menos memoria\n",
        "    gradient_accumulation_steps=4,  # Simular batch_size=16\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./modelo\")\n",
        "\n",
        "device = next(model.parameters()).device"
      ],
      "metadata": {
        "id": "7DPmb35uY5YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una funcion para predecir la clase de un texto"
      ],
      "metadata": {
        "id": "1w0j_sMXZdTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predecir_lotes(textos, batch_size=8):\n",
        "    all_predictions = []\n",
        "    for i in range(0, len(textos), batch_size):\n",
        "        batch_texts = textos[i:i+batch_size]\n",
        "        batch_inputs = tokenizer(batch_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch_inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "            all_predictions.extend(predictions)\n",
        "\n",
        "    return np.array(all_predictions)"
      ],
      "metadata": {
        "id": "C-4K-qtvap31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model.eval()\n",
        "predictions = predecir_lotes(test_docs)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "EC9QhlgOatZa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
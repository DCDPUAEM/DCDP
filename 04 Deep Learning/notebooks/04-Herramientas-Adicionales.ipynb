{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP_2022/blob/main/04%20Deep%20Learning/notebooks/04-Herramientas-Adicionales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "n-VKWu5-95LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Herramientas Adicionales</h1>\n",
        "\n",
        "El objetivo de esta notebook es mostrar algunas herramientas adicionales para mejorar el entrenamiento y/o desempe√±o de redes neuronales. En particular, veremos:\n",
        "\n",
        "* Callbacks\n",
        "    * Early Stopping\n",
        "    * Checkpoint\n",
        "    * Reduce on Plateu\n",
        "* Dropout\n",
        "* Batch Normalization\n",
        "* Gridsearch\n",
        "\n",
        "Adem√°s, realizaremos ejemplos de clasificaci√≥n binaria."
      ],
      "metadata": {
        "id": "QnpFmzTeM5Lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recuerda la simbolog√≠a de las secciones:\n",
        "\n",
        "* üîΩ Esta secci√≥n no forma parte del proceso usual de Machine Learning. Es una exploraci√≥n did√°ctica de alg√∫n aspecto del funcionamiento del algoritmo.\n",
        "* ‚ö° Esta secci√≥n incluye t√©cnicas m√°s avanzadas destinadas a optimizar o profundizar en el uso de los algoritmos.\n",
        "* ‚≠ï Esta secci√≥n contiene un ejercicio o pr√°ctica a realizar. A√∫n si no se establece una fecha de entrega, es muy recomendable realizarla para practicar conceptos clave de cada tema."
      ],
      "metadata": {
        "id": "andCwrj_XJgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ö° De esta forma podemos verificar que tenemos una GPU:"
      ],
      "metadata": {
        "id": "DnX6pDkyT6vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('GPU presente en: {}'.format(tf.test.gpu_device_name()))"
      ],
      "metadata": {
        "id": "cBYEZQgDT6_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks"
      ],
      "metadata": {
        "id": "ZCB9b3aRwB91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un *callback* es un objeto que puede realizar acciones en varias etapas del entrenamiento (por ejemplo, al inicio o al final de una √©poca, antes o despu√©s de un *batch*, etc.).\n",
        "\n",
        "Puedes usar *callbacks* para:\n",
        "\n",
        "* Escribir los registros de TensorBoard despu√©s de cada lote de entrenamiento para monitorizar tus m√©tricas\n",
        "* Guardar peri√≥dicamente tu modelo en el disco\n",
        "* Hacer un *early stopping*.\n",
        "* Obtener una visi√≥n de los estados internos y las estad√≠sticas de un modelo durante el entrenamiento.\n",
        "\n",
        "Podemos consultar la lista completa de callbacks en https://keras.io/api/callbacks/"
      ],
      "metadata": {
        "id": "26r9vfsUMjM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ilustrar algunos callbacks, y otro tipo de capas, consideremos el siguiente ejemplo. Entrenaremos una red neuronal MLP para la tarea de **clasificaci√≥n multiclase** en el siguiente dataset Fashion MNIST."
      ],
      "metadata": {
        "id": "7WPQLIByweBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "EuxPZw8Dy4wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Label | Clase               |\n",
        "|-------|---------------------|\n",
        "| 0     | T-shirt/top         |\n",
        "| 1     | Trouser             |\n",
        "| 2     | Pullover            |\n",
        "| 3     | Dress               |\n",
        "| 4     | Coat                |\n",
        "| 5     | Sandal              |\n",
        "| 6     | Shirt               |\n",
        "| 7     | Sneaker             |\n",
        "| 8     | Bag                 |\n",
        "| 9     | Ankle boot          |"
      ],
      "metadata": {
        "id": "e5LILWRw0f-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "idxs = np.random.choice(X_train.shape[0],5,replace=False)\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=5,figsize=(12,6))\n",
        "for idx, ax in zip(idxs, axs.flatten()):\n",
        "    ax.imshow(X_train[idx], cmap='gray')\n",
        "    ax.set_title(f\"Label: {y_train[idx]}\")\n",
        "    ax.axis('off')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "sRBhkBzRc2zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,train_size=0.8,random_state=199)"
      ],
      "metadata": {
        "id": "DYPQ29la1PkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "nBU7jsUgzW6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels, conteos = np.unique(y_train,return_counts=True)\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(labels,conteos)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "usa-ry5MzOWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_val_cat = to_categorical(y_val)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "print(f\"y_train_cat shape: {y_train_cat.shape}\")\n",
        "print(f\"y_val_cat shape: {y_val_cat.shape}\")\n",
        "print(f\"y_test_cat shape: {y_test_cat.shape}\")"
      ],
      "metadata": {
        "id": "gM-smFJq1b_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definamos una funci√≥n para crear el modelo base con el que estaremos experimentando"
      ],
      "metadata": {
        "id": "jZ-PWyxu9xI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "\n",
        "\n",
        "def build_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "\n",
        "    #---- Completa la capa de salida -----\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    #-------------------------------------\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "SMQ3Ta2x9w98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Callbacks: `EarlyStopping`"
      ],
      "metadata": {
        "id": "6sEU7Qa-XI6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El [`EarlyStopping`](https://keras.io/api/callbacks/early_stopping/) es un *callback* que nos permite detener el entrenamiento para evitar el overfitting. El callback monitorea el entrenamiento y lo detiene en el momento en el que una m√©trica (o perdida) deja de mejorar.\n",
        "\n",
        "Algunos de los principales hiperpar√°metros son:\n",
        "\n",
        "* `monitor`: Cantidad a controlar.\n",
        "* `min_delta`: Cambio m√≠nimo en la cantidad monitorizada para calificar como mejora, es decir, un cambio absoluto inferior a min_delta, contar√° como no mejora.\n",
        "* `patience`: N√∫mero de √©pocas sin mejora tras las cuales se detendr√° el entrenamiento.\n",
        "* `mode`: Puede ser {\"auto\", \"min\", \"max\"}. En el modo \"min\", el entrenamiento se detendr√° cuando la cantidad supervisada haya dejado de disminuir; en el modo \"max\", se detendr√° cuando la cantidad supervisada haya dejado de aumentar; en el modo \"auto\", la direcci√≥n se deduce autom√°ticamente del nombre de la cantidad o m√©trica supervisada.\n",
        "* `restore_best_weights`: Indica si se restauran los pesos del modelo a la √©poca con el mejor valor en la cantidad monitoreada."
      ],
      "metadata": {
        "id": "tmZ7gGrWHtDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos y entrenamos una red neuronal. Observa el n√∫mero de neuronas en la capa de salida y la funci√≥n de activaci√≥n.\n",
        "\n",
        "Su desempe√±o tendr√° un accuracy mucho menor en el conjunto de prueba. Observa las curvas de aprendizaje, **es un caso claro de overfitting**.\n",
        "\n",
        "*El tiempo de ejecuci√≥n es de alrededor de 4 minutos*"
      ],
      "metadata": {
        "id": "oHXLYWRbKFZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "\n",
        "input_shape = X_train[0].shape # Extraemos el shape de cualquiera de los\n",
        "print(f\"Input shape: {input_shape}\")\n",
        "\n",
        "model = build_model(input_shape)\n",
        "\n",
        "#----- Entremamos el modelo ------\n",
        "history = model.fit(X_train, y_train_cat,\n",
        "                    validation_data=(X_val, y_val_cat),\n",
        "                    epochs=35\n",
        "                    )"
      ],
      "metadata": {
        "id": "TOr9VSuxFwqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluamos el modelo\n",
        "_, train_acc = model.evaluate(X_train, y_train_cat, verbose=0) # No nos importa guardar el loss en una variable\n",
        "_, val_acc = model.evaluate(X_val, y_val_cat, verbose=0)\n",
        "print(f'Train accuracy: {round(train_acc,3)}. Validation accuracy : {round(val_acc,3)}')\n",
        "\n",
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-Ie9v0cKKyhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, usemos el callback para detener el entrenamiento en el momento adecuado.\n",
        "\n",
        "Definimos el *callback*. Es una clase por lo que tenemos que inicializarla con hiperpar√°metros y obtenemos un objeto."
      ],
      "metadata": {
        "id": "29gnf-X7LlHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "callback_es = EarlyStopping(monitor='val_loss',\n",
        "                   mode='min',\n",
        "                   verbose=1,\n",
        "                   patience=3,\n",
        "                   restore_best_weights=True\n",
        "                   )"
      ],
      "metadata": {
        "id": "kF-9IFJQL57P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos usando el *callback*. Observa en cu√°ntas √©pocas detiene el entrenamiento."
      ],
      "metadata": {
        "id": "xojm6PU0N7Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(input_shape)\n",
        "\n",
        "history = model.fit(X_train, y_train_cat,\n",
        "                    validation_data=(X_val, y_val_cat),\n",
        "                    epochs=35,\n",
        "                    # verbose=0,\n",
        "                    callbacks=[callback_es]\n",
        "                    )\n",
        "\n",
        "\n",
        "_, train_acc = model.evaluate(X_train, y_train_cat, verbose=0) # Observar qu√© est√° pasando aqu√≠\n",
        "_, val_acc = model.evaluate(X_val, y_val_cat, verbose=0)\n",
        "print(f\"Train accuracy: {train_acc}.\\nValidation accuracy: {val_acc}\")"
      ],
      "metadata": {
        "id": "3vI2CwoeL8RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ Observa que el accuracy en la validaci√≥n mejor√≥ y en el entrenamiento empeor√≥ un poco, ¬øqu√© interpretaci√≥n le damos a esto?"
      ],
      "metadata": {
        "id": "HsyItI4iM23S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficamos el entrenamiento"
      ],
      "metadata": {
        "id": "ssQIT6_smEeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ScSHqVD1mHHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Callbacks: `ModelCheckpoint`"
      ],
      "metadata": {
        "id": "Qigch9fzoRyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este callback sirve para guardar el modelo en el momento en que comenz√≥ el overfitting y se comenz√≥ a perder accuracy en el conjunto de validaci√≥n. Algunos par√°metros importantes:\n",
        "\n",
        "* `save_best_only`: if save_best_only=True, it only saves when the model is considered the \"best\" and the latest best model according to the quantity monitored will not be overwritten. If filepath doesn't contain formatting options like {epoch} then filepath will be overwritten by each new better model.\n",
        "* `mode`: one of {'auto', 'min', 'max'}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc, this should be max, for val_loss this should be min, etc. In auto mode, the mode is set to max if the quantities monitored are 'acc' or start with 'fmeasure' and are set to min for the rest of the quantities.\n",
        "* `save_weights_only`: if True, then only the model's weights will be saved (model.save_weights(filepath)), else the full model is saved (model.save(filepath))."
      ],
      "metadata": {
        "id": "y-DDMrLJrqh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el modelo"
      ],
      "metadata": {
        "id": "h37m4BMTwO30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(input_shape)"
      ],
      "metadata": {
        "id": "vjhLpU4LwPIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el callback"
      ],
      "metadata": {
        "id": "YZ_Y_5VJvw6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath = 'best_model.keras'\n",
        "callback_cp_best = ModelCheckpoint(\n",
        "                            filepath=filepath,\n",
        "                            monitor='val_loss',\n",
        "                            verbose=1,\n",
        "                            save_best_only=True,\n",
        "                            mode='min'\n",
        "                            )\n",
        "\n",
        "callbacks = [callback_cp_best]"
      ],
      "metadata": {
        "id": "1cXjQXTAoRin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambi√©n podemos combinar callbacks"
      ],
      "metadata": {
        "id": "cNI6nGtuFSY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# callbacks = [callback_cp_best,callback_es]"
      ],
      "metadata": {
        "id": "RFlucUWWFSPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ö°**Por ahora no lo ejecutemos.** Tambi√©n podemos guardar varios modelos, con informaci√≥n sobre la √©poca y loss"
      ],
      "metadata": {
        "id": "dAtvrBRF4FFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filepath = 'my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.keras'\n",
        "\n",
        "# callback_cp_all = ModelCheckpoint(filepath=filepath,\n",
        "#                              monitor='val_loss',\n",
        "#                              verbose=1,\n",
        "#                              save_best_only=True,\n",
        "#                              mode='min')\n",
        "# callbacks = [callback_cp_all]"
      ],
      "metadata": {
        "id": "FzLFoLRV4EdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos el modelo usando el callback definido previamente. Observar que, en este caso, realizar√° el entrenamiento con todas las √©pocas y s√≥lo guardar√° el m√≥delo cuando alcance un nuevo m√≠nimo en la perdida de la validaci√≥n."
      ],
      "metadata": {
        "id": "3CWLOZvww6Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train_cat,\n",
        "                    validation_data=(X_val, y_val_cat),\n",
        "                    epochs=25,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "tKv2utyNw8KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pu1U0WGpxfhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° Leemos el modelo y realizamos predicciones con √©l"
      ],
      "metadata": {
        "id": "YHvS-OsKnICP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leemos y evaluamos usando el modelo guardado del callback anterior.\n",
        "\n",
        "**En esta parte, adem√°s, evaluamos *externamente* las m√©tricas usuales de rendimiento.**"
      ],
      "metadata": {
        "id": "Rv6mTKnWyMZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "filepath = '/content/best_model.keras'\n",
        "\n",
        "model_reloaded = load_model(filepath)\n",
        "y_pred_cat = model_reloaded.predict(X_val)"
      ],
      "metadata": {
        "id": "rzGaSIGwxgPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa la forma que tienen las predicciones, son probabilidades de pertenecer a la clase positiva (la clase 1). Recuerda que la √∫ltima capa tiene una activaci√≥n sigmoide que est√° en un rango $(0,1)$."
      ],
      "metadata": {
        "id": "ykj5crr3qT9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred_cat.shape)\n",
        "np.round(y_pred_cat[:5],3)"
      ],
      "metadata": {
        "id": "yh-FF8vXqZC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convirtamoslas a predicciones de clases para fin de evaluar tambi√©n usando las m√©tricas de rendimiento de clasificaci√≥n de scikit-learn (precision, accuracy, etc)."
      ],
      "metadata": {
        "id": "irP6wMziqkoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_pred = np.argmax(y_pred_cat,axis=1)\n",
        "print(y_pred.shape)\n",
        "y_pred[:5]"
      ],
      "metadata": {
        "id": "uWsZHoK6qkKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora s√≠, podemos evaluar. Recordar que, para el `roc_auc_score` necesitamos las probabilidades de pertenecer a la clase."
      ],
      "metadata": {
        "id": "hQT7mhtFrsxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_val,y_pred)}\")\n",
        "print(f\"F1 Score: {f1_score(y_val,y_pred,average='macro')}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_val,y_pred_cat,multi_class='ovr')}\")"
      ],
      "metadata": {
        "id": "mP_lQh-QrscO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout"
      ],
      "metadata": {
        "id": "ZOtxgHiqXHPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es f√°cil que las redes neuronales de aprendizaje profundo se sobreajusten r√°pidamente a un conjunto de datos de entrenamiento con pocos ejemplos.\n",
        "\n",
        "Se sabe que los conjuntos de redes neuronales con diferentes configuraciones de modelos reducen el sobreajuste, pero requieren el gasto computacional adicional de entrenar y mantener m√∫ltiples modelos.\n",
        "\n",
        "Se puede utilizar un √∫nico modelo para simular que se dispone de un gran n√∫mero de arquitecturas de red diferentes mediante la eliminaci√≥n aleatoria de nodos durante el entrenamiento. Esto se denomina *dropout* y ofrece un m√©todo de regularizaci√≥n muy barato desde el punto de vista computacional y notablemente eficaz para reducir el sobreajuste y mejorar el error de generalizaci√≥n en redes neuronales profundas de todo tipo.\n",
        "\n",
        "**Esta estrategia no siempre mejora el rendimiento de la red y hay opiniones divididas en cuanto a su eficacia. Sin embargo, es una t√©cnica cl√°sica del deep learning.**\n",
        "\n",
        "\n",
        "<img align=\"center\" width=\"50%\" src=\"https://github.com/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/img/dropout.png?raw=1\"/>\n",
        "\n",
        "[Art√≠culo donde se propuso](https://arxiv.org/abs/1207.0580)\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "El Dropout es como estudiar en grupo vs. estudiar siempre con los mismos amigos\n",
        "\n",
        "\n",
        "* Sin Dropout (Overfitting): Imagina que siempre estudias con el mismo grupo de 3 amigos muy inteligentes. Te acostumbras tanto a sus formas de explicar y resolver problemas que:\n",
        "\n",
        " * En casa, con ellos, resuelves todo perfectamente\n",
        " * Pero en el examen real (solo), te bloqueas porque no est√°n tus amigos para ayudarte\n",
        " * Te volviste demasiado dependiente de ellos\n",
        "\n",
        "* Con Dropout: Ahora imagina que cada d√≠a de estudio, aleatoriamente algunos de tus amigos no pueden venir:\n",
        "\n",
        " * Te vuelves m√°s independiente y vers√°til\n",
        " * Aprendes a resolver problemas sin depender de personas espec√≠ficas\n",
        " * En el examen real, rindes mejor porque no dependes de nadie m√°s\n",
        " * Desarrollas tus propias estrategias"
      ],
      "metadata": {
        "id": "dMR-ZBJE5tvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sin dropout"
      ],
      "metadata": {
        "id": "wu0Lac-5DgUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout, Input, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "def build_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    #---- Completa la capa de salida -----\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    #-------------------------------------\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "OwJsrGcZk2ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = X_train[0].shape # Extraemos el shape de cualquiera de los\n",
        "print(f\"Input shape: {input_shape}\")\n",
        "\n",
        "model = build_model(input_shape)\n",
        "\n",
        "history = model.fit(X_train, y_train_cat,\n",
        "                    validation_data=(X_val, y_val_cat),\n",
        "                    epochs=30,\n",
        "                    # verbose=0\n",
        "                    )"
      ],
      "metadata": {
        "id": "CW_ar5O5IGDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(X_train, y_train_cat)\n",
        "_, test_acc = model.evaluate(X_test, y_test_cat)\n",
        "print(f\"Train accuracy: {round(train_acc,3)}\\nTest accuracy : {round(test_acc,3)}\")\n",
        "\n",
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jKqs4R-5A0RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando dropout: Efecto en el overfitting"
      ],
      "metadata": {
        "id": "UqOTwOtDoYKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos la misma arquitectura general de la red. A√±adimos dos capas de dropout, las tasas de dropout fueron seleccionadas con gridsearch."
      ],
      "metadata": {
        "id": "4xURtdsJ8bLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout, Input, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "def build_model_droput(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))  # Dropout m√°s agresivo\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.4))  # Dropout en cada capa densa\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    #---- Completa la capa de salida -----\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    #-------------------------------------\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "6tUFgJGHDCgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = X_train[0].shape # Extraemos el shape de cualquiera de los\n",
        "\n",
        "model_do = build_model_droput(input_shape=input_shape)\n",
        "\n",
        "history = model_do.fit(X_train, y_train_cat,\n",
        "                        validation_data=(X_val, y_val_cat),\n",
        "                        epochs=25)"
      ],
      "metadata": {
        "id": "MDeJx-kLD7_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model_do.evaluate(X_train, y_train_cat)\n",
        "_, test_acc = model_do.evaluate(X_test, y_test_cat)\n",
        "print(f\"Train accuracy: {round(train_acc,3)}\\nTest accuracy : {round(test_acc,3)}\")\n",
        "\n",
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QvQDal7EEDIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Normalization\n",
        "\n",
        "Es una t√©cnica que normaliza las salidas de cada capa de la red, ajust√°ndolos para que tengan media 0 y varianza 1 por cada lote de datos. Esto acelera el entrenamiento, evita gradientes inestables y reduce la necesidad de ajustes finos en la inicializaci√≥n. Adem√°s, introduce par√°metros aprendibles para mantener la flexibilidad del modelo.\n",
        "\n",
        "* Velocidad de Convergencia:\n",
        " * Con BatchNorm: El modelo aprende mucho m√°s r√°pido\n",
        " * Sin BatchNorm: Convergencia m√°s lenta, especialmente en redes profundas\n",
        "\n",
        "* Estabilidad del Entrenamiento:\n",
        " * Con BatchNorm: Curvas de loss m√°s suaves, menos \"saltos\"\n",
        " * Sin BatchNorm: Loss m√°s err√°tico, puede tener picos y valles\n",
        "\n",
        "* Learning Rate:\n",
        " * Con BatchNorm: Puedes usar learning rates m√°s altos (ej: 0.01 vs 0.001)\n",
        " * Sin BatchNorm: Necesitas learning rates m√°s conservadores\n",
        "\n",
        "* Accuracy Final:\n",
        " * Con BatchNorm: Generalmente mejor accuracy final\n",
        " * Sin BatchNorm: Puede alcanzar resultados similares, pero tomar√° m√°s tiempo\n",
        "\n",
        "El efecto es m√°s notorio en redes m√°s profundas y problemas m√°s complejos"
      ],
      "metadata": {
        "id": "Ig3bUyyWovvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def build_model_bn(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    #---- Completa la capa de salida -----\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    #-------------------------------------\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "6tdS_ECVtvDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = X_train[43].shape # Extraemos el shape de cualquiera de los ejemplos\n",
        "\n",
        "model_bn = build_model_bn(input_shape)\n",
        "\n",
        "model_bn.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train_cat,\n",
        "                    validation_data=(X_val, y_val_cat),\n",
        "                    epochs=25)"
      ],
      "metadata": {
        "id": "7F1qeueRlN6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model_bn.evaluate(X_train, y_train_cat)\n",
        "_, test_acc = model_bn.evaluate(X_test, y_test_cat)\n",
        "print(f\"Train accuracy: {round(train_acc,3)}\\nTest accuracy : {round(test_acc,3)}\")\n",
        "\n",
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sEMc8YPFldr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö°‚ö° Gridsearch"
      ],
      "metadata": {
        "id": "8WBFeTAfS9RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci√≥n se muestra c√≥mo realizar un gridsearch para obtener los mejores par√°metros de una red neuronal, es decir, los que producen las mejores m√©tricas. Estos par√°metros pueden ser el n√∫mero de neuronas, la tasa de dropout, las √©pocas, etc.\n",
        "\n",
        "Para poder usar el gridsearch de scikit-learn es necesario *traducir* el m√≥delo de red neuronal a un clasificador de scikit-learn. Esto lo hacemos con la clase `KerasClassifier`.\n",
        "\n",
        "**‚ö†‚ùóWarning**: Si se especifican un gran n√∫mero de par√°metros en la busqueda, esta puede tardar mucho y pueden ser penalizados en el uso de GPU en Colab. Usar con cuidado."
      ],
      "metadata": {
        "id": "xSHg3GJNsWJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seguimos con el dataset de [diabetes](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database). Resumamos el preprocesamiento en una sola celda:"
      ],
      "metadata": {
        "id": "qAA4F5s6vV5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI4YT7B23uEJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/03%20Machine%20Learning/data/diabetes.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "display(df)\n",
        "\n",
        "X = df.iloc[:,:8].values\n",
        "y = df.iloc[:,8].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.875,random_state=89)\n",
        "\n",
        "idxs_to_impute = [1,2,3,4,5]\n",
        "imputer = SimpleImputer(missing_values=0, strategy='mean')\n",
        "X_train[:,idxs_to_impute] = imputer.fit_transform(X_train[:,idxs_to_impute])\n",
        "X_test[:,idxs_to_impute] = imputer.transform(X_test[:,idxs_to_impute])\n",
        "\n",
        "scl = StandardScaler()\n",
        "X_train = scl.fit_transform(X_train)\n",
        "X_test = scl.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es necesario crear una funci√≥n que cree el modelo, esta debe depender de los par√°metros sobre los que se quiere realizar la busqueda. Es necesario crear el modelo, compilarlo y regresarlo ya compilado.\n",
        "\n",
        "Con la finalidad de no usar muchos recursos, problemas con un modelo muy sencillo, en el cual variaremos:\n",
        "\n",
        "* El n√∫mero de neuronas en la capa oculta.\n",
        "* La activaci√≥n de la capa oculta."
      ],
      "metadata": {
        "id": "mFJFifE4t4t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "\n",
        "def create_model(n_neurons,activation):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Input(shape=(8,)))\n",
        "\tmodel.add(Dense(n_neurons, activation=activation))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "nX1esrAZ5Hpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_neurons = [5,10,15,20,30]\n",
        "activations = ['relu','sigmoid','tanh']\n",
        "\n",
        "best_acc = 0\n",
        "best_model = None\n",
        "best_params = None\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss',patience=2)\n",
        "\n",
        "for n in num_neurons:\n",
        "    for act in activations:\n",
        "        model = create_model(n,act)\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            callbacks = [es],\n",
        "                            epochs=50,\n",
        "                            verbose=0,\n",
        "                            validation_data=(X_test,y_test)\n",
        "                            )\n",
        "        if history.history['val_accuracy'][-1] > best_acc:\n",
        "            best_acc = history.history['val_accuracy'][-1]\n",
        "            best_model = model\n",
        "            print(f\"New best model: {best_acc}\")\n",
        "            best_model.save('best_model.keras')\n",
        "            best_params = {'n_neurons':n,'activation':act}\n"
      ],
      "metadata": {
        "id": "xspqJ_og8yoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_params)"
      ],
      "metadata": {
        "id": "RbMpdr1iAlJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un modelo de clasificador de scikit-learn usando la API de Keras. Esta empaqueta el m√≥delo de keras como un estimador de scikit-learn. Despu√©s podemos usar el GridSearchCV de scikit learn.\n",
        "\n",
        "[Documentaci√≥n](https://adriangb.com/scikeras/stable/quickstart.html#training-a-model)"
      ],
      "metadata": {
        "id": "2VVc6awqtpyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras[tensorflow]"
      ],
      "metadata": {
        "id": "lh1lXEhR3IMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚≠ï Pr√°ctica\n",
        "\n",
        "En esta pr√°ctica haremos regresi√≥n multi-output usando el dataset `mo_regression.csv`.\n",
        "\n"
      ],
      "metadata": {
        "id": "qATM0TY9Qznf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trabajo en clase"
      ],
      "metadata": {
        "id": "ymS5VByF3Tfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/04%20Deep%20Learning/data/mo_regression.csv\"\n",
        "\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "df"
      ],
      "metadata": {
        "id": "SCBD5wQLQ3AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü¢ Explora el rango de las variables predictoras, ¬øes necesario hacer re-escalamiento?"
      ],
      "metadata": {
        "id": "Gtvq3TLa1_r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "2NRVvVAk2pMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "df.iloc[:,:-3].hist()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e6OydNJkUx1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Divide en train/validation/test usando `train_size=0.8` en ambas divisiones."
      ],
      "metadata": {
        "id": "hcSKgVvE5Hwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = ...\n",
        "y = ...\n",
        "\n",
        "X_train, X_test, y_train, y_test = ...\n",
        "X_train, X_val, y_train, y_val = ..."
      ],
      "metadata": {
        "id": "008F-j8P5InL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Define una red neuronal MLP para modelar este problema de regresi√≥n multi-target. La red debe tener al menos 2 capas ocultas (elige la activaci√≥n de las capas ocultas).\n",
        "\n",
        "Usa como funci√≥n de perdida `MSE` y como m√©trica `MAE`. No olvides compilar el modelo.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "AZPtfwzc3Q1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ..."
      ],
      "metadata": {
        "id": "lOdkWgyf3Qkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define un callback de tipo `EarlyStopping`, con paciencia 3, que est√© monitoreando la perdida."
      ],
      "metadata": {
        "id": "p_QyDpK37DZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n"
      ],
      "metadata": {
        "id": "-nAlVXL97D1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrena durante un n√∫mero de √©pocas $10\\leq n \\leq 50$."
      ],
      "metadata": {
        "id": "YVFdTBRT7AQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = ..."
      ],
      "metadata": {
        "id": "WR_B__ff7AEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü¢ Grafica las curvas de entrenamiento"
      ],
      "metadata": {
        "id": "ToImHOG749sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- graficamos la funci√≥n de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la m√©trica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.suptitle(\"Validation and Training MAE\",fontsize=14)\n",
        "plt.plot(history.history['mae'], label='train')\n",
        "plt.plot(history.history['val_mae'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CVA2JaM-49f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Evalua el desempe√±o en el conjunto de entrenamiento y prueba usando `model.evaluate`"
      ],
      "metadata": {
        "id": "0qh5D1Kw5xGe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pi9B6QhP6gIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ ¬øHay se√±ales de overfitting? ¬øQu√© modificaciones al modelo consideras que se pueden hacer para mejorar el rendimiento de la tarea?"
      ],
      "metadata": {
        "id": "Fci-d0-26vZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trabajo para entregar"
      ],
      "metadata": {
        "id": "dKDC2d3t3WlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realiza los siguientes pasos:\n",
        "\n",
        "0. Divide el conjunto de datos en 80% entrenamiento y 20% prueba.\n",
        "1. Define dos modelos de red neuronal MLP para resolver esta tarea. La funci√≥n de perdida a usar ser√° 'MAE'. Un m√≥delo tendr√° una arquitectura *sencilla* y el otro, una arquitectura *compleja*. T√∫ decide la arquitectura concreta.\n",
        "2. Entrena cada uno de ellos durante 30 √©pocas, compara las m√©tricas MAE. Observa las curvas de entrenamiento.\n",
        "3. Repite el entrenamiento de cada uno de ellos usando los callbacks `EarlyStopping` y `ModelCheckpoint` simultaneamente. Adem√°s, en cada uno de ellos, agrega una capa de Dropout. Observa las curvas de entrenamiento.\n",
        "\n",
        "¬øCu√°l de los 4 modelos lo hizo mejor?"
      ],
      "metadata": {
        "id": "TvO6f--667hT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vu1FunCS2LEL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
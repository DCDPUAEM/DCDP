{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/02-Machine-Learning/notebooks/06-SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM\n",
        "\n",
        "En esta notebook mostraremos el uso del clasificador **SVM** (Support Vector Machine). Realizaremos un ejemplo con datos artificiales, con fines did√°cticos, y un ejemplo m√°s grande, con datos reales.\n",
        "\n",
        "Usaremos la implementaci√≥n de sklearn, llamada [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (Support Vector Classifier)"
      ],
      "metadata": {
        "id": "vmWhZfIxi6yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "UlqlbDHkTIsl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glVe2E6ySpTP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones que necesitamos para graficar las fronteras de decisi√≥n"
      ],
      "metadata": {
        "id": "8cxOlJX_il7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def make_meshgrid(x, y, h=.02):\n",
        "    x_min, x_max = x.min() - 1, x.max() + 1\n",
        "    y_min, y_max = y.min() - 1, y.max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    return xx, yy\n",
        "\n",
        "def plot_contours(ax, clf, xx, yy, **params):\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    out = ax.contourf(xx, yy, Z, **params)\n",
        "    return out"
      ],
      "metadata": {
        "id": "DFox_ShDikzl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### El conjunto de datos"
      ],
      "metadata": {
        "id": "awsozxk0i4e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un conjunto de datos con una condici√≥n XOR"
      ],
      "metadata": {
        "id": "iX00M-qDi1zU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDNOtk2FTRDv"
      },
      "outputs": [],
      "source": [
        "X = np.random.randn(1000, 2)\n",
        "Y = np.array([int(np.logical_xor(x[0] > 0, x[1] > 0)) for x in X])\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[Y==0, 0], X[Y==0, 1], s=10, label='0')\n",
        "plt.scatter(X[Y==1, 0], X[Y==1, 1], s=10, label='1')\n",
        "plt.legend()\n",
        "plt.xlabel('x1',fontsize=16)\n",
        "plt.ylabel('x2',fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separamos el conjunto de datos en train y test."
      ],
      "metadata": {
        "id": "Jy64NizLi95C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7-Hi2BmTUrd",
        "outputId": "19f5e741-90fd-4de4-f898-02fc5393cd9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Train: (800, 2)\n",
            "X Test: (200, 2)\n",
            "Y Train: (800,)\n",
            "Y Test: (200,)\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "print(f\"X Train: {x_train.shape}\")\n",
        "print(f\"X Test: {x_test.shape}\")\n",
        "print(f\"Y Train: {y_train.shape}\")\n",
        "print(f\"Y Test: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEmnVOw6XJqq"
      },
      "source": [
        "### Clasificaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SVM lineal"
      ],
      "metadata": {
        "id": "MLnixjtrjGoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt_JmIokV0xU",
        "outputId": "83b8b11b-4a27-4232-930b-137bfa9eebc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mean accuracy: 0.618\n",
            "Test mean accuracy: 0.635\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "lin_svm = SVC(kernel='linear')\n",
        "lin_svm.fit(x_train, y_train)\n",
        "\n",
        "# Performance\n",
        "print(f\"Training mean accuracy: {round(lin_svm.score(x_train, y_train),3)}\")\n",
        "print(f\"Test mean accuracy: {round(lin_svm.score(x_test, y_test),3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xx, yy = make_meshgrid(X[:,0], X[:,1]) # Hacemos el grid para graficar las regiones\n",
        "\n",
        "fig, ax = plt.subplots(dpi=100)  # El par√°metro dpi especif√≠ca los puntos por pulgada (DPI) de la imagen\n",
        "plot_contours(ax, lin_svm, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "ax.scatter(X[:,0], X[:,1], c=Y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "ax.set_ylabel('x2', fontsize=16)\n",
        "ax.set_xlabel('x1', fontsize=16)\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "ax.set_title('Frontera de decisi√≥n del SVM')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m7S5VurHjcs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚≠ï Probar otros kernels"
      ],
      "metadata": {
        "id": "EGMoIZhmsUbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documentaci√≥n: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
      ],
      "metadata": {
        "id": "Yl7aeHt7sm1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. Repetir el experimento de clasificaci√≥n de arriba, usando otros kernels.\n",
        "2. Graficar\n",
        "3. ¬øQu√© kernel parece dar mejor resultado?\n",
        "'''"
      ],
      "metadata": {
        "id": "jY4JoakxsULC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7bf715ec-071d-4fcb-a183-6cb8c9d5570c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. Repetir el experimento de clasificaci√≥n de arriba, usando otros kernels.\\n2. Graficar\\n3. ¬øQu√© kernel parece dar mejor resultado?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* El kernel lineal es mejor para datos linealmente separables. Es una opci√≥n cuando el conjunto de datos es grande. \n",
        "* El kernel Gaussiano (RBF) tiende a dar buenos resultados cuando no se tiene informaci√≥n adicional sobre los datos.\n",
        "* Los kernels polinomiales tienden a dar buenos resultados cuando los datos de entrenamiento est√°n normalizados."
      ],
      "metadata": {
        "id": "Vqs84jSTs43l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando gridsearch para encontrar los mejores par√°metros"
      ],
      "metadata": {
        "id": "2AFz2eTyjT3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) toma un estimador (por ejemplo, SVM) y un conjunto de par√°metros del estimador. Sobre estos par√°metros hace una busqueda para encontrar la combinaci√≥n de par√°metros que da mejores resultados en el estimador. \n",
        "\n",
        "GridSearchCV tiene m√©todos ‚Äúfit‚Äù y ‚Äúscore‚Äù method, entre otros. Es decir, no es necesario tomar los par√°metros e introducirlos en el estimador."
      ],
      "metadata": {
        "id": "fC23ZQAg7EpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encuentra los mejores par√°metros para el clasificador SVM utilizando grid search. Gu√≠ate por el desempe√±o en el set de entrenamiento y validaci√≥n.\n",
        "\n",
        "Prueba los siguientes hyperpar√°metros.\n",
        "* kernel = linear, polynomial, rbf\n",
        "* C = 0.01, 0.1, 1.0, 10, 100\n",
        "* grado del polinomio = 1, 2, 3, 4 (solo para el kernel polinomial)\n",
        "* gamma = auto, scale:"
      ],
      "metadata": {
        "id": "5ymSFVzkcf03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los par√°metros sobre los que se har√° la busqueda"
      ],
      "metadata": {
        "id": "CJZKn4kxo7LH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fehVJTIZZEmP"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'kernel': ('linear', 'poly', 'rbf'),\n",
        "              'degree': [1, 2, 3, 4], 'gamma': ('auto', 'scale')}\n",
        "param_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos una busqueda sobre estos par√°metros "
      ],
      "metadata": {
        "id": "j6-yzMJUI40z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_mFhIYkaG_y"
      },
      "outputs": [],
      "source": [
        "svc = SVC()\n",
        "clf = GridSearchCV(svc, param_grid)\n",
        "clf.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgCYpURZbCH2",
        "outputId": "dd61ab06-c4e1-431c-90c0-06a3f3661031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best score: 0.9900\n",
            "Best params: {'C': 100, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'}\n"
          ]
        }
      ],
      "source": [
        "# Print info about best score and best hyperparameters\n",
        "print(f\"Best score: {clf.best_score_:.4f}\")\n",
        "print(f\"Best params: {clf.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jclaEUreczue",
        "outputId": "8439e951-11b1-4484-eb2c-a857a5f2005a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train mean accuracy: 0.9862\n",
            "Test mean accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on the test set\n",
        "best_svm = SVC(C=100, kernel='poly', degree=2, gamma='auto')\n",
        "best_svm.fit(x_train, y_train)\n",
        "\n",
        "print(f\"Train mean accuracy: {best_svm.score(x_train, y_train):6.4f}\")\n",
        "print(f\"Test mean accuracy: {best_svm.score(x_test, y_test):6.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficamos la frontera de decisi√≥n"
      ],
      "metadata": {
        "id": "cyE6AWhdh1OV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTSvVYqTdEwM"
      },
      "outputs": [],
      "source": [
        "xx, yy = make_meshgrid(X[:,0], X[:,1]) # Hacemos el grid para graficar las regiones\n",
        "\n",
        "fig, ax = plt.subplots(dpi=100)  # El par√°metro dpi especif√≠ca los puntos por pulgada (DPI) de la imagen\n",
        "plot_contours(ax, best_svm, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "ax.scatter(X[:,0], X[:,1], c=Y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "ax.set_ylabel('x2', fontsize=16)\n",
        "ax.set_xlabel('x1', fontsize=16)\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "ax.set_title('Frontera de decisi√≥n del SVM')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparando el SVM lineal con el OLS (clasificador lineal)"
      ],
      "metadata": {
        "id": "fyhJ9XBnZCTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejercicio vamos a comparar la clasificaci√≥n y la frontera de decisi√≥n del clasificador de la sesi√≥n anterior (discriminante lineal OLS) con el SVM con kernel lineal.\n",
        "\n",
        "Para esto, vamos a usar ambos clasificadores en el mismo conjunto de datos. Despu√©s, compararemos la frontera de decisi√≥n."
      ],
      "metadata": {
        "id": "GyBLYOZ5ZKb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que el clasificador lo implementamos como una clase, podemos usarlo en esta notebook directamente. Hay dos maneras de hacerlo:\n",
        "\n",
        "* Copiando el c√≥digo y definiendo la clase otra vez:"
      ],
      "metadata": {
        "id": "qhjvxInbCZdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "class LeastSquaresClassifier():\n",
        "    def __init__(self, W:np.ndarray=None):\n",
        "        '''\n",
        "        W es la matriz de pesos, la cual puede ser especificada desde un principio, esto\n",
        "        es opcional.\n",
        "        '''\n",
        "        self.W = W\n",
        "\n",
        "    def encoderT(self, y:np.ndarray):\n",
        "        K = np.max(y) + 1\n",
        "        identidad = np.eye(K)\n",
        "        return identidad[y] \n",
        "\n",
        "    def fit(self, X:np.ndarray, y:np.ndarray):\n",
        "        '''\n",
        "        Este m√©todo calcula la matriz de pesos para la matriz de puntos \"aumentada\" X\n",
        "        y el conjunto de etiquetas y.\n",
        "        '''\n",
        "        T = self.encoderT(y)\n",
        "        self.W = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ T\n",
        "        \n",
        "    def clasifica(self, X:np.ndarray):\n",
        "        '''\n",
        "        Este m√©todo predice las etiquetas para el conjunto de puntos X\n",
        "        '''\n",
        "        return np.argmax(X@self.W,axis=1)"
      ],
      "metadata": {
        "id": "m-z82RYuZxR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importando la clase desde un archivo:"
      ],
      "metadata": {
        "id": "P72hCNryCnxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from clasificador_lineal import LeastSquaresClassifier"
      ],
      "metadata": {
        "id": "96ypeu4tZiYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el conjunto de datos, usaremos un dataset de scikit-learn:"
      ],
      "metadata": {
        "id": "-dxo2D_2C-_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "x_train, y_train = make_moons(n_samples = 120, random_state=89,noise=0.1)\n",
        "\n",
        "#--- Lo graficamos para verlo ---\n",
        "plt.figure()\n",
        "plt.scatter(x_train[:,0], x_train[:,1], c=y_train)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "35_M3k2NEbua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï Realiza la clasificaci√≥n usando el clasificador OLS y grafica la frontera de decisi√≥n.\n",
        "\n",
        "Puedes usar el c√≥digo para clasificar y graficar que usamos en la sesi√≥n anterior"
      ],
      "metadata": {
        "id": "fvRzxc_GMfO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "x1_test, x2_test = np.meshgrid(np.linspace(-2, 2.5, 100), np.linspace(-1, 1.5, 100))\n",
        "x_test = np.array([x1_test, x2_test]).reshape(2, -1).T\n",
        "\n",
        "features = PolynomialFeatures(1)\n",
        "X_train = features.fit_transform(x_train)\n",
        "X_test = features.fit_transform(x_test)\n",
        "\n",
        "#------ COMPLETAR ------\n",
        "\n",
        "#----------------------\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
        "plt.contour(x1_test, x2_test, y_ols.reshape(100, 100))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qbcn1VCtC4Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï Ahora, usemos SVM lineal\n",
        "\n",
        "Realiza la clasificaci√≥n en el mismo dataset, usando SVM con kernel lineal y grafica la frontera de decisi√≥n. Puedes usar el c√≥digo para clasificar y graficar que usamos anteriormente."
      ],
      "metadata": {
        "id": "vv8GUSe4NTzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1_test, x2_test = np.meshgrid(np.linspace(-2, 2.5, 100), np.linspace(-1, 1.5, 100))\n",
        "x_test = np.array([x1_test, x2_test]).reshape(2, -1).T\n",
        "\n",
        "#------ COMPLETAR ------\n",
        "\n",
        "#----------------------\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
        "plt.contour(x1_test, x2_test, y_svm.reshape(100, 100))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4BjC4BY7KHud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dibujamos ambas FD juntas. "
      ],
      "metadata": {
        "id": "9OeJaZ81enO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "#-----Dibujar los datos---------------------------------\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
        "#-----Dibujar X_test (la malla de fondo para ver las regiones ------------\n",
        "plt.contour(x1_test, x2_test, y_ols.reshape(100, 100),colors='green')\n",
        "plt.contour(x1_test, x2_test, y_svm.reshape(100, 100),colors='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b1AUz8FuXeMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observar que no son la misma.\n",
        "\n",
        "üîµ ¬øPor qu√© no?"
      ],
      "metadata": {
        "id": "I5CLQPSEPmmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2"
      ],
      "metadata": {
        "id": "QlT0ezfs_PwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este problema usaremos el datset de Kaggle: [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
        "\n",
        "**Contexto**\n",
        "\n",
        "Los conjuntos de datos contienen transacciones realizadas con tarjetas de cr√©dito en septiembre de 2013 por titulares de tarjetas europeos. Este conjunto de datos presenta transacciones que ocurrieron en dos d√≠as, donde tenemos 492 fraudes de 284,807 transacciones. El conjunto de datos est√° altamente desequilibrado, la clase positiva (fraudes) representa el 0.172% de todas las transacciones.\n",
        "\n",
        "Contiene solo variables de entrada num√©ricas que son el resultado de una transformaci√≥n PCA. Desafortunadamente, debido a problemas de confidencialidad, no se pueden obtener las caracter√≠sticas originales y m√°s informaci√≥n de fondo sobre los datos. Las caracter√≠sticas $V_1$, $V_2$, ..., $V_{28}$ son los componentes principales obtenidos con PCA, las √∫nicas caracter√≠sticas que no se han transformado con PCA son 'Tiempo' y 'Cantidad'. La funci√≥n 'Tiempo' contiene los segundos transcurridos entre cada transacci√≥n y la primera transacci√≥n en el conjunto de datos. La caracter√≠stica 'Cantidad' es la Cantidad de la transacci√≥n, esta caracter√≠stica se puede utilizar para el aprendizaje sensible al costo dependiente del ejemplo. La caracter√≠stica 'Clase' es la variable de respuesta y toma el valor 1 en caso de fraude y 0 en caso contrario."
      ],
      "metadata": {
        "id": "-cCQM40V_dJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install > /dev/null subversion\n",
        "\n",
        "!svn checkout \"https://github.com/DCDPUAEM/DCDP/trunk/02-Machine-Learning/data/\""
      ],
      "metadata": {
        "id": "oM1KK1hoUr1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraer el archivo zip"
      ],
      "metadata": {
        "id": "WQNavL28_3jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "from zipfile import ZipFile \n",
        "\n",
        "archivo = \"/content/data/creditcard.zip\"\n",
        "\n",
        "print('Extrayendo contenido...') \n",
        "with ZipFile(archivo, 'r') as Zip: \n",
        "    Zip.extractall() \n",
        "    print('Extracci√≥n finalizada.') \n",
        "\n",
        "credito = pd.read_csv(\"creditcard.csv\")"
      ],
      "metadata": {
        "id": "8AGYagMwh8GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credito.head()"
      ],
      "metadata": {
        "id": "60RwWl3OAM7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credito.describe()"
      ],
      "metadata": {
        "id": "xb2G0chpAP-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "\n",
        "# Graficamos los que no son fraude\n",
        "time_amount = credito[credito['Class'] == 0][['Time','Amount']].values\n",
        "plt.scatter(time_amount[:,0], time_amount[:,1], \n",
        "            c='green',alpha=0.25,label='Clean')\n",
        "# Graficamos los que s√≠ son fraude\n",
        "time_amount = credito[credito['Class'] == 1][['Time','Amount']].values\n",
        "plt.scatter(time_amount[:,0], time_amount[:,1], \n",
        "            c='red',label='Fraud')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Amount vs fraud')\n",
        "plt.xlabel('Time', fontsize=16)\n",
        "plt.ylabel('Amount', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dId7E_NrAXAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "sns.countplot(x = \"Class\", data = credito)\n",
        "plt.show()\n",
        "\n",
        "credito.loc[:, 'Class'].value_counts()"
      ],
      "metadata": {
        "id": "rHQqW8EqAbW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "No_of_frauds= len(credito[credito[\"Class\"]==1])\n",
        "No_of_normals = len(credito[credito[\"Class\"]==0])\n",
        "print(\"Hay {} transacciones fraudulentas ( Class 1)\".format(No_of_frauds))\n",
        "print(\"Hay {} transacciones normales ( Class 0)\".format(No_of_normals))\n",
        "total= No_of_frauds + No_of_normals\n",
        "pf= (No_of_frauds / total)*100\n",
        "pn= (No_of_normals / total)*100\n",
        "print(\"Porcentaje clase 0 = {}%\".format(np.round(pn,2)))\n",
        "print(\"Porcentaje clase 1 = {}%\".format(np.round(pf,2)))"
      ],
      "metadata": {
        "id": "OaWkLhQvBLKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFExW0e-20l7"
      },
      "source": [
        "### Submuestro\n",
        "\n",
        "Se necesita hacer un submuestreo para balancear las clases\n",
        "\n",
        "* Est√° claro que la Clase 1 est√° subrepresentada ya que solo  representa el 0.17% de todo el conjunto de datos. \n",
        "* Si entrenamos nuestro modelo usando este conjunto de datos, el modelo ser√° ineficiente y ser√° entrenado para predecir solo la Clase 0 porque no tendr√° suficientes datos de entrenamiento.\n",
        "* Podemos obtener una alta exactitud al probar el modelo, pero no debemos confundirnos con esto porque nuestro conjunto de datos no tiene datos de prueba equilibrados. Por lo tanto, tenemos que confiar en el recall que se basa en TP y FP.\n",
        "* En los casos en que tengamos datos asim√©tricos, agregar datos adicionales de la caracter√≠stica subrepresentada (sobremuestreo) es una opci√≥n, mediante la modelaci√≥n de la distribuci√≥n de los datos. Por ahora no tenemos esa opci√≥n, as√≠ que tendremos que recurrir al submuestreo.\n",
        "* El submuestreo del conjunto de datos implica mantener todos nuestros datos subrepresentados (Clase 1) mientras se muestrea el mismo n√∫mero de caracter√≠sticas de la Clase 0 para crear un nuevo conjunto de datos que comprenda una representaci√≥n igual de ambas clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MucsJV0r20l7"
      },
      "source": [
        "Obtenemos un conjunto de datos m√°s balanceado que contenga el doble de instancias no fraudulentas respecto a las fraudulentas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4OPAV8M20l8"
      },
      "outputs": [],
      "source": [
        "#lista los indices de fraude\n",
        "fraud_idxs = credito[credito[\"Class\"]==1].index.to_list()\n",
        "\n",
        "#lista de indices normales del data set completo\n",
        "normal_idxs = credito[credito[\"Class\"]==0].index.to_list()\n",
        "\n",
        "#seleccion del numero de indices aleatorias igual al de transacciones fraudulentas\n",
        "random_normal_idxs = np.random.choice(normal_idxs, No_of_frauds*2, replace= False)\n",
        "random_normal_idxs = np.array(random_normal_idxs)\n",
        "\n",
        "#concatena indices fraudulentos y normales para tener una lista de indices\n",
        "undersampled_indices = np.concatenate([fraud_idxs, random_normal_idxs])\n",
        "\n",
        "#usa la lista de indices sub-muestreados para obtener el data frame\n",
        "undersampled_data = credito.iloc[undersampled_indices, :]\n",
        "\n",
        "print(f\"Fraude: {len(fraud_idxs)}, Normales: {len(random_normal_idxs)}\")\n",
        "\n",
        "undersampled_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Wrm6M620l8"
      },
      "source": [
        "Comprobemos que los datos quedaron balanceados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TRLcfkS20l9"
      },
      "outputs": [],
      "source": [
        "No_of_frauds_sampled = len(undersampled_data[undersampled_data[\"Class\"]== 1])\n",
        "\n",
        "No_of_normals_sampled = len(undersampled_data[undersampled_data[\"Class\"]== 0])\n",
        "\n",
        "print(\"N√∫mero de transacciones fraudulentas (clase 1): \", No_of_frauds_sampled)\n",
        "print(\"N√∫mero de transacciones normales (clase 0): \", No_of_normals_sampled)\n",
        "total_sampled= No_of_frauds_sampled + No_of_normals_sampled\n",
        "print(\"N√∫mero total de instancias: \", total_sampled)\n",
        "\n",
        "Fraud_percent_sampled = (No_of_frauds_sampled / total_sampled)*100\n",
        "Normal_percent_sampled = (No_of_normals_sampled / total_sampled)*100\n",
        "print(\"Porcentaje clase 0 = \", Normal_percent_sampled)\n",
        "print(\"Porcentaje clase 1 = \", Fraud_percent_sampled)\n",
        "\n",
        "\n",
        "count_sampled = pd.value_counts(undersampled_data[\"Class\"], sort= True)\n",
        "count_sampled.plot(kind= 'bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_PjO_td20l9"
      },
      "source": [
        "### Oversampling\n",
        "\n",
        "Ahora haremos un proceso llamado [SMOTE: Synthetic Minority Over-sampling Technique](https://arxiv.org/abs/1106.1813)\n",
        "\n",
        "\n",
        "Para ello necesitamos instalar la librer√≠a de _aprendizaje desequilibrado_ ``imbalanced-learn`` de Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa_Vb4NP20l_"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4fPH1Bd20mA"
      },
      "outputs": [],
      "source": [
        "import imblearn\n",
        "print(imblearn.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTSJsp2K20mB"
      },
      "source": [
        "Re-escalemos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqtPP2k520mB"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "sc = preprocessing.StandardScaler()\n",
        "\n",
        "undersampled_data.loc[:,\"scaled_Amount\"] = sc.fit_transform(undersampled_data[\"Amount\"].values.reshape(-1,1))\n",
        "\n",
        "# quitamos las columnas \"Time\" y \"Amount\"\n",
        "undersampled_data.drop([\"Time\",\"Amount\"], axis= 1,inplace=True)\n",
        "\n",
        "undersampled_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaNpF9Cu20mB"
      },
      "source": [
        "‚≠ï Obt√©n la matriz de datos $X$ y el vector de clases $y$ correspondiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fWtfSkU20mB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF4zrMIG20mC"
      },
      "source": [
        "Hagamos el proceso de sobre-muestreo [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfmmO0By20mC"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG7Hf5ix20mD"
      },
      "source": [
        "Verifiquemos la cantidad de datos ahora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDdO9Cyp20mD"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "counter = Counter(y)\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb2gdqXw20mH"
      },
      "source": [
        "### Crear el conjunto de entrenamiento y prueba\n",
        "\n",
        "Separa los datos en datos de entrenamiento (75%) y prueba (25%) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3loC9tl20mH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state= 0)\n",
        "print(\"X_train: \", len(X_train))\n",
        "print(\"X_test: \", len(X_test))\n",
        "print(\"y_train: \", len(y_train))\n",
        "print(\"y_test: \", len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5GqWW6Q20mH"
      },
      "source": [
        "‚≠ï Elige una SVM y entr√©nalo con un conjunto de par√°metros de tu elecci√≥n "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekwgZfiG20mH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp_YhOUu20mI"
      },
      "source": [
        "### Prueba el modelo \n",
        "\n",
        "Realiza las predicciones con el conjunto de prueba y bserva la matriz de confusi√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTRsqljv20mI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "CM = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(CM)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambi√©n podemos calcular las m√©tricas de rendimiento *manualmente*."
      ],
      "metadata": {
        "id": "2wgQFPGzZMnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The accuracy is \"+str((CM[1,1]+CM[0,0])/(CM[0,0] + CM[0,1]+CM[1,0] + CM[1,1])*100) + \" %\")\n",
        "print(\"The recall from the confusion matrix is \"+ str(CM[1,1]/(CM[1,0] + CM[1,1])*100) +\" %\")"
      ],
      "metadata": {
        "id": "5bYk4F11ZMVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï Calcula tambi√©n el *F1-score* y el *precision score*"
      ],
      "metadata": {
        "id": "WGQVuXnXZdHd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b9sl11tpbkdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRmyrMl720mK"
      },
      "source": [
        "### Aplica GridSearch para obtener los mejores par√°metros para una SVM "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJEOf9L320mK"
      },
      "outputs": [],
      "source": [
        "parameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
        "              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
        "\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 5,\n",
        "                           n_jobs = -1)\n",
        "\n",
        "grid_search = grid_search.fit(X_train, y_train.ravel())\n",
        "best_accuracy = grid_search.best_score_\n",
        "print(\"The best accuracy using gridSearch is\", best_accuracy)\n",
        "\n",
        "best_parameters = grid_search.best_params_\n",
        "print(\"The best parameters for using this model is\", best_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfaYAhAK20mL"
      },
      "source": [
        "### Utiliza los mejores par√°metros para probar de nuevo tu modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9EpQKyf20mM",
        "outputId": "4031e8ca-851a-42b1-91d7-823b7931586b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[229  12]\n",
            " [ 23 228]]\n",
            "The accuracy is 92.88617886178862 %\n",
            "The recall from the confusion matrix is 90.83665338645417 %\n",
            "[[237   4]\n",
            " [ 31 220]]\n",
            "The accuracy is 92.88617886178862 %\n",
            "The recall from the confusion matrix is 87.64940239043824 %\n"
          ]
        }
      ],
      "source": [
        "classifier_with_best_parameters =  SVC(C= best_parameters[\"C\"], \n",
        "                                       kernel= best_parameters[\"kernel\"], \n",
        "                                       random_state= 0)\n",
        "classifier_with_best_parameters.fit(X_train, y_train.ravel())\n",
        "\n",
        "#predicting the Class \n",
        "y_pred_best_parameters = classifier_with_best_parameters.predict(X_test)\n",
        "\n",
        "CM2 = confusion_matrix(y_test, y_pred_best_parameters)\n",
        "#visualizing the confusion matrix\n",
        "print(CM2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wgbaCD020mN"
      },
      "source": [
        "### ‚≠ï Prueba el modelo con el data set completo (sesgado)\n",
        "\n",
        "Entrena un clasificador usando el dataset original sesgado, sin usar undersampling ni oversampling.\n",
        "\n",
        "Puedes usar GridSearch (puede ser tardado). Al final, reportar las m√©tricas de rendimiento y la matriz de confusi√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c31eW8Dh20mN"
      },
      "outputs": [],
      "source": [
        "#creating a new dataset to test our model\n",
        "datanew = credito.copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c2hBanv20mO"
      },
      "source": [
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
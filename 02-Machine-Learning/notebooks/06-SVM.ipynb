{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/02-Machine-Learning/notebooks/06-SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM\n",
        "\n",
        "En esta notebook mostraremos el uso del clasificador **SVM** (Support Vector Machine). Realizaremos un ejemplo con datos artificiales, con fines did√°cticos, y un ejemplo m√°s grande, con datos reales.\n",
        "\n",
        "Usaremos la implementaci√≥n de sklearn, llamada [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (Support Vector Classifier)"
      ],
      "metadata": {
        "id": "vmWhZfIxi6yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "UlqlbDHkTIsl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "glVe2E6ySpTP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones que necesitamos para graficar las fronteras de decisi√≥n"
      ],
      "metadata": {
        "id": "8cxOlJX_il7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_meshgrid(x, y, h=.02):\n",
        "    '''\n",
        "    funci√≥n para hacer la malla de puntos para colorear las regiones de decisi√≥n,\n",
        "    la malla de puntos abarca la regi√≥n donde se encuentran los puntos (x,y)\n",
        "    'h' es el tama√±o de paso\n",
        "    '''\n",
        "    x_min, x_max = x.min() - 1, x.max() + 1\n",
        "    y_min, y_max = y.min() - 1, y.max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    return xx, yy\n",
        "\n",
        "def plot_contours(ax, clf, xx, yy, **params):\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    out = ax.contourf(xx, yy, Z, **params)\n",
        "    return out"
      ],
      "metadata": {
        "id": "DFox_ShDikzl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### El conjunto de datos"
      ],
      "metadata": {
        "id": "awsozxk0i4e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un conjunto de datos con una condici√≥n XOR"
      ],
      "metadata": {
        "id": "iX00M-qDi1zU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDNOtk2FTRDv"
      },
      "outputs": [],
      "source": [
        "np.random.seed(17) # Fijamos un seed para la reproducibilidad de resultados\n",
        "\n",
        "X = np.random.randn(1000, 2)\n",
        "Y = np.array([int(np.logical_xor(x[0] > 0, x[1] > 0)) for x in X])\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[Y==0, 0], X[Y==0, 1], s=20, color='blue', label='Clase 0')\n",
        "plt.scatter(X[Y==1, 0], X[Y==1, 1], s=20, color='red',label='Clase 1')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separamos el conjunto de datos en train y test."
      ],
      "metadata": {
        "id": "Jy64NizLi95C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7-Hi2BmTUrd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2023)\n",
        "\n",
        "print(f\"X Train: {x_train.shape}\")\n",
        "print(f\"X Test: {x_test.shape}\")\n",
        "print(f\"Y Train: {y_train.shape}\")\n",
        "print(f\"Y Test: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEmnVOw6XJqq"
      },
      "source": [
        "### Clasificaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SVM lineal\n",
        "\n",
        "Entrenemos el clasificador usando el kernel lineal. Observar que, por default, $C=1$."
      ],
      "metadata": {
        "id": "MLnixjtrjGoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt_JmIokV0xU"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observemos el accuracy en el conjunto de entrenamiento y prueba. En este caso, el m√©todo `score` de la clase `SVC` calcula el accuracy."
      ],
      "metadata": {
        "id": "ccEI8ng4glyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training mean accuracy: {round(clf.score(x_train, y_train),3)}\")\n",
        "print(f\"Test mean accuracy: {round(clf.score(x_test, y_test),3)}\")"
      ],
      "metadata": {
        "id": "jZMNw4GybN9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observemos la frontera de decisi√≥n calculada por el clasificador y los conjuntos de entrenamiento y prueba."
      ],
      "metadata": {
        "id": "qI_ajLA3kp_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xx, yy = make_meshgrid(X[:,0], X[:,1]) # Hacemos el grid para graficar las regiones\n",
        "  \n",
        "fig, (ax1, ax2) = plt.subplots(1,2,dpi=100,figsize=(10,4)) # El par√°metro dpi especif√≠ca los puntos por pulgada (DPI) de la imagen\n",
        "\n",
        "fig.suptitle(\"Fronteras de decisi√≥n\")\n",
        "\n",
        "plot_contours(ax1, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "ax1.scatter(x_train[:,0], x_train[:,1], c=y_train, cmap=plt.cm.coolwarm, s=20)\n",
        "ax1.set_xticks(())\n",
        "ax1.set_yticks(())\n",
        "ax1.set_title('Conjunto de entrenamiento')\n",
        "\n",
        "plot_contours(ax2, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "ax2.scatter(x_test[:,0], x_test[:,1], c=y_test, cmap=plt.cm.coolwarm, s=20)\n",
        "ax2.set_xticks(())\n",
        "ax2.set_yticks(())\n",
        "ax2.set_title('Conjunto de prueba')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YGTC1EmOh_nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚≠ï Probar otros kernels\n",
        "\n",
        "Con los mismos conjuntos de prueba y entrenamiento:\n",
        "\n",
        "1. Repetir el experimento de clasificaci√≥n de arriba, usando otros kernels.\n",
        "2. En cada caso que pruebes grafica los puntos (los de prueba) y la frontera de decisi√≥n.\n",
        "3. En cada caso, reporta el valor de accuracy y recall, usando el conjunto de prueba solamente.\n",
        "\n",
        "**¬øQu√© kernel parece dar mejor resultado?**"
      ],
      "metadata": {
        "id": "EGMoIZhmsUbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documentaci√≥n: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
      ],
      "metadata": {
        "id": "Yl7aeHt7sm1H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jY4JoakxsULC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* El kernel lineal es mejor para datos linealmente separables. Es una opci√≥n cuando el conjunto de datos es grande. \n",
        "* El kernel Gaussiano (RBF) tiende a dar buenos resultados cuando no se tiene informaci√≥n adicional sobre los datos.\n",
        "* Los kernels polinomiales tienden a dar buenos resultados cuando los datos de entrenamiento est√°n normalizados.\n",
        "\n",
        "[El truco del kernel](https://www.geogebra.org/m/xawkavxe)"
      ],
      "metadata": {
        "id": "Vqs84jSTs43l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï Prueba tambi√©n con otros valores de `C` y repite los pasos de arriba, ¬øqu√© efecto tiene el modificar este valor en la clasificaci√≥n?"
      ],
      "metadata": {
        "id": "-nXR_1f3m4Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z2-UV1uOnQyH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando gridsearch para encontrar los mejores par√°metros"
      ],
      "metadata": {
        "id": "2AFz2eTyjT3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) toma un estimador (por ejemplo, SVM) y un conjunto de par√°metros del estimador. Sobre estos par√°metros hace una busqueda para encontrar la combinaci√≥n de par√°metros que da mejores resultados en el estimador. \n",
        "\n",
        "GridSearchCV tiene m√©todos ‚Äúfit‚Äù y ‚Äúscore‚Äù method, entre otros. Es decir, no es necesario tomar los par√°metros e introducirlos en el estimador."
      ],
      "metadata": {
        "id": "fC23ZQAg7EpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "lVkQd-v1Z4aV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encuentra los mejores par√°metros para el clasificador SVM utilizando grid search. Gu√≠ate por el desempe√±o en el set de entrenamiento y validaci√≥n.\n",
        "\n",
        "Prueba los siguientes hyperpar√°metros.\n",
        "* kernel = linear, polynomial, rbf\n",
        "* C = 0.01, 0.1, 1.0, 10, 100\n",
        "* grado del polinomio = 1, 2, 3, 4 (solo para el kernel polinomial)\n",
        "* gamma = auto, scale:"
      ],
      "metadata": {
        "id": "5ymSFVzkcf03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los par√°metros sobre los que se har√° la busqueda"
      ],
      "metadata": {
        "id": "CJZKn4kxo7LH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fehVJTIZZEmP"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'kernel': ('linear', 'poly', 'rbf'),\n",
        "              'degree': [1, 2, 3, 4], 'gamma': ('auto', 'scale')}\n",
        "param_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos una busqueda sobre estos par√°metros "
      ],
      "metadata": {
        "id": "j6-yzMJUI40z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_mFhIYkaG_y"
      },
      "outputs": [],
      "source": [
        "clf = SVC()\n",
        "gs = GridSearchCV(clf, param_grid)\n",
        "gs.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos los mejores hiper-par√°metros"
      ],
      "metadata": {
        "id": "OntcfhwVrWgt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgCYpURZbCH2"
      },
      "outputs": [],
      "source": [
        "print(f\"Best score: {gs.best_score_:.4f}\")\n",
        "print(f\"Best params: {gs.best_params_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definamos un clasificador SVM con estos mejores hiperpar√°metros"
      ],
      "metadata": {
        "id": "0n2iAE_IrbCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jclaEUreczue"
      },
      "outputs": [],
      "source": [
        "best_svm = SVC(C=100, kernel='poly', degree=2, gamma='auto')\n",
        "best_svm.fit(x_train, y_train)\n",
        "\n",
        "print(f\"Train mean accuracy: {best_svm.score(x_train, y_train):6.4f}\")\n",
        "print(f\"Test mean accuracy: {best_svm.score(x_test, y_test):6.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficamos la frontera de decisi√≥n"
      ],
      "metadata": {
        "id": "cyE6AWhdh1OV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTSvVYqTdEwM"
      },
      "outputs": [],
      "source": [
        "xx, yy = make_meshgrid(X[:,0], X[:,1]) # Hacemos el grid para graficar las regiones\n",
        "\n",
        "fig, ax = plt.subplots(dpi=100)  # El par√°metro dpi especif√≠ca los puntos por pulgada (DPI) de la imagen\n",
        "plot_contours(ax, best_svm, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "ax.scatter(X[:,0], X[:,1], c=Y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "ax.set_title('Frontera de decisi√≥n del SVM')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparando el SVM lineal con el OLS (clasificador lineal)"
      ],
      "metadata": {
        "id": "fyhJ9XBnZCTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejercicio vamos a comparar la clasificaci√≥n y la frontera de decisi√≥n del clasificador de la sesi√≥n anterior (discriminante lineal OLS) con el SVM con kernel lineal.\n",
        "\n",
        "Para esto, vamos a usar ambos clasificadores en el mismo conjunto de datos. Despu√©s, compararemos la frontera de decisi√≥n."
      ],
      "metadata": {
        "id": "GyBLYOZ5ZKb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que el clasificador lo implementamos como una clase, podemos usarlo en esta notebook directamente. Hay dos maneras de hacerlo:\n",
        "\n",
        "* Copiando el c√≥digo y definiendo la clase otra vez:\n",
        "* Descargando el archivo desde github:"
      ],
      "metadata": {
        "id": "qhjvxInbCZdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/02-Machine-Learning/data/clasificador_lineal.py\"\n",
        "!wget --no-cache --backups=1 {url}"
      ],
      "metadata": {
        "id": "caLo57ZRsZa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya est√° copiado en la misma carpeta donde estamos trabajando por lo que ya lo podemos importar directamente"
      ],
      "metadata": {
        "id": "P72hCNryCnxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from clasificador_lineal import LeastSquaresClassifier"
      ],
      "metadata": {
        "id": "96ypeu4tZiYI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el conjunto de datos, usaremos un dataset de scikit-learn:"
      ],
      "metadata": {
        "id": "-dxo2D_2C-_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "x_train, y_train = make_moons(n_samples = 120, random_state=89, noise=0.1)\n",
        "\n",
        "#--- Lo graficamos para verlo ---\n",
        "plt.figure()\n",
        "plt.scatter(x_train[:,0], x_train[:,1], c=y_train)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "35_M3k2NEbua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï Realiza la clasificaci√≥n usando el clasificador OLS y grafica la frontera de decisi√≥n.\n",
        "\n",
        "Puedes usar el c√≥digo para clasificar y graficar que usamos en la sesi√≥n anterior"
      ],
      "metadata": {
        "id": "fvRzxc_GMfO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "x1_test, x2_test = np.meshgrid(np.linspace(-2, 2.5, 100), np.linspace(-1, 1.5, 100))\n",
        "x_test = np.array([x1_test, x2_test]).reshape(2, -1).T\n",
        "\n",
        "features = PolynomialFeatures(1)\n",
        "X_train = features.fit_transform(x_train)\n",
        "X_test = features.fit_transform(x_test)\n",
        "\n",
        "#------ COMPLETAR ------\n",
        "modelo = LeastSquaresClassifier()   \n",
        "modelo.fit(X_train,y_train)          \n",
        "y_ols = modelo.clasifica(X_test)\n",
        "#----------------------\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
        "plt.contour(x1_test, x2_test, y_ols.reshape(100, 100))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qbcn1VCtC4Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï Ahora, usemos SVM lineal\n",
        "\n",
        "Realiza la clasificaci√≥n en el mismo dataset, usando SVM con kernel lineal y grafica la frontera de decisi√≥n. Puedes usar el c√≥digo para clasificar y graficar que usamos anteriormente."
      ],
      "metadata": {
        "id": "vv8GUSe4NTzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1_test, x2_test = np.meshgrid(np.linspace(-2, 2.5, 100), np.linspace(-1, 1.5, 100))\n",
        "x_test = np.array([x1_test, x2_test]).reshape(2, -1).T\n",
        "\n",
        "#------ COMPLETAR ------\n",
        "lin_svm = SVC(kernel='linear')\n",
        "lin_svm.fit(x_train, y_train)\n",
        "y_svm = lin_svm.predict(x_test)\n",
        "#----------------------\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
        "plt.contour(x1_test, x2_test, y_svm.reshape(100, 100))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4BjC4BY7KHud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dibujamos ambas FD juntas. "
      ],
      "metadata": {
        "id": "9OeJaZ81enO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "#-----Dibujar los datos---------------------------------\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
        "#-----Dibujar X_test (la malla de fondo para ver las regiones ------------\n",
        "plt.contour(x1_test, x2_test, y_ols.reshape(100, 100),colors='green')\n",
        "plt.contour(x1_test, x2_test, y_svm.reshape(100, 100),colors='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b1AUz8FuXeMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observar que no son la misma.\n",
        "\n",
        "üîµ ¬øPor qu√© no?"
      ],
      "metadata": {
        "id": "I5CLQPSEPmmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2"
      ],
      "metadata": {
        "id": "QlT0ezfs_PwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este problema usaremos el datset de Kaggle.\n",
        "\n",
        "**Contexto**\n",
        "\n",
        "Los conjuntos de datos contienen transacciones realizadas con tarjetas de cr√©dito en septiembre de 2013 por titulares de tarjetas europeos. Este conjunto de datos presenta transacciones que ocurrieron en dos d√≠as, donde tenemos 492 fraudes de 284,807 transacciones. El conjunto de datos est√° altamente desequilibrado, la clase positiva (fraudes) representa el 0.172% de todas las transacciones.\n",
        "\n",
        "Contiene solo variables de entrada num√©ricas que son el resultado de una transformaci√≥n PCA. Desafortunadamente, debido a problemas de confidencialidad, no se pueden obtener las caracter√≠sticas originales y m√°s informaci√≥n de fondo sobre los datos. Las caracter√≠sticas $V_1$, $V_2$, ..., $V_{28}$ son los componentes principales obtenidos con PCA, las √∫nicas caracter√≠sticas que no se han transformado con PCA son 'Tiempo' y 'Cantidad'. La funci√≥n 'Tiempo' contiene los segundos transcurridos entre cada transacci√≥n y la primera transacci√≥n en el conjunto de datos. La caracter√≠stica 'Cantidad' es la Cantidad de la transacci√≥n, esta caracter√≠stica se puede utilizar para el aprendizaje sensible al costo dependiente del ejemplo. La caracter√≠stica 'Clase' es la variable de respuesta y toma el valor 1 en caso de fraude y 0 en caso contrario.\n",
        "\n",
        "\n",
        "Recordemos las buenas pr√°cticas del Machine Learning: https://scikit-learn.org/stable/common_pitfalls.html"
      ],
      "metadata": {
        "id": "-cCQM40V_dJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install > /dev/null subversion\n",
        "\n",
        "!svn checkout \"https://github.com/DCDPUAEM/DCDP/trunk/02-Machine-Learning/data/\""
      ],
      "metadata": {
        "id": "oM1KK1hoUr1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraer el archivo zip"
      ],
      "metadata": {
        "id": "WQNavL28_3jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from zipfile import ZipFile \n",
        "import pandas as pd\n",
        "# pd.options.mode.chained_assignment = None\n",
        "\n",
        "archivo = \"/content/data/creditcard.zip\"\n",
        "\n",
        "print('Extrayendo contenido...') \n",
        "with ZipFile(archivo, 'r') as Zip: \n",
        "    Zip.extractall() \n",
        "    print('Extracci√≥n finalizada.') \n",
        "\n",
        "credito = pd.read_csv(\"creditcard.csv\")"
      ],
      "metadata": {
        "id": "8AGYagMwh8GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credito.head()"
      ],
      "metadata": {
        "id": "60RwWl3OAM7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credito.describe()"
      ],
      "metadata": {
        "id": "xb2G0chpAP-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Graficamos los que no son fraude\n",
        "time_amount = credito[credito['Class'] == 0][['Time','Amount']].values\n",
        "plt.scatter(time_amount[:,0], time_amount[:,1], \n",
        "            c='green',alpha=0.25,label='Clean')\n",
        "# Graficamos los que s√≠ son fraude\n",
        "time_amount = credito[credito['Class'] == 1][['Time','Amount']].values\n",
        "plt.scatter(time_amount[:,0], time_amount[:,1], \n",
        "            c='red',label='Fraud',marker='x')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Amount vs fraud')\n",
        "plt.xlabel('Time', fontsize=16)\n",
        "plt.ylabel('Amount', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dId7E_NrAXAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure()\n",
        "sns.countplot(x = \"Class\", data = credito)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rHQqW8EqAbW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "No_of_frauds = credito[credito[\"Class\"]==1].shape[0]\n",
        "No_of_normals = credito[credito[\"Class\"]==0].shape[0]\n",
        "print(\"Hay {} transacciones normales (clase 0)\".format(No_of_normals))\n",
        "print(\"Hay {} transacciones fraudulentas (clase 1)\".format(No_of_frauds))\n",
        "total = No_of_frauds + No_of_normals\n",
        "pf= (No_of_frauds / total)*100\n",
        "pn= (No_of_normals / total)*100\n",
        "print(\"Porcentaje clase 0 = {}%\".format(np.round(pn,2)))\n",
        "print(\"Porcentaje clase 1 = {}%\".format(np.round(pf,2)))"
      ],
      "metadata": {
        "id": "OaWkLhQvBLKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFExW0e-20l7"
      },
      "source": [
        "### Submuestro\n",
        "\n",
        "Se necesita hacer un submuestreo para balancear las clases\n",
        "\n",
        "* Est√° claro que la Clase 1 est√° subrepresentada ya que solo  representa el 0.17% de todo el conjunto de datos. \n",
        "* Si entrenamos nuestro modelo usando este conjunto de datos, el modelo ser√° ineficiente y ser√° entrenado para predecir solo la Clase 0 porque no tendr√° suficientes datos de entrenamiento.\n",
        "* Podemos obtener una alta exactitud al probar el modelo, pero no debemos confundirnos con esto porque nuestro conjunto de datos no tiene datos de prueba equilibrados. Por lo tanto, tenemos que confiar en el recall que se basa en TP y FP.\n",
        "* En los casos en que tengamos datos asim√©tricos, agregar datos adicionales de la caracter√≠stica subrepresentada (sobremuestreo) es una opci√≥n, mediante la modelaci√≥n de la distribuci√≥n de los datos. Por ahora no tenemos esa opci√≥n, as√≠ que tendremos que recurrir al submuestreo.\n",
        "* El submuestreo del conjunto de datos implica mantener todos nuestros datos subrepresentados (Clase 1) mientras se muestrea el mismo n√∫mero de caracter√≠sticas de la Clase 0 para crear un nuevo conjunto de datos que comprenda una representaci√≥n igual de ambas clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MucsJV0r20l7"
      },
      "source": [
        "Obtenemos un conjunto de datos m√°s balanceado que contenga el doble de instancias no fraudulentas respecto a las fraudulentas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4OPAV8M20l8"
      },
      "outputs": [],
      "source": [
        "# lista los √≠ndices de fraude del data set completo\n",
        "fraud_idxs = credito[credito[\"Class\"]==1].index.to_list()\n",
        "\n",
        "# lista de √≠ndices normales del data set completo\n",
        "normal_idxs = credito[credito[\"Class\"]==0].index.to_list()\n",
        "\n",
        "# seleccionamos aleatoriamente el doble de √≠ndices de transacciones normales que de normales\n",
        "random_normal_idxs = np.random.choice(normal_idxs, No_of_frauds*2, replace= False)\n",
        "\n",
        "# concatenamos los √≠ndices fraudulentos y normales y creamos el dataframe sub-sampleado\n",
        "undersampled_indices = np.concatenate([fraud_idxs, random_normal_idxs])\n",
        "undersampled_data = credito.iloc[undersampled_indices, :]\n",
        "\n",
        "print(f\"Fraude: {len(fraud_idxs)}, Normales: {len(random_normal_idxs)}\")\n",
        "undersampled_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Wrm6M620l8"
      },
      "source": [
        "Comprobemos que los datos quedaron balanceados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TRLcfkS20l9"
      },
      "outputs": [],
      "source": [
        "No_of_frauds_sampled = len(undersampled_data[undersampled_data[\"Class\"]== 1])\n",
        "\n",
        "No_of_normals_sampled = len(undersampled_data[undersampled_data[\"Class\"]== 0])\n",
        "\n",
        "print(\"N√∫mero de transacciones normales (clase 0): \", No_of_normals_sampled)\n",
        "print(\"N√∫mero de transacciones fraudulentas (clase 1): \", No_of_frauds_sampled)\n",
        "total_sampled = No_of_frauds_sampled + No_of_normals_sampled\n",
        "print(\"N√∫mero total de instancias: \", total_sampled)\n",
        "\n",
        "Fraud_percent_sampled = (No_of_frauds_sampled / total_sampled)*100\n",
        "Normal_percent_sampled = (No_of_normals_sampled / total_sampled)*100\n",
        "print(f\"Porcentaje clase 0: {round(Normal_percent_sampled,2)}\")\n",
        "print(f\"Porcentaje clase 1: {round(Normal_percent_sampled,2)}\")\n",
        "\n",
        "count_sampled = pd.value_counts(undersampled_data[\"Class\"], sort= True)\n",
        "count_sampled.plot(kind= 'bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quitamos las columnas \"Time\" y \"Amount\"\n",
        "undersampled_data.drop([\"Time\"], axis= 1,inplace=True)"
      ],
      "metadata": {
        "id": "_7a2FuivMYhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_PjO_td20l9"
      },
      "source": [
        "### Oversampling\n",
        "\n",
        "Ahora haremos un proceso llamado [SMOTE: Synthetic Minority Over-sampling Technique](https://arxiv.org/abs/1106.1813)\n",
        "\n",
        "\n",
        "Para ello necesitamos instalar la librer√≠a de _aprendizaje desequilibrado_ ``imbalanced-learn`` de Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa_Vb4NP20l_"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos imprimir informaci√≥n sobre el m√≥dulo"
      ],
      "metadata": {
        "id": "ZuEXCTrtugUW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4fPH1Bd20mA"
      },
      "outputs": [],
      "source": [
        "import imblearn\n",
        "print(imblearn.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaNpF9Cu20mB"
      },
      "source": [
        "Obtenemos la matriz de datos $X$ y el vector de clases $y$ correspondiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fWtfSkU20mB"
      },
      "outputs": [],
      "source": [
        "X = undersampled_data.loc[:, undersampled_data.columns != \"Class\"].values\n",
        "y = undersampled_data.loc[:, undersampled_data.columns == \"Class\"].values\n",
        "\n",
        "print(f\"Matriz de features: {X.shape}\")\n",
        "print(f\"Matriz de etiquetas: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF4zrMIG20mC"
      },
      "source": [
        "Hagamos el proceso de sobre-muestreo [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qfmmO0By20mC"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "oversample = SMOTE()\n",
        "X_oversampled, y_oversampled = oversample.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG7Hf5ix20mD"
      },
      "source": [
        "Verifiquemos la cantidad de datos ahora"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(f\"Matriz de features: {X_oversampled.shape}\")\n",
        "print(f\"Matriz de etiquetas: {y_oversampled.shape}\")\n",
        "\n",
        "print(Counter(y_oversampled))"
      ],
      "metadata": {
        "id": "NRujZ0ltPzEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb2gdqXw20mH"
      },
      "source": [
        "### Crear el conjunto de entrenamiento y prueba\n",
        "\n",
        "Separamos los datos en datos de entrenamiento (75%) y prueba (25%) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3loC9tl20mH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, \n",
        "                                                    test_size = 0.25, \n",
        "                                                    random_state = 359)\n",
        "\n",
        "print(\"The split of the under_sampled data is as follows\")\n",
        "print(\"X_train: \", len(X_train))\n",
        "print(\"X_test: \", len(X_test))\n",
        "print(\"y_train: \", len(y_train))\n",
        "print(\"y_test: \", len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTSJsp2K20mB"
      },
      "source": [
        "### Re-escalemos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqtPP2k520mB"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "sc = preprocessing.StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "\n",
        "X_test = sc.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5GqWW6Q20mH"
      },
      "source": [
        "‚≠ï Elige una SVM y entr√©nalo con un conjunto de par√°metros de tu elecci√≥n. Obtener el accuracy usando el m√©todo `score` del clasificador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekwgZfiG20mH"
      },
      "outputs": [],
      "source": [
        "classifier = SVC(C=1, kernel= 'rbf', random_state=0, gamma='scale')\n",
        "classifier.fit(X_train, y_train)\n",
        "classifier.score(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp_YhOUu20mI"
      },
      "source": [
        "### Prueba el modelo \n",
        "\n",
        "Realiza las predicciones con el conjunto de prueba y bserva la matriz de confusi√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTRsqljv20mI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "CM = confusion_matrix(y_test, y_pred)\n",
        "print(CM)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambi√©n podemos calcular las m√©tricas de rendimiento *manualmente*."
      ],
      "metadata": {
        "id": "2wgQFPGzZMnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = round((CM[1,1]+CM[0,0])/(CM[0,0] + CM[0,1]+CM[1,0] + CM[1,1])*100,3)\n",
        "rec = round(CM[1,1]/(CM[1,0] + CM[1,1])*100,3)\n",
        "\n",
        "print(f\"Accuracy: {acc}\")\n",
        "print(f\"Recall: {rec}\")"
      ],
      "metadata": {
        "id": "5bYk4F11ZMVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï Calcula tambi√©n el *F1-score* y el *precision score*"
      ],
      "metadata": {
        "id": "WGQVuXnXZdHd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JjrOfFZ9Th4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRmyrMl720mK"
      },
      "source": [
        "### Aplica GridSearch para obtener los mejores par√°metros para una SVM "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJEOf9L320mK"
      },
      "outputs": [],
      "source": [
        "parameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
        "              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}\n",
        "              ]\n",
        "\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 5,\n",
        "                           n_jobs = -1)\n",
        "\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "best_accuracy = grid_search.best_score_\n",
        "print(\"The best accuracy using gridSearch is\", best_accuracy)\n",
        "\n",
        "best_parameters = grid_search.best_params_\n",
        "print(\"The best parameters for using this model is\", best_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfaYAhAK20mL"
      },
      "source": [
        "### Utiliza los mejores par√°metros para probar de nuevo tu modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9EpQKyf20mM"
      },
      "outputs": [],
      "source": [
        "classifier_with_best_parameters =  SVC(C= best_parameters[\"C\"], \n",
        "                                       kernel= best_parameters[\"kernel\"], \n",
        "                                       random_state= 0)\n",
        "classifier_with_best_parameters.fit(X_train, y_train)\n",
        "\n",
        "y_pred_best_parameters = classifier_with_best_parameters.predict(X_test)\n",
        "\n",
        "CM2 = confusion_matrix(y_test, y_pred_best_parameters)\n",
        "print(CM2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï Calcula las m√©tricas de rendimiento: Accuracy, Recall, F1-score, Precision"
      ],
      "metadata": {
        "id": "rEJwIr5PVbX-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-BBLmTOSbOSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "htKoV-CNfgJA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wgbaCD020mN"
      },
      "source": [
        "### ‚≠ï Pr√°ctica\n",
        "\n",
        "La pr√°ctica consiste en dos ejercicios:\n",
        "\n",
        "1. Usa el modelo anterior (**no lo vuelvas a entrenar**) para obtener las predicciones en todos el conjunto de datos original. Reporta las 4 m√©tricas de rendimiento, as√≠ como la matriz de confusi√≥n. *Hint*: Puedes usar un pipeline para facilitar el proceso.\n",
        "\n",
        "2. Usa el clasificador lineal OLS con el conjunto de datos entrenamiento balanceado usado en la sesi√≥n (el de tama√±o 1476). Reporta las 4 m√©tricas de rendimiento, as√≠ como la matriz de confusi√≥n.\n",
        "\n",
        "3. Entrena un nuevo clasificador SVM en todo el conjunto sesgado. \n",
        "\n",
        "    3.1. Separa el conjunto completo en 75% de entrenamiento y 20% de prueba.\n",
        "\n",
        "    3.2. Entrena un nuevo modelo en este nuevo conjunto de entrenamiento y obten las predicciones en el conjunto de prueba. \n",
        "    \n",
        "    3.3 Reporta las 4 m√©tricas de rendimiento, as√≠ como la matriz de confusi√≥n. \n",
        "    \n",
        "    Puedes usar t√©cnicas de re-escalamiento, gridsearch, selecci√≥n de features. Puedes usar un pipeline para facilitar el proceso.\n",
        "\n",
        "Redacta una conclusi√≥n comparando el desempe√±o de la parte 1 y la parte 3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7y7HAzbbWOKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6moxfCAm20mP"
      },
      "source": [
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
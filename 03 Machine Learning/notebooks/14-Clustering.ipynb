{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5y98twIUC-T"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP_2022/blob/main/03%20Machine%20Learning/notebooks/14-Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdQaBQt7UC-X"
      },
      "source": [
        "<h1>Clustering</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpuaRVvnUC-c"
      },
      "source": [
        "* El an√°lisis de agrupamiento, o agrupamiento, es una tarea de aprendizaje autom√°tico no supervisada.\n",
        "\n",
        "* Implica descubrir autom√°ticamente la agrupaci√≥n natural de los datos. A diferencia del aprendizaje supervisado (como el modelado predictivo), los algoritmos de agrupaci√≥n solo interpretan los datos de entrada y encuentran grupos o agrupaciones naturales en el espacio de caracter√≠sticas.\n",
        "\n",
        "* Un grupo, o cluster, es un en el espacio de caracter√≠sticas donde las instancias est√°n m√°s cerca del grupo que de otros grupos.\n",
        "\n",
        "* Es probable que estos grupos reflejen alg√∫n mecanismo en funcionamiento en el dominio del que se extraen las instancias, un mecanismo que hace que algunas instancias tengan un parecido m√°s fuerte entre s√≠ que con las instancias restantes.\n",
        "\n",
        "* La agrupaci√≥n en cl√∫sters puede ser √∫til como actividad de an√°lisis de datos para obtener m√°s informaci√≥n sobre el dominio del problema, el llamado descubrimiento de patrones o descubrimiento de conocimiento.\n",
        "\n",
        "* El agrupamiento tambi√©n puede ser √∫til como un tipo de ingenier√≠a de caracter√≠sticas, donde los ejemplos existentes y nuevos se pueden mapear y etiquetar como pertenecientes a uno de los grupos identificados en los datos.\n",
        "\n",
        "* La evaluaci√≥n de los grupos identificados es subjetiva y puede requerir un experto en el dominio, aunque existen muchas medidas cuantitativas espec√≠ficas de los grupos.\n",
        "\n",
        "&#128214; <u>Referencias bibliogr√°ficas</u>:\n",
        "* Flach, Peter (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of Data. Cambridge University Press.\n",
        "\n",
        "[Algoritmos de clustering en scikit-learn](https://scikit-learn.org/stable/modules/clustering.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy_dshVLUC-g"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recuerda la simbolog√≠a de las secciones:\n",
        "\n",
        "* üîΩ Esta secci√≥n no forma parte del proceso usual de Machine Learning. Es una exploraci√≥n did√°ctica de alg√∫n aspecto del funcionamiento del algoritmo.\n",
        "* ‚ö° Esta secci√≥n incluye t√©cnicas m√°s avanzadas destinadas a optimizar o profundizar en el uso de los algoritmos.\n",
        "* ‚≠ï Esta secci√≥n contiene un ejercicio o pr√°ctica a realizar. A√∫n si no se establece una fecha de entrega, es muy recomendable realizarla para practicar conceptos clave de cada tema."
      ],
      "metadata": {
        "id": "Aow9Mtnl20yW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrhsqwc9UC-j"
      },
      "source": [
        "# Demostraci√≥n ilustrativa de algunos algoritmos de agrupamiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN"
      ],
      "metadata": {
        "id": "bMNIcQ3lhySs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5cjPkORUC-k"
      },
      "source": [
        "Generaci√≥n de datos sint√©ticos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIrvqFYVUC-k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# X, y = make_blobs(n_samples=500,centers=3, random_state=24)\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
        "axs[0].scatter(X[:, 0], X[:, 1],c='black')\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "axs[0].set_title(\"Datos para clustering\")\n",
        "axs[1].scatter(X[:, 0], X[:, 1],c=y)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "axs[1].set_title(\"Datos etiquetados\")\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferencia del aprendizaje supervisado, en los m√©todos de clustering (implementados en scikit-learn) el proceso es de la siguiente manera:\n",
        "\n",
        "1. Inicializar el objeto, por ejemplo `modelo = KMeans(n_clusters=3)`.\n",
        "2. Hacer fit: `modelo.fit(X)`. Hay dos opciones:\n",
        "    * Obtener la lista de etiquetas de clusters como `y_clusters = modelo.predict(X)`.\n",
        "    * Obtener la lista de etiquetas de clusters usando el atributo `labels_` como `y_clusters = modelo.labels_`.\n",
        "\n",
        "Tambi√©n pueden obtenerse las etiquetas de los clusters directamente con el m√©todo `fit_transform()`.\n",
        "\n",
        "En esta notebook estaremos usando indistintamente los 3 m√©todos para ejemplificar su uso."
      ],
      "metadata": {
        "id": "-HKVaJOGLe8e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XewRUibwUC-u"
      },
      "source": [
        "## [K-MEANS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probemos varios valores de `n_clusters`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s_I4uuysgJbU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diS9kzsmUC-u"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "modelo = KMeans(n_clusters=3)\n",
        "modelo.fit(X)\n",
        "y_clusters = modelo.predict(X)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0], X[:,1], c=y_clusters)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos la tarea de clustering usando la m√©trica de [score de silueta](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html):"
      ],
      "metadata": {
        "id": "hdI7D6OnzY0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_score(X, y_clusters)"
      ],
      "metadata": {
        "id": "OzgZ1rbSzYfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos la tarea de clustering usando la m√©trica de [adjusted mutual information (AMI)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html).\n",
        "\n",
        "Recuerda que, para esta m√©trica, necesitamos dos clusterings que comparar. Idealmente, uno es el *ground-truth clustering*. Para esto usemos las etiquetas `y`."
      ],
      "metadata": {
        "id": "Gq3rmqrG16JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_mutual_info_score\n",
        "\n",
        "adjusted_mutual_info_score(y, y_clusters)"
      ],
      "metadata": {
        "id": "G0ni5_5g2Hgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estamos comparando estos clusterings:"
      ],
      "metadata": {
        "id": "gjE9jegq2Xt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Estamos comparando estos dos clusterings\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
        "axs[0].scatter(X[:, 0], X[:, 1],c=y)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "axs[0].set_title(\"Ground truth\")\n",
        "axs[1].scatter(X[:, 0], X[:, 1],c=y_clusters)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "axs[1].set_title(\"K-means\")\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Tr__5Fob2ZzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîΩ Veamos los centroides de cada cluster"
      ],
      "metadata": {
        "id": "PyyDVjX57DQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centers = modelo.cluster_centers_\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1],c=y_clusters)\n",
        "plt.scatter(centers[:,0],centers[:,1],color='black', marker='x',s=80)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2N1VQf9z6ZUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° ¬øC√≥mo sabemos cu√°ntos clusters buscar?\n",
        "\n",
        "Cuando los m√©todos de clustering requieren, como hiperpar√°metro, el n√∫mero de clusters a encontrar, podemos realizar cualquiera de los siguientes an√°lisis."
      ],
      "metadata": {
        "id": "NMispnH_B-24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Elbow value\n",
        "\n",
        "‚ùó S√≥lo es para K-Means"
      ],
      "metadata": {
        "id": "olDjYLtJ07XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_num_clusters = 20\n",
        "\n",
        "inertias = []\n",
        "k_values = list(range(1,max_num_clusters))\n",
        "for k in k_values:\n",
        "    modelo = KMeans(n_clusters=k, n_init='auto')\n",
        "    modelo.fit(X)\n",
        "    inertias.append(modelo.inertia_)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,inertias,color='red')\n",
        "plt.axvline(x=3,linestyle='dashed',color='gray')\n",
        "plt.ylabel(\"Inertia\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.xticks(k_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ugk8MAJT7IuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### An√°lisis de silueta\n",
        "\n",
        "**Funciona para cualquier m√©todo**\n",
        "\n",
        "Tambi√©n podemos usar el **score de silueta**, el cual es un valor $-1\\leq s\\leq 1$ que mide que tan coherente son los puntos dentro de sus propios clusters, en t√©rminos de las distancias a los dem√°s clusters. Entre m√°s alto el valor, la configuraci√≥n del cl√∫ster es apropiada. **Este score es intr√≠nseco del clustering**.\n",
        "\n",
        "Usaremos la implementaci√≥n de [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)."
      ],
      "metadata": {
        "id": "XIArcI2_ECzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "siluetas = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto').fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    siluetas.append(silhouette_score(X, labels, metric='euclidean'))\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,siluetas,color='red')\n",
        "plt.axvline(x=3,linestyle='dashed',color='gray')\n",
        "plt.xticks(k_values)\n",
        "plt.ylabel(\"Silhoutte Scores\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A7mTsL6mDQXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### An√°isis de Adjusted Mutual Information (AMI)\n",
        "\n",
        "Buscamos el clustering con el mayor valor de AMI.\n",
        "\n",
        "Es un score que asigna una puntuaci√≥n a la comparaci√≥n entre dos clusterings. La Informaci√≥n Mutua Ajustada mide que tanta informaci√≥n comparten dos clusterings en t√©rminos de los elementos que comparten, es decir, del tama√±o de la intersecci√≥n.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2A2Vmdrozb63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_mutual_info_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "scores = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto')\n",
        "    kmeans.fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    scores.append(adjusted_mutual_info_score(y, labels))\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,scores,color='red')\n",
        "plt.axvline(x=3,linestyle='dashed',color='gray')\n",
        "plt.xticks(k_values)\n",
        "plt.ylabel(\"Adjusted Mutual Information\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dfYSiusEy5_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, en los tres casos se valida la hip√≥tesis de que el mejor valor para $K$ es $K=3$. Esta hip√≥tesis tiene mayor peso por el conocimiento previo del problema, es decir, al generar los datos sabiamos que teniamos 3 grupos de puntos."
      ],
      "metadata": {
        "id": "AYVmcBtfEq1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîΩ K-Means es sensible a outliers"
      ],
      "metadata": {
        "id": "9RH5O2sb2awy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=500,centers=3, random_state=174)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DpExNQY52XnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xp = X.copy()\n",
        "\n",
        "Xp[2] = Xp[2] + [4,0]\n",
        "Xp[5] = Xp[5] + [4.25,-1]\n",
        "Xp[1] = Xp[1] + [-3,0]\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(Xp[:,0],Xp[:,1],c=y)\n",
        "plt.scatter(Xp[[1,2,5],0],Xp[[1,2,5],1],marker='x',color='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O9Tkyiyc27o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "kmeans_p = KMeans(n_clusters=3)\n",
        "kmeans_p.fit(Xp)\n",
        "labels_p = kmeans_p.labels_\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(9,5),sharey=True)\n",
        "axs[0].scatter(X[:,0],X[:,1],c=labels)\n",
        "axs[1].scatter(Xp[:,0],Xp[:,1],c=labels_p)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "FMz7ly403R7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "siluetas_1 = []\n",
        "siluetas_2 = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    kmeans_1 = KMeans(n_clusters=k, n_init='auto').fit(X)\n",
        "    labels_1 = kmeans_1.labels_\n",
        "    siluetas_1.append(silhouette_score(X, labels_1, metric='euclidean'))\n",
        "    kmeans_2 = KMeans(n_clusters=k, n_init='auto').fit(X)\n",
        "    labels_2 = kmeans_2.labels_\n",
        "    siluetas_2.append(silhouette_score(Xp, labels_2, metric='euclidean'))\n",
        "\n",
        "fig, axs = plt.subplots(1,2)\n",
        "fig.suptitle(\"Silhoutte Scores\")\n",
        "axs[0].plot(k_values,siluetas_1,color='red')\n",
        "axs[0].set_title(\"Original dataset\")\n",
        "axs[1].plot(k_values,siluetas_2,color='red')\n",
        "axs[1].set_title(\"Con outliers\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uhgLD5P074NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT3pieSEUC-p"
      },
      "source": [
        "## [Clustering Jer√°rquico](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
        "\n",
        "* La agrupaci√≥n jer√°rquica es una familia general de algoritmos de agrupaci√≥n que crean agrupaciones anidadas fusion√°ndolas o dividi√©ndolas sucesivamente. Esta jerarqu√≠a de grupos se representa como un √°rbol (o dendrograma). La ra√≠z del √°rbol es el grupo √∫nico que re√∫ne todas las muestras, siendo las hojas los grupos con una sola muestra.\n",
        "* La implementaci√≥n [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) de scikit-learn realiza una agrupaci√≥n jer√°rquica utilizando un enfoque ascendente (bottom-up): cada observaci√≥n comienza en su propio grupo, y los grupos se fusionan sucesivamente. Los criterios de vinculaci√≥n (_linkage_) determinan la m√©trica utilizada para la estrategia de fusi√≥n:\n",
        "  - _Ward_ minimiza la suma de las diferencias al cuadrado dentro de todos los grupos. Es un enfoque que minimiza la varianza y, en este sentido, es similar a la funci√≥n objetivo de k-means pero se aborda con un enfoque jer√°rquico aglomerativo.\n",
        "  - _Maximum_ o _complete Linkage_ minimiza la distancia m√°xima entre observaciones de pares de grupos.\n",
        "  - _Average linkage_ minimiza el promedio de las distancias entre todas las observaciones de pares de grupos.\n",
        "  - _Single linkage_ minimiza la distancia entre las observaciones m√°s cercanas de pares de grupos.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Se puede especificar, ya sea el n√∫mero de clusters o el umbral de distancia m√°xima:\n",
        "\n",
        "* Si `n_clusters`$\\geq2$ entonces nos regresa ese n√∫mero de clusters.\n",
        "* Si `n_clusters`=None, hay que especificar un `distance_threshold`.\n",
        "* Si `distance_threshold`$\\neq$None, `n_clusters` debe ser None y `compute_full_tree` debe ser True.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIZ_4p5HUC-q"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# modelo = AgglomerativeClustering(n_clusters=3)\n",
        "# no_dendo = True\n",
        "\n",
        "# descomenta la siguiente l√≠nea si quieres ver un dendograma\n",
        "modelo = AgglomerativeClustering(distance_threshold=5.0,\n",
        "                                 n_clusters=None,\n",
        "                                 compute_full_tree=True)\n",
        "no_dendo=False\n",
        "\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "plt.show()\n",
        "\n",
        "print(f\"{len(clusters)} clusters encontrados.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambi√©n podemos usar criterios externos para escoger un n√∫mero de clusters adecuado. Por ejemplo, podemos usar el score de silueta. La conclusi√≥n es usar 3 clusters."
      ],
      "metadata": {
        "id": "6SeUcJL4H6VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "siluetas = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    ac = AgglomerativeClustering(n_clusters=k)\n",
        "    ac.fit(X)\n",
        "    labels = ac.labels_\n",
        "    siluetas.append(silhouette_score(X, labels, metric='euclidean'))\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,siluetas,color='red')\n",
        "plt.axvline(x=3,linestyle='dashed',color='gray')\n",
        "plt.xticks(k_values)\n",
        "plt.ylabel(\"Silhoutte Scores\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "okjT7LBkHfle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMSw79gBUC-r"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
        "                                      counts]).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "if not no_dendo:\n",
        "    plt.figure(dpi=120)\n",
        "    plt.title('Hierarchical Clustering Dendrogram')\n",
        "    # plot the top three levels of the dendrogram\n",
        "    plot_dendrogram(modelo, truncate_mode='level', p=3)\n",
        "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JogoUSPjUC-t"
      },
      "source": [
        "## [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
        "* [Martin Ester, Hans-Peter Kriegel, J√∂rg Sander, Xiaowei Xu (1996). _A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise_. Proceedings of Knowledge Discovery and Databases - The International Conference on Knowledge Discovery & Data Mining.](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)\n",
        "* DBSCAN recurre a una noci√≥n de c√∫mulos basada en la densidad de los mismos, que est√° dise√±ada para descubrir grupos de formas arbitrarias. DBSCAN requiere solo un par√°metro de entrada $\\varepsilon$, el cual determina la distancia m√°xima entre dos puntos para considerarse cercanos. El otro par√°metro importante es el `min_samples` el cual representa el n√∫mero m√≠nimo de puntos que puede haber en un cluster.\n",
        "\n",
        "* DBSCAN reconoce ruido. Es decir, puede dejar puntos sin asignar a ning√∫n cluster.\n",
        "\n",
        "* \"_La raz√≥n principal por la que reconocemos los grupos, es que dentro de cada grupo tenemos una densidad t√≠pica de puntos que es considerablemente m√°s alta que fuera del grupo. Adem√°s, la densidad dentro de las √°reas de ruido es menor que la densidad en cualquiera de los grupos._\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1LX9VnhUC-t"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "modelo = DBSCAN(eps=0.530, min_samples=9)\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = [j for j in np.unique(yhat) if j!=-1]    # Todos los clusters que no son ruido\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[yhat==-1,0],X[yhat==-1,1],marker='x',color='gray',label='ruido')\n",
        "for cluster in clusters:\n",
        "    filas = np.where(yhat == cluster)\n",
        "    plt.scatter(X[filas, 0], X[filas, 1])\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬øC√≥mo denota DBSCAN a los puntos que no pertenecen a ning√∫n cluster (ruido)?"
      ],
      "metadata": {
        "id": "KpTEpVasivPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(yhat)"
      ],
      "metadata": {
        "id": "ps_FP5NHivAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una particularidad de DBSCAN es la susceptibilidad de los resultados en funci√≥n del principal par√°metro $\\varepsilon$."
      ],
      "metadata": {
        "id": "Ldbjw07gv4sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valores_eps = np.linspace(0.0001,10,100)\n",
        "num_clusters = []\n",
        "\n",
        "for eps in valores_eps:\n",
        "    modelo = DBSCAN(eps=eps, min_samples=5)\n",
        "    yhat = modelo.fit_predict(X)\n",
        "    clusters = np.unique(yhat)\n",
        "    num_clusters.append(len(clusters))\n",
        "\n",
        "plt.plot(valores_eps,num_clusters)\n",
        "plt.suptitle(\"N√∫mero de clusters en funci√≥n del par√°metro eps\")\n",
        "plt.xlabel(\"eps\",fontsize=15)\n",
        "plt.ylabel(\"N√∫mero de clusters\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "etCJvkj_Rcc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîΩ Comparaci√≥n de K-Means, AgglomerativeClustering y DBSCAN en datasets grandes"
      ],
      "metadata": {
        "id": "OgQLbw2a4cp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=15000,centers=3, random_state=174)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1],c='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dJOUl_US4kOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "\n",
        "modelos = [KMeans(n_clusters=3),AgglomerativeClustering(n_clusters=3),DBSCAN()]\n",
        "\n",
        "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
        "\n",
        "for modelo,ax in zip(modelos,axs):\n",
        "    inicio = time.time()\n",
        "    modelo.fit(X)\n",
        "    final = time.time()\n",
        "    print(f\"{modelo.__class__.__name__}\\tTiempo de ejecuci√≥n: {final-inicio}\")\n",
        "    clusters = modelo.labels_\n",
        "    ax.scatter(X[:,0],X[:,1],c=clusters)\n",
        "    ax.set_title(f\"{modelo.__class__.__name__}\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "AIMIZCoziRUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones**\n",
        "\n",
        "* KMeans es bueno con datasets grandes.\n",
        "* AgglomerativeClustering tarda con datasets grandes.\n",
        "* DBSCAN es el mejor con datasets grandes."
      ],
      "metadata": {
        "id": "vfQcOuopkXq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Otros m√©todos"
      ],
      "metadata": {
        "id": "68s4pOjOw8mF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PeZ1RewUC-m"
      },
      "source": [
        "### [Affinity Propagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html)\n",
        "\n",
        "* [Brendan J. Frey, Delbert Dueck (2007). Clustering by passing messages between data points. Science 315 (5814); pp. 972-6.](https://pdfs.semanticscholar.org/ea78/2c8b0848987e9575ea648e0419054d3f5bbf.pdf?_ga=2.62870572.1030401696.1591021245-1055786045.1581021538)\n",
        "\n",
        "* AffinityPropagation crea clusters enviando mensajes entre pares de muestras hasta la convergencia. Los mensajes enviados entre pares representan la idoneidad de una muestra para ser el ejemplar de la otra, que se actualiza en respuesta a los valores de otros pares. Esta actualizaci√≥n se produce de forma iterativa hasta la convergencia, momento en el que se eligen los ejemplares definitivos y, por tanto, se obtiene la agrupaci√≥n final.\n",
        "\n",
        "* AffinityPropagation puede ser interesante, ya que elige el n√∫mero de clusters en funci√≥n de los datos proporcionados. Para ello, los dos par√°metros importantes son la preferencia, que controla cu√°ntos ejemplares se utilizan, y el factor de amortiguaci√≥n, que amortigua los mensajes de responsabilidad y disponibilidad para evitar oscilaciones num√©ricas al actualizar estos mensajes.\n",
        "\n",
        "* El principal inconveniente de la propagaci√≥n por afinidad es su complejidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEEowNZbUC-o"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "modelo = AffinityPropagation(damping=0.5,max_iter=500,random_state=None)\n",
        "\n",
        "modelo.fit(X)\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "print(f\"Se encontraron {clusters.shape[0]} clusters\")\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ywQ1YzUC-r"
      },
      "source": [
        "### [BIRCH](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html)\n",
        "\n",
        "* Tian Zhang, Raghu Ramakrishnan and Miron Livny (1996). _BIRCH: An Efficient Data Clustering Method for Very Large Databases_.  ACM SIGMOD Record. DOI:10.1145/235968.233324.\n",
        "* BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) es un m√©todo de clustering de tipo jer√°rquico bottom-up, especialmente dise√±ado para grandes conjuntos de datos.\n",
        "* BIRCH agrupa de forma incremental y din√°mica datos, con la m√©trica Euclidiana, de entrada para intentar producir la mejor calidad de agrupaci√≥n con los recursos disponibles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onKzt5_aUC-s"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import Birch\n",
        "\n",
        "modelo = Birch(threshold=0.01, n_clusters=3)\n",
        "\n",
        "modelo.fit(X)\n",
        "yhat = modelo.predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [OPTICS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html)\n",
        "\n",
        "OPTICS (Ordering Points To Identify the Clustering Structure) est√° muy relacionado con DBSCAN, tambi√©n encuentra puntos *nucleo* y expande a partir de ellos. A diferencia de DBSCAN, mantiene una jerarqu√≠a de clusters para un intervalo de radios peque√±os. Es una mejor alternativa a DBSCAN en datasets grandes."
      ],
      "metadata": {
        "id": "Yk-Fo_ZkRGLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "modelo = OPTICS(max_eps=200, min_samples=9)\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    filas = np.where(yhat == cluster)\n",
        "    plt.scatter(X[filas, 0], X[filas, 1])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bDeYrKNhRFbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = []\n",
        "valores_eps = np.linspace(0.0001,200,100)\n",
        "\n",
        "for eps in valores_eps:\n",
        "    modelo = OPTICS(eps=eps, min_samples=5)\n",
        "    yhat = modelo.fit_predict(X)\n",
        "    clusters = np.unique(yhat)\n",
        "    num_clusters.append(len(clusters))\n",
        "\n",
        "plt.plot(valores_eps,num_clusters)\n",
        "plt.suptitle(\"N√∫mero de clusters en funci√≥n del par√°metro eps\")\n",
        "plt.xlabel(\"eps\",fontsize=15)\n",
        "plt.ylabel(\"N√∫mero de clusters\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e0BzdpuQSVXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY5OUxZbUC-u"
      },
      "source": [
        "### [Spectral Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)\n",
        "\n",
        "* La agrupaci√≥n espectral es una clase general de m√©todos de agrupaci√≥n, extra√≠da del √°lgebra lineal.\n",
        "\n",
        "* \"_Una alternativa prometedora que ha surgido recientemente en varios campos es utilizar m√©todos espectrales para la agrupaci√≥n. Aqu√≠, uno usa los vectores propios m√°s altos de una matriz derivada de la distancia entre puntos._\" [Andrew Y. Ng, Michael I. Jordan and Yair Weiss (2002). _On Spectral Clustering: Analysis and an algorithm_. In ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS.](https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf)\n",
        "\n",
        "* Un review del m√©todo: https://link.springer.com/article/10.1007/s11222-007-9033-z\n",
        "\n",
        "* El hiperpar√°metro \"n_clusters\" es utilizado para especificar el n√∫mero estimado de cl√∫steres en los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knNzMY8vUC-u"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "modelo = SpectralClustering(n_clusters=2)\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## M√°s m√©tricas"
      ],
      "metadata": {
        "id": "woMLwJCXiqFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay m√°s m√©tricas que pueden usarse para el clustering. Las hay de dos tipos:\n",
        "\n",
        "1. Cuando no se conoce un clustering *ground truth*, en este caso se evalua el m√≥delo unicamente con la informaci√≥n de √©l mismo. Por ejemplo:\n",
        "\n",
        "    * [Silhoutte score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\n",
        "    * [Calinski-Harabasz Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score)\n",
        "    * [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score)\n",
        "    * ...\n",
        "\n",
        "2. Cuando se conoce un clustering *ground truth* o se quieren comparar dos clusterings. Por ejemplo:\n",
        "    * [AMI](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score)\n",
        "    * [V-measure](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score)\n",
        "    * [Rand index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html#sklearn.metrics.rand_score)\n",
        "    * ...\n"
      ],
      "metadata": {
        "id": "TwZ6xR8Ditvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚≠ï Pr√°ctica"
      ],
      "metadata": {
        "id": "xd9VdumvETie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬øPuedes encontrar buenos clusterings para los siguientes datasets?\n",
        "\n",
        "Considera dos datasets DS1 y DS2/DS3. En cada uno de ellos, prueba los siguientes m√©todos:\n",
        "\n",
        "1. K-Means\n",
        "2. AgglomerativeClustering\n",
        "3. DBSCAN\n",
        "\n",
        "Tareas a realizar:\n",
        "\n",
        "* Realiza una busqueda de hiperpar√°metros bas√°ndote en alguna m√©trica como AMI, Silhoutte, Elbow Value (para el caso de K-Means) o alguna otra.\n",
        "* Con estos hiperpar√°metros, escoge el mejor clustering de cada uno de los tres m√©todos.\n",
        "* Compara visualmente los 3 clusterings obtenidos.\n",
        "* Usando el score de silueta y el √≠ndice Calinski-Harabasz, ¬øcu√°l de los tres clusterings fue mejor?\n",
        "* En el caso del dataset DS1, tienes un *ground truth* clustering. Reporta el valor de la m√©trica [AMI](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score) y [ARI](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score)."
      ],
      "metadata": {
        "id": "U4pvJ24QjBeT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqCnMpbRUC-z"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons, make_blobs\n",
        "import numpy as np\n",
        "\n",
        "n_samples = 500\n",
        "\n",
        "DS1 = make_moons(n_samples=n_samples, noise=.05)\n",
        "DS2 = np.random.rand(n_samples, 2)\n",
        "DS3 = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=170)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# X, y = DS1\n",
        "X, y = DS3\n",
        "# X = DS2\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FrbW98Gz940R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GK6IZ8wOntck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficar los clusters:"
      ],
      "metadata": {
        "id": "Su_f19cQnuMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zepYvT6PUC-z"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1,2,figsize=(9,5),sharey=True)\n",
        "axs[0].scatter(X[:,0],X[:,1], c=y)\n",
        "axs[0].set_title(\"Original dataset\")\n",
        "axs[1].scatter(X[:,0],X[:,1], c=y_clusters)\n",
        "axs[1].set_title(\"Clustering\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 1: Documentos de Wikipedia\n",
        "\n",
        "Usaremos otra vez el dataset de documentos de Wikipedia, tomaremos la versi√≥n parcial y preprocesada de la sesi√≥n pasada.\n",
        "\n",
        "El objetivo de la pr√°ctica es segmentar los documentos en grupos con tem√°ticas similares. Para esto, usaremos algoritmos de clustering. Si las representaciones vectoriales de los documentos son *buenas*, lograremos este objetivo."
      ],
      "metadata": {
        "id": "baK_yZkgTibB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud -qq"
      ],
      "metadata": {
        "id": "vy_MugDLg2Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/03%20Machine%20Learning/data/spanish-wikipedia-dataframe.csv\"\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "df"
      ],
      "metadata": {
        "id": "ON51TIO0Tj6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos quedamos con documentos que tengan entre 50 y 200 palabras"
      ],
      "metadata": {
        "id": "97lA4vxehL0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Palabras'] = df['Texto'].apply(lambda x: x.split())\n",
        "df['Total'] = df['Palabras'].apply(lambda x: len(x))\n",
        "df.drop(columns=['Palabras'],inplace=True)\n",
        "\n",
        "df = df[(df['Total'] < 200) & (df['Total'] > 50)]\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "Mk9O0MoXV5PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construimos la matriz BOW de los documentos y hacemos PCA para obtener una matriz de caracteristicas (num√©ricas continuas)."
      ],
      "metadata": {
        "id": "I6zM-rCnhQ3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "docs_list = df['Texto'].values\n",
        "\n",
        "cv = CountVectorizer(max_features=None)\n",
        "X_bow = cv.fit_transform(docs_list)\n",
        "print(X_bow.shape)\n",
        "\n",
        "pca = PCA(svd_solver='auto')\n",
        "X_pca = pca.fit_transform(np.asarray(X_bow.todense()))\n",
        "print(X_pca.shape)"
      ],
      "metadata": {
        "id": "g1r0VLlGUjN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tomamos las primeras `n_dim` componentes principales como representaci√≥n vectorial de cada documento."
      ],
      "metadata": {
        "id": "65nedGJjhcnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_dim = 20\n",
        "\n",
        "X_pca_dim = X_pca[:,:n_dim]\n",
        "print(X_pca_dim.shape)"
      ],
      "metadata": {
        "id": "v1EwVtj_VY1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos clustering a las representaciones vectoriales."
      ],
      "metadata": {
        "id": "3Mtik-oKirjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "clustering = KMeans(n_clusters=3)\n",
        "clustering.fit(X_pca_dim)\n",
        "clusters = clustering.labels_"
      ],
      "metadata": {
        "id": "Pr19w-riUksN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idxs_per_cluster = {j:np.where(clusters==j)[0] for j in np.unique(clusters)}\n",
        "documents_per_cluster = {j:df.loc[idxs_per_cluster[j],'Texto'].values for j in np.unique(clusters)}\n",
        "\n",
        "documents_per_cluster"
      ],
      "metadata": {
        "id": "clyEjbRtVprv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la finalidad de explorar el contenido de los textos de cada cluster, hacemos una nube de palabras de los documentos de cada cluster. Para esto, usamos el m√≥dulo [wordcloud](https://pypi.org/project/wordcloud/).\n",
        "\n",
        "Aqu√≠ puedes ver [ejemplos de su uso](https://github.com/amueller/word_cloud/tree/main)"
      ],
      "metadata": {
        "id": "_RQzX5gShqcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=3,figsize=(15,5),dpi=100)\n",
        "for k,ax in enumerate(axs):\n",
        "    wordcloud = WordCloud().generate(\" \".join(documents_per_cluster[k]))\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(f\"Cluster {k}\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "WCmQvHOdd2WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(f\"Score de silueta: {silhouette_score(X_pca_dim,clusters)}\")"
      ],
      "metadata": {
        "id": "bHXfJgDrjXg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùì\n",
        "\n",
        "* ¬øPuedes ver de qu√© tratan los documentos de cada cluster?\n",
        "* Prueba a cambiar el n√∫mero de clusters.\n",
        "* Prueba a cambiar el n√∫mero de dimensiones de las representaciones vectoriales.\n",
        "* Prueba a cambiar el m√©todo de clustering\n",
        "* En cada tarea de clustering, mide el coeficiente de silueta."
      ],
      "metadata": {
        "id": "HtwoFKxDiL15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 2: Canciones de Spotify"
      ],
      "metadata": {
        "id": "NrEN69yjS9Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este conjunto de datos contiene estad√≠sticas de audio de las 2.000 canciones top de Spotify. Los datos contienen alrededor de 15 columnas que describen la canci√≥n y algunas de sus cualidades. Se incluyen canciones publicadas desde 1956 hasta 2019 de algunos artistas notables y famosos. Estos datos contienen caracter√≠sticas de audio como Danceability, BPM, Liveness, Valence(Positivity) y algunas m√°s:\n",
        "\n",
        "* √çndice: ID\n",
        "* T√≠tulo: Nombre de la pista\n",
        "* Artista: Nombre del artista\n",
        "* G√©nero superior: G√©nero de la pista\n",
        "* A√±o: A√±o de lanzamiento de la pista\n",
        "* Pulsaciones por minuto (BPM): El tempo de la canci√≥n\n",
        "* Energy: La energ√≠a de una canci√≥n: cuanto m√°s alto sea el valor, m√°s energ√©tica ser√° la canci√≥n.\n",
        "* Danceability: Cuanto m√°s alto sea el valor, m√°s f√°cil ser√° bailar esta canci√≥n.\n",
        "* Loudness: Cuanto m√°s alto sea el valor, m√°s fuerte ser√° la canci√≥n.\n",
        "* Liveness: ...\n",
        "* Valence: Cuanto m√°s alto sea el valor, m√°s positivo ser√° el estado de √°nimo de la canci√≥n.\n",
        "* Duraci√≥n: La duraci√≥n de la canci√≥n.\n",
        "* Acousticness: Cuanto m√°s alto sea el valor, m√°s ac√∫stica ser√° la canci√≥n.\n",
        "* Speechiness: Cuanto m√°s alto sea el valor, m√°s palabras habladas contiene la canci√≥n.\n",
        "* Popularity: Cuanto m√°s alto sea el valor, m√°s popular es la canci√≥n.\n",
        "\n",
        "Este dataset se encuentra en [Kaggle](https://www.kaggle.com/datasets/iamsumat/spotify-top-2000s-mega-dataset)"
      ],
      "metadata": {
        "id": "YUSnLgDc6yxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'https://github.com/DCDPUAEM/DCDP/raw/main/03%20Machine%20Learning/data/spotify-2000.csv'\n",
        "df = pd.read_csv(url,index_col=0,thousands=',')\n",
        "df"
      ],
      "metadata": {
        "id": "2p5rJTnaEKAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df['Length (Duration)'].values)\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "AuqCCslwj11Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generos = df['Top Genre'].unique()\n",
        "print(f\"Hay {len(generos)} g√©neros √∫nicos:\")\n",
        "print(generos)"
      ],
      "metadata": {
        "id": "FUbx_iOg_c7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "oRqQgbox70gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A manera de an√°lisis exploratorio, veamos las correlaciones entre variables, ¬øqu√© observamos?"
      ],
      "metadata": {
        "id": "U4-A0tlA9kRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from seaborn import heatmap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "correlaciones = df.iloc[:,3:].corr()\n",
        "heatmap(correlaciones)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5of0zRUs9I24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que algunos m√©todos de clustering son susceptibles a la escala de valores, hacemos un escalamiento."
      ],
      "metadata": {
        "id": "_hud5RYP7zhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "df2 = df[[\"Beats Per Minute (BPM)\", \"Loudness (dB)\",\n",
        "              \"Liveness\", \"Valence\", \"Acousticness\",\n",
        "              \"Speechiness\"]].copy()\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df2[df2.columns] = scaler.fit_transform(df2[df2.columns])\n",
        "X = df2.values\n",
        "\n",
        "df2.head(3)"
      ],
      "metadata": {
        "id": "N_Tb3EgWTljS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.describe()"
      ],
      "metadata": {
        "id": "Cdq-yECD-rBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos K-means para segmentar en 10 grupos"
      ],
      "metadata": {
        "id": "cK5a1QqrB03t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "modelo = KMeans(n_clusters=10, n_init='auto')\n",
        "\n",
        "modelo.fit(X)\n",
        "clusters = modelo.labels_\n",
        "\n",
        "print(f\"Las primeras 10 etiquetas: {clusters[:10]}\")"
      ],
      "metadata": {
        "id": "BC2tD2N28yVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integramos la informaci√≥n de los clusters al dataframe original."
      ],
      "metadata": {
        "id": "QGUA-MzoCCDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Music Segments\"] = clusters\n",
        "df[\"Music Segments\"] = df[\"Music Segments\"].map({0: \"Cluster 1\", 1:\n",
        "    \"Cluster 2\", 2: \"Cluster 3\", 3: \"Cluster 4\", 4: \"Cluster 5\",\n",
        "    5: \"Cluster 6\", 6: \"Cluster 7\", 7: \"Cluster 8\",\n",
        "    8: \"Cluster 9\", 9: \"Cluster 10\"})\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "rhHEN2zCUo0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observemos un cluster"
      ],
      "metadata": {
        "id": "dn_xNOGSl4vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster = 'Cluster 3'\n",
        "\n",
        "df[df['Music Segments']==cluster][['Artist','Title','Top Genre','Year']]"
      ],
      "metadata": {
        "id": "MCCBqzzVl6-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver los artistas en este cluster"
      ],
      "metadata": {
        "id": "YueDpOXR59u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Music Segments']==cluster]['Artist'].unique()"
      ],
      "metadata": {
        "id": "zEI8jGX86AUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficamos usando solamente 3 features. Usamos el m√≥dulo [plotly](https://plotly.com/python/) para gr√°ficas interactivas.\n",
        "\n",
        "Otra alternativa es [Bokeh](https://bokeh.org/)."
      ],
      "metadata": {
        "id": "wcoMmKJDCHfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "PLOT = go.Figure()\n",
        "\n",
        "for i in list(df[\"Music Segments\"].unique()):\n",
        "    PLOT.add_trace(go.Scatter3d(x = df[df[\"Music Segments\"]==i]['Beats Per Minute (BPM)'],\n",
        "                                    y = df[df[\"Music Segments\"] ==i]['Energy'],\n",
        "                                    z = df[df[\"Music Segments\"] ==i]['Danceability'],\n",
        "                                    mode = 'markers',marker_size = 6, marker_line_width = 1,\n",
        "                                    name = str(i)))\n",
        "PLOT.update_traces(hovertemplate='Beats Per Minute (BPM): %{x} <br>Energy: %{y} <br>Danceability: %{z}')\n",
        "\n",
        "PLOT.update_layout(width = 800, height = 800, autosize = True, showlegend = True,\n",
        "                   scene = dict(xaxis=dict(title = 'Beats Per Minute (BPM)', titlefont_color = 'black'),\n",
        "                                yaxis=dict(title = 'Energy', titlefont_color = 'black'),\n",
        "                                zaxis=dict(title = 'Danceability', titlefont_color = 'black')),\n",
        "                   font = dict(family = \"Arial\", color  = 'black', size = 12))\n",
        "\n",
        "PLOT.show()"
      ],
      "metadata": {
        "id": "m1dXHIGFUG04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando s√≥lo dos dimensiones:"
      ],
      "metadata": {
        "id": "7okOAKI4GRMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(dpi=120)\n",
        "for segment in df[\"Music Segments\"].unique():\n",
        "    plt.scatter(x = df[df[\"Music Segments\"]==segment]['Beats Per Minute (BPM)'],\n",
        "                y = df[df[\"Music Segments\"] ==segment]['Energy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2VhzrHq0FYFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(dpi=120)\n",
        "for segment in df[\"Music Segments\"].unique():\n",
        "    plt.scatter(x = df[df[\"Music Segments\"]==segment]['Danceability'],\n",
        "                y = df[df[\"Music Segments\"] ==segment]['Energy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B5nypRH6GVDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(f\"Score de silueta: {silhouette_score(X,clusters)}\")"
      ],
      "metadata": {
        "id": "EKMSEGpXgP5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï Preguntas:\n",
        "* Siendo K-Means, ¬øpor qu√© se no se ve la separaci√≥n perfecta?\n",
        "\n",
        "‚≠ï Ejercicio 1. Continuando con este m√©todo de K-Means:\n",
        "* ¬øQu√© valor de K es mejor? Puedes usar cualquiera de los 3 criteros de arriba, empezando por el *elbow value*.\n",
        "* Una vez que hayas escogido un valor para $K$, reportar los valores de las m√©tricas de clustering: score de Silueta, [Calinski-Harabasz Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score) y [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
        "\n",
        "‚≠ï Ejercicio 2:\n",
        "\n",
        "* Repetir el experimento, ahora usando Agglomerative Clustering y DBSCAN.\n",
        "* ¬øPuedes elevar las m√©tricas de clustering? Considera las m√©tricas score de Silueta y [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
        "* Explora visualmente algunas canciones de los clusters, ¬øtiene sentido el agrupamiento?"
      ],
      "metadata": {
        "id": "ACYNyhKPefTU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2gwkihWG6Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 3: `20newsgroups`"
      ],
      "metadata": {
        "id": "0saXLbUX3uh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retomamos el corpus de documentos `20newsgroups`. Realizaremos un clustering en las representaciones de los documentos para ver qu√© documentos se agrupan juntos.\n",
        "\n",
        "Este dataset se encuentra disponible en sklearn, [documentaci√≥n](https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)."
      ],
      "metadata": {
        "id": "TvuZKGxMXqlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Descargamos el dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "data_full = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),\n",
        "                               categories=['soc.religion.christian','sci.space','rec.autos'])"
      ],
      "metadata": {
        "id": "-1ujSDe-4xPx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = data_full.data\n",
        "print(f\"N√∫mero de documentos: {len(docs)}\")\n",
        "topics = data_full.target\n",
        "print(f\"N√∫mero de t√≥picos: {np.unique(topics).shape[0]}\")"
      ],
      "metadata": {
        "id": "Y5Z3lysw4-D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:2]"
      ],
      "metadata": {
        "id": "TfektfBKjNd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Representaciones *term-document* y *tf-idf*\n",
        "\n",
        "Hemos visto que los documentos se pueden representar por medio de vectores *sparse* cuyas componentes son conteos de las apariciones de ciertas palabras. Es decir, el modelo **BOW** (bag of words).\n",
        "\n",
        "Otra forma parecida de representar documentos es por medio de vectores cuyas componentes ahora representan √≠ndices *tf-idf* (term frequency - inverse document frequency). Estas tienen la ventaja de restar importancia a palabras que aparecen en muchos documentos.\n",
        "\n",
        "Para un corpus $D$ de documentos, el √≠ndice $tfidf$ de un t√©rmino $t$ en un documento $d$ se calcula como\n",
        "\n",
        "$$tfidf(t,d,D)=tf(t,d)idf(t,D)$$\n",
        "\n",
        "donde\n",
        "\n",
        "$$tf(t,d)=\\frac{f_{t,d}}{\\sum_{s\\in d}f_{s,d}}$$\n",
        "\n",
        "$$idf(t,D)=\\log\\frac{N}{|\\{d\\in D : t\\in d\\}|}$$\n",
        "\n",
        "$f_{t,d}$ es el n√∫mero de ocurrencias de un t√©rmino $t$ en un documento $d$, $N$ es el n√∫mero de documentos en $D$."
      ],
      "metadata": {
        "id": "pJ2riMEvdhLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "corpus = [\n",
        "        'This is the first document.',\n",
        "        'This document is the second document.',\n",
        "        'And this is the third one.',\n",
        "        'Is this the first document?',\n",
        "        ]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(f\"Vocabulario:\\n{tfidf_vectorizer.get_feature_names_out()}\")\n",
        "print(f\"Es una matriz sparse:\\n{X_tfidf}\")\n",
        "print(f\"Primeras tres columnas:\\n{X_tfidf.todense()[:,:3]}\")"
      ],
      "metadata": {
        "id": "s1X2lUBMfxFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos el modelo **BOW**"
      ],
      "metadata": {
        "id": "YqaG2PNowXpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_counts = count_vectorizer.fit_transform(corpus)\n",
        "print(f\"El vocabulario:\\n{count_vectorizer.get_feature_names_out()}\")\n",
        "print(f\"Las primeras tres columnas:\\n{X_counts.todense()[:,:3]}\")"
      ],
      "metadata": {
        "id": "9eph-7ZsglIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ],
      "metadata": {
        "id": "qe3PsrBrdqKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traemos la funci√≥n para limpiar texto de hace algunas sesiones"
      ],
      "metadata": {
        "id": "nelHZO35lj9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/03%20Machine%20Learning/data/limpiador_texto.py\"\n",
        "!wget --no-cache --backups=1 {url}"
      ],
      "metadata": {
        "id": "O8hfDrERlecE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download\n",
        "\n",
        "download('stopwords')\n",
        "download('punkt')"
      ],
      "metadata": {
        "id": "sm99y0I5lqlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from limpiador_texto import preprocesar_textos\n",
        "\n",
        "clean_docs = preprocesar_textos(docs)"
      ],
      "metadata": {
        "id": "MO7c9yE4l0iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos el clustering con las dos representaciones y calculamos las m√©tricas de rendimiento del clustering."
      ],
      "metadata": {
        "id": "WwvyWWKmlw3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                             max_features=200)\n",
        "X_tfidf = vectorizer.fit_transform(docs)\n",
        "# X_tfidf = vectorizer.fit_transform(clean_docs)\n",
        "print(X_tfidf.shape)\n",
        "\n",
        "clustering = KMeans(n_clusters=3, n_init='auto')\n",
        "clustering.fit(X_tfidf)\n",
        "clusters = clustering.labels_\n",
        "\n",
        "print(f\"AMI: {adjusted_mutual_info_score(topics,clusters)}\")\n",
        "print(f\"AR: {adjusted_rand_score(topics,clusters)}\")\n",
        "print(f\"Silhoutte del clustering obtenido: {silhouette_score(X_tfidf,clusters)}\")\n",
        "print(f\"Silhoutte de los t√≥picos: {silhouette_score(X_tfidf,topics)}\")"
      ],
      "metadata": {
        "id": "0CTGrbYo6BwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english',\n",
        "                             max_features=200)\n",
        "# X_counts = vectorizer.fit_transform(docs)\n",
        "X_counts = vectorizer.fit_transform(clean_docs)\n",
        "print(X_counts.shape)\n",
        "\n",
        "clustering = KMeans(n_clusters=3, n_init='auto',random_state=57)\n",
        "clustering.fit(np.asarray(X_counts.todense()))\n",
        "clusters_counts = clustering.labels_\n",
        "\n",
        "print(f\"AMI: {adjusted_mutual_info_score(topics,clusters_counts)}\")\n",
        "print(f\"AR: {adjusted_rand_score(topics,clusters_counts)}\")\n",
        "print(f\"Silhoutte del clustering obtenido: {silhouette_score(np.asarray(X_counts.todense()),clusters_counts)}\")\n",
        "print(f\"Silhoutte de los t√≥picos: {silhouette_score(np.asarray(X_counts.todense()),topics)}\")"
      ],
      "metadata": {
        "id": "nKV0jP58FYYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english',\n",
        "                             max_features=200)\n",
        "# X_counts = vectorizer.fit_transform(docs)\n",
        "X_counts = vectorizer.fit_transform(clean_docs)\n",
        "print(X_counts.shape)\n",
        "\n",
        "pca = PCA(svd_solver='auto',n_components=30)\n",
        "X_pca = pca.fit_transform(np.asarray(X_counts.todense()))\n",
        "print(X_pca.shape)\n",
        "\n",
        "clustering = KMeans(n_clusters=3, n_init='auto',random_state=57)\n",
        "clustering.fit(np.asarray(X_pca))\n",
        "clusters_counts = clustering.labels_\n",
        "\n",
        "print(f\"AMI: {adjusted_mutual_info_score(topics,clusters_counts)}\")\n",
        "print(f\"AR: {adjusted_rand_score(topics,clusters_counts)}\")\n",
        "print(f\"Silhoutte del clustering obtenido: {silhouette_score(X_pca,clusters_counts)}\")\n",
        "print(f\"Silhoutte de los t√≥picos: {silhouette_score(X_pca,topics)}\")"
      ],
      "metadata": {
        "id": "-ujg9kANLDN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podr√≠amos explorar el comportamiento del par√°metro `n_clusters` usando los criterios de intertia y silhoutte."
      ],
      "metadata": {
        "id": "xBGsfhT5hXVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sils = []\n",
        "inertias = []\n",
        "\n",
        "for n in range(2,20):\n",
        "    clustering = KMeans(n_clusters=n, n_init='auto')\n",
        "    clustering.fit(np.asarray(X_counts.todense()))\n",
        "    # sil = silhouette_score(np.asarray(X_counts.todense()),clustering.labels_)\n",
        "    # sils.append(sil)\n",
        "    inertias.append(clustering.inertia_)\n",
        "\n",
        "\n",
        "plt.plot(list(range(2,20)),inertias)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bpecWbud1DOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploraci√≥n de los clusters"
      ],
      "metadata": {
        "id": "ukpKNSfcdsad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos probar algunas t√©cnicas adicionales para explorar los clusters"
      ],
      "metadata": {
        "id": "VQz3WBlK5nuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq wordcloud"
      ],
      "metadata": {
        "id": "XkLYugf8Vn1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=3,figsize=(15,10),dpi=100)\n",
        "for k,ax in enumerate(axs):\n",
        "    cluster_text = \" \".join([clean_docs[j] for j,cluster in enumerate(clusters_counts) if cluster==k])\n",
        "    wc = WordCloud().generate(cluster_text)\n",
        "    ax.set_title(f\"Cluster {k}\")\n",
        "    ax.imshow(wc, interpolation='bilinear')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "whFCsDYgYwp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprimimos algunos documentos en cada cluster"
      ],
      "metadata": {
        "id": "6E8NOEer7UEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(43)\n",
        "\n",
        "for k in range(3):\n",
        "    docs_in_cluster = [clean_docs[j] for j,cluster in enumerate(clusters_counts) if cluster==k]\n",
        "    print(f\"Cluster {k} {20*'-'}\")\n",
        "    print(f\"N√∫mero de documentos en el cluster: {clusters_counts[clusters_counts==k].shape[0]}\")\n",
        "    some_docs = np.random.choice(docs_in_cluster,size=3)\n",
        "    print(some_docs)"
      ],
      "metadata": {
        "id": "iYxzthhrgTyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï **Ejercicio**\n",
        "\n",
        "Repite el experimento variando el par√°metro `max_features` de los vectorizadores para ver si puedes subir las m√©tricas de rendimiento y obtener una mejor separaci√≥n de t√≥picos, esto hazlo *visualmente* (usando las nubes de palabras y algunos documentos).\n",
        "\n",
        "Usa los dos vectorizadores\n"
      ],
      "metadata": {
        "id": "PfeHRFHsmB-e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bODgImjXkmfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCaOm_vrd4qr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
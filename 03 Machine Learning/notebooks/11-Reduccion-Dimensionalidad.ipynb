{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03%20Machine%20Learning/notebooks/11-Reduccion-Dimensionalidad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF0c86Jec2AK"
      },
      "source": [
        "# Reducci√≥n de dimensionalidad\n",
        "\n",
        "Recuerda la simbolog√≠a de las secciones:\n",
        "\n",
        "* üîΩ Esta secci√≥n no forma parte del proceso usual de Machine Learning. Es una exploraci√≥n did√°ctica de alg√∫n aspecto del funcionamiento del algoritmo.\n",
        "* ‚ö° Esta secci√≥n incluye t√©cnicas m√°s avanzadas destinadas a optimizar o profundizar en el uso de los algoritmos.\n",
        "* ‚≠ï Esta secci√≥n contiene un ejercicio o pr√°ctica a realizar. A√∫n si no se establece una fecha de entrega, es muy recomendable realizarla para practicar conceptos clave de cada tema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHnR3KHEEE3t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Maldici√≥n de la dimensionalidad"
      ],
      "metadata": {
        "id": "rTyb8_SjatCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este experimento generaremos puntos aleatorios en espacios de diferentes dimensiones y calcularemos las distancias euclidianas entre ellos. Conforme aumenta la dimensionalidad, observaremos c√≥mo todas las distancias tienden a volverse muy similares entre s√≠, perdiendo la capacidad de discriminar entre puntos \"cercanos\" y \"lejanos\". Este fen√≥meno, conocido como la maldici√≥n de la dimensionalidad, explica por qu√© muchos algoritmos de machine learning (como k-NN o clustering) pierden efectividad en espacios de alta dimensi√≥n."
      ],
      "metadata": {
        "id": "jm67ZfLHwrF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Experimento maldici√≥n de la dimensionalidad\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "np.random.seed(42)\n",
        "side = 10\n",
        "n_puntos = 100\n",
        "dims = list(range(2, 500, 3))\n",
        "n_repetitions = 5\n",
        "\n",
        "average_distances = []\n",
        "min_distances = []\n",
        "max_distances = []\n",
        "std_distances = []\n",
        "relative_std = []  # Desviaci√≥n est√°ndar relativa (coeficiente de variaci√≥n)\n",
        "\n",
        "for i, dim in enumerate(dims):\n",
        "    exp_avg, exp_min, exp_max, exp_std = [], [], [], []\n",
        "\n",
        "    for _ in range(n_repetitions):\n",
        "        puntos = np.random.uniform(size=(n_puntos, dim), low=-side, high=side)\n",
        "        distances = pdist(puntos, metric='euclidean')\n",
        "\n",
        "        exp_avg.append(np.mean(distances))\n",
        "        exp_min.append(np.min(distances))\n",
        "        exp_max.append(np.max(distances))\n",
        "        exp_std.append(np.std(distances))\n",
        "\n",
        "    # Promediar los experimentos\n",
        "    avg_dist = np.mean(exp_avg)\n",
        "    average_distances.append(avg_dist)\n",
        "    min_distances.append(np.mean(exp_min))\n",
        "    max_distances.append(np.mean(exp_max))\n",
        "    std_distances.append(np.mean(exp_std))\n",
        "\n",
        "    # Coeficiente de variaci√≥n (desviaci√≥n est√°ndar relativa)\n",
        "    relative_std.append(np.mean(exp_std) / avg_dist)\n",
        "\n",
        "\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('La Maldici√≥n de la Dimensionalidad: Convergencia de Distancias', fontsize=16)\n",
        "\n",
        "# 1. Distancias absolutas:\n",
        "ax1.plot(dims, average_distances, 'b-', linewidth=2, label='Distancia promedio')\n",
        "ax1.plot(dims, min_distances, 'g--', linewidth=2, label='Distancia m√≠nima')\n",
        "ax1.plot(dims, max_distances, 'r--', linewidth=2, label='Distancia m√°xima')\n",
        "ax1.fill_between(dims, min_distances, max_distances, alpha=0.2, color='gray')\n",
        "ax1.set_xlabel('Dimensiones')\n",
        "ax1.set_ylabel('Distancia Euclidiana')\n",
        "ax1.set_title('Evoluci√≥n de las Distancias')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Raz√≥n m√°ximo/m√≠nimo:\n",
        "ratio_max_min = np.array(max_distances) / np.array(min_distances)\n",
        "ax2.plot(dims, ratio_max_min, 'purple', linewidth=2)\n",
        "ax2.set_xlabel('Dimensiones')\n",
        "ax2.set_ylabel('Ratio Distancia M√°x/M√≠n')\n",
        "ax2.set_title('Convergencia: Ratio Distancia M√°xima/M√≠nima')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Coeficiente de variaci√≥n:\n",
        "ax3.plot(dims, relative_std, 'orange', linewidth=2)\n",
        "ax3.set_xlabel('Dimensiones')\n",
        "ax3.set_ylabel('Coeficiente de Variaci√≥n')\n",
        "ax3.set_title('Variabilidad Relativa de las Distancias')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Distribuci√≥n de distancias para dimensiones espec√≠ficas\n",
        "sample_dims = [3, 10, 100, 1000]\n",
        "colors = ['blue', 'green', 'orange', 'red']\n",
        "\n",
        "for dim, color in zip(sample_dims, colors):\n",
        "    puntos_sample = np.random.uniform(size=(n_puntos, dim), low=-side, high=side)\n",
        "    distances_sample = pdist(puntos_sample, metric='euclidean')\n",
        "    ax4.hist(distances_sample,alpha=0.6, color=color,edgecolor='black',\n",
        "            label=f'{dim}D', density=True, linewidth=0.5)\n",
        "\n",
        "ax4.set_xlabel('Distancia Euclidiana')\n",
        "ax4.set_ylabel('Densidad de Probabilidad')\n",
        "ax4.set_title('Distribuci√≥n de Distancias por Dimensionalidad')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eSk3ACGcav1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto nos dice que si tenemos *muchas* features, usar distancias entre vectores se vuelve algo cada vez menos significativo. **Reducir features** se vuelve muy √∫til para combatir este problema, podemos hacerlo:\n",
        "\n",
        "1. **Seleccionando features** (selecci√≥n de features): [`SelectKBest`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html), [`RFE`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html), [`VarianceThreshold`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html)\n",
        "\n",
        "2. **Proyectando en espacios de menor dimensi√≥n** (reducci√≥n de dimensionalidad): [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), [`TSNE`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), [`UMAP`](https://umap-learn.readthedocs.io/), [`TruncatedSVD`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)\n",
        "\n",
        "En esta notebook analizaremos la segunda estrategia."
      ],
      "metadata": {
        "id": "OuKqFU3bx0Z0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAWLmShPFJae"
      },
      "source": [
        "# üîΩ Ejemplos ilustrativos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqQ9azo48MPK"
      },
      "source": [
        "## PCA\n",
        "\n",
        "Exploremos c√≥mo funciona [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) y los resultados que produce."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SwFsQzy8g-t"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXnMXy3uGkbK"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles, make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#------- definimos el dataset -------\n",
        "X,y = make_blobs(n_samples=500, centers=2, random_state=11)\n",
        "idxs = np.where(y==0)[0]\n",
        "X[idxs,:] = X[idxs,:] + [[8,5]]\n",
        "idxs = np.where(y==1)[0]\n",
        "X[idxs,:] = X[idxs,:] + [[-7,2]]\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "#----- Aplicamos PCA ----------\n",
        "pca = PCA()\n",
        "X_red = pca.fit_transform(X)  # No entrenamos con las etiquetas\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "#-------- Graficamos ----------\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,6))\n",
        "axs[0].title.set_text(\"Dataset original\")\n",
        "axs[0].scatter(X[:,0],X[:,1],color='blue')\n",
        "axs[1].title.set_text(\"Dataset original y componentes principales\")\n",
        "axs[1].scatter(X_red[:,0],X_red[:,1],alpha=0.25,color='orange',label=\"PCA (PC 1,2)\")\n",
        "axs[1].scatter(X_red[:,0],[0 for x in X_red],color='green',label=\"PCA (PC 1)\")\n",
        "axs[1].scatter(X[:,0],X[:,1],alpha=0.4,color='blue',label=\"Original set\")\n",
        "# Dibujamos los ejes\n",
        "axs[1].axhline(0,color='gray',linestyle='--')\n",
        "axs[1].axvline(0,color='gray',linestyle='--')\n",
        "axs[1].legend(loc='best')\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Graficar en HTML un dataset en 3D usando plotly\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "\n",
        "\n",
        "n_samples = 300  # Total de puntos (100 por blob)\n",
        "n_features = 3    # 3 dimensiones\n",
        "centers = 3       # 3 blobs\n",
        "\n",
        "X3d, y3d = make_blobs(n_samples=n_samples,\n",
        "                  n_features=n_features,\n",
        "                  centers=centers,\n",
        "                  cluster_std=1.5,\n",
        "                  random_state=42)\n",
        "\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# A√±adir cada blob como una traza separada (para colores distintos)\n",
        "for i in range(centers):\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=X[y == i, 0],\n",
        "        y=X[y == i, 1],\n",
        "        z=X[y == i, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=5,\n",
        "            opacity=0.8\n",
        "        ),\n",
        "        name=f'Blob {i+1}'\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='3 Blobs en 3D',\n",
        "    scene=dict(\n",
        "        xaxis_title='X',\n",
        "        yaxis_title='Y',\n",
        "        zaxis_title='Z'\n",
        "    ),\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "# Guardar como HTML\n",
        "pio.write_html(fig, file='blobs_3d.html', auto_open=True)"
      ],
      "metadata": {
        "id": "x4i9FS99AxW0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_red = pca.fit_transform(X3d)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title(\"Dataset visualizado con 2 PC\")\n",
        "plt.scatter(X_red[:,0],X_red[:,1],c=y3d)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h11XwOZPBLqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SamHuym7f2l9"
      },
      "source": [
        "En este ejemplo podemos ver la perdida de informaci√≥n del conjunto original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAmEf-LaPpSf"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X,_ = make_moons(n_samples=500,random_state=1945,noise=0.1)\n",
        "\n",
        "X = X + [[3,3]]  # Trasladamos todo el dataset para fines de visualizaci√≥n\n",
        "\n",
        "pca = PCA()\n",
        "X_red = pca.fit_transform(X)\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,6))\n",
        "axs[0].title.set_text(\"Dataset original\")\n",
        "axs[0].scatter(X[:,0],X[:,1],color='blue')\n",
        "axs[1].title.set_text(\"Dataset original y componentes principales\")\n",
        "axs[1].scatter(X_red[:,0],X_red[:,1],alpha=0.25,color='orange',label=\"PCA (PC 1,2)\")\n",
        "axs[1].scatter(X_red[:,0],[0 for x in X_red],color='green',label=\"PCA (PC 1)\")\n",
        "axs[1].scatter(X[:,0],X[:,1],alpha=0.4,color='blue',label=\"Original set\")\n",
        "# Dibujamos los ejes\n",
        "axs[1].axhline(0,color='gray')\n",
        "axs[1].axvline(0,color='gray')\n",
        "axs[1].legend(loc='best')\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp13Sq4bRe-b"
      },
      "source": [
        "## t-SNE\n",
        "\n",
        "La implementaci√≥n de t-SNE se encuentra en [`sklear.manifold.TSNE`](https://scikit-learn.org/0.16/modules/generated/sklearn.manifold.TSNE.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyxeadiuRlqH"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pRqJN_TXbNi"
      },
      "source": [
        "* Observar que t-SNE centra los datos en el origen.\n",
        "* Es diferente hacer t-SNE con dos componentes y tomar la primera, a hacerlo con una componente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBP4DeawRgvM"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles, make_blobs\n",
        "\n",
        "X,_ = make_blobs(n_samples=1000, centers=1, random_state=1945)\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "X_red_2 = tsne.fit_transform(X)\n",
        "print(X_red_2.shape)\n",
        "\n",
        "tsne = TSNE(n_components=1)\n",
        "X_red_1 = tsne.fit_transform(X)\n",
        "print(X_red_1.shape)\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
        "axs[0].title.set_text(\"Dataset original\")\n",
        "axs[0].scatter(X[:,0],X[:,1],color='blue')\n",
        "axs[1].title.set_text(\"Dataset original y componentes principales\")\n",
        "axs[1].scatter(X_red_2[:,0],X_red_2[:,1],alpha=0.25,color='orange',label=\"t-SNE 2dim\")\n",
        "axs[1].scatter(X_red_1[:,0],[0 for x in X_red_1],alpha=0.25,color='green',label=\"t-SNE 1dim\")\n",
        "axs[1].scatter(X[:,0],X[:,1],alpha=0.4,color='blue',label=\"Original set\")\n",
        "# Dibujamos los ejes\n",
        "axs[1].axhline(0,color='gray',linestyle='--')\n",
        "axs[1].axvline(0,color='gray',linestyle='--')\n",
        "axs[1].legend(loc='best')\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpu-3G6hT2lK"
      },
      "source": [
        "Veamos el efecto del hiper-par√°metro `perplexity`. Est√° relacionado con el n√∫mero de vecinos de cada punto que se toman en cuenta, entre m√°s grande sea el dataset, ·∏øas grande suele tomarse el par√°metro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpziLI9nSj6h"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles, make_blobs\n",
        "\n",
        "X,_ = make_blobs(n_samples=1000, centers=2, random_state=1945)\n",
        "\n",
        "tsne = TSNE(n_components=2,perplexity=5)\n",
        "X_red_2 = tsne.fit_transform(X)\n",
        "\n",
        "tsne = TSNE(n_components=1,perplexity=5)\n",
        "X_red_1 = tsne.fit_transform(X)\n",
        "\n",
        "tsne = TSNE(n_components=2,perplexity=50)\n",
        "X_red_3 = tsne.fit_transform(X)\n",
        "\n",
        "tsne = TSNE(n_components=1,perplexity=50)\n",
        "X_red_4 = tsne.fit_transform(X)\n",
        "\n",
        "fig, axs = plt.subplots(2,2)\n",
        "axs[0,0].title.set_text(\"Dataset original\")\n",
        "axs[0,0].scatter(X[:,0],X[:,1],color='blue')\n",
        "axs[0,1].title.set_text(\"TSNE 2-dim y 1-dim\")\n",
        "axs[0,1].scatter(X_red_2[:,0],X_red_2[:,1],alpha=0.25,color='orange')\n",
        "axs[0,1].scatter(X_red_1[:,0],[0 for x in X_red_1],alpha=0.25,color='green')\n",
        "# Dibujamos los ejes\n",
        "axs[0,1].axhline(0,color='gray',linestyle='--')\n",
        "axs[0,1].axvline(0,color='gray',linestyle='--')\n",
        "\n",
        "axs[1,0].scatter(X[:,0],X[:,1],color='blue')\n",
        "axs[1,1].scatter(X_red_3[:,0],X_red_3[:,1],alpha=0.25,color='orange')\n",
        "axs[1,1].scatter(X_red_4[:,0],[0 for x in X_red_4],alpha=0.25,color='green')\n",
        "# Dibujamos los ejes\n",
        "axs[1,1].axhline(0,color='gray',linestyle='--')\n",
        "axs[1,1].axvline(0,color='gray',linestyle='--')\n",
        "\n",
        "for ax in axs.flatten():\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aD2HNJ3WQyu"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X,y = make_moons(n_samples=500,random_state=1945,noise=0.1)\n",
        "\n",
        "tsne = TSNE(n_components=1,perplexity=35)\n",
        "X_red_A = tsne.fit_transform(X)\n",
        "\n",
        "tsne = TSNE(n_components=1,perplexity=7)\n",
        "X_red_B = tsne.fit_transform(X)\n",
        "\n",
        "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
        "axs[0].title.set_text(\"Dataset original\")\n",
        "axs[0].scatter(X[:,0],X[:,1],c=y)\n",
        "axs[1].title.set_text(\"TSNE: high perplexity\")\n",
        "axs[1].scatter(X_red_A[:,0],[0 for x in X_red_A],c=y)\n",
        "axs[2].title.set_text(\"TSNE: low perplexity\")\n",
        "axs[2].scatter(X_red_B[:,0],[0 for x in X_red_B],c=y)\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos el dataset en 3 dimensiones"
      ],
      "metadata": {
        "id": "kM_jq5oaCGGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "X_red = tsne.fit_transform(X3d)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title(\"Dataset visualizado con t-SNE\")\n",
        "plt.scatter(X_red[:,0],X_red[:,1],c=y3d)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GKO8ExIkB776"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVSEepzLYpFy"
      },
      "source": [
        "## ‚ö° UMAP\n",
        "\n",
        "Uniform Manifold Approximation and Projection (UMAP) es una t√©cnica de reducci√≥n dimensional que puede utilizarse para la visualizaci√≥n de forma similar a t-SNE, pero tambi√©n para la reducci√≥n dimensional no lineal general. El algoritmo se basa en tres hip√≥tesis sobre los datos\n",
        "\n",
        "* Los datos se distribuyen uniformemente en una variedad Riemanniana.\n",
        "* La m√©trica Riemanniana es localmente constante (o puede aproximarse como tal).\n",
        "* La variedad es localmente conexa.\n",
        "\n",
        "A partir de estos supuestos, es posible modelizar el colector con una estructura topol√≥gica difusa. La incrustaci√≥n se encuentra buscando una proyecci√≥n de baja dimensi√≥n de los datos que tenga la estructura topol√≥gica difusa equivalente m√°s cercana posible.\n",
        "\n",
        "Los detalles pueden verse en [pre-print de ArXiv](https://arxiv.org/abs/1802.03426):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KNSIeLzYqjH"
      },
      "outputs": [],
      "source": [
        "!pip install -qq umap-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOCB-2AsjEV9"
      },
      "source": [
        "Algunos par√°metros importantes de UMAP:\n",
        "\n",
        "* `n_neighbors`: N√∫mero de vecinos en los que se basa para aprender la estructura de los datos. Valores bajos de n_neighbors forzar√°n a UMAP a concentrarse en la estructura muy local, mientras que valores grandes empujar√°n a UMAP a mirar vecindarios m√°s grandes de cada punto cuando estime la estructura m√∫ltiple de los datos, perdiendo la estructura fina.\n",
        "* `min_dist` controla lo ajustado que UMAP puede representar los puntos. Controla la distancia m√≠nima entre los puntos en la representaci√≥n de baja dimensi√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSWRU8MEY28j"
      },
      "outputs": [],
      "source": [
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X,y = make_moons(n_samples=500,random_state=1945,noise=0.1)\n",
        "\n",
        "umap = UMAP(n_components=1,n_neighbors=50)\n",
        "X_red_1 = umap.fit_transform(X)\n",
        "\n",
        "umap = UMAP(n_components=1,n_neighbors=5)\n",
        "X_red_2 = umap.fit_transform(X)\n",
        "\n",
        "fig, axs = plt.subplots(1,3,figsize=(12,4))\n",
        "axs[0].title.set_text(\"Dataset original\")\n",
        "axs[0].scatter(X[:,0],X[:,1],c=y)\n",
        "axs[1].title.set_text(\"UMAP: high n_neighbors\")\n",
        "axs[1].scatter(X_red_1[:,0],[0 for x in X_red_1],c=y)\n",
        "axs[2].title.set_text(\"UMAP: low n_neighbors\")\n",
        "axs[2].scatter(X_red_2[:,0],[0 for x in X_red_2],c=y)\n",
        "for ax in axs.flatten():\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12kpxbPTj7pg"
      },
      "outputs": [],
      "source": [
        "umap = UMAP(n_components=1,\n",
        "            n_neighbors=50,\n",
        "            min_dist=0.9)\n",
        "X_red_1 = umap.fit_transform(X)\n",
        "\n",
        "umap = UMAP(n_components=1,\n",
        "            n_neighbors=50,\n",
        "            min_dist=0.1)\n",
        "X_red_2 = umap.fit_transform(X)\n",
        "\n",
        "fig, axs = plt.subplots(1,3,figsize=(12,4))\n",
        "axs[0].title.set_text(\"Dataset original\")\n",
        "axs[0].scatter(X[:,0],X[:,1],c=y)\n",
        "axs[1].title.set_text(\"UMAP: high min_dist\")\n",
        "axs[1].scatter(X_red_1[:,0],[0 for x in X_red_1],c=y)\n",
        "axs[2].title.set_text(\"UMAP: low min_dist\")\n",
        "axs[2].scatter(X_red_2[:,0],[0 for x in X_red_2],c=y)\n",
        "for ax in axs.flatten():\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "umap = UMAP(n_components=2)\n",
        "X_red = umap.fit_transform(X3d)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title(\"Dataset visualizado con UMAP\")\n",
        "plt.scatter(X_red[:,0],X_red[:,1],c=y3d)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qkacNcDMCKnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvI6SgIxFGBZ"
      },
      "source": [
        "# Pr√°ctica 1: MNIST\n",
        "\n",
        "En esta pr√°ctica aplicaremos reducci√≥n de dimensionalidad al conjunto de datos MNIST completo. Usaremos dos t√©cnicas:\n",
        "\n",
        "1. PCA\n",
        "2. TSNE\n",
        "\n",
        "En cada una de ellas visualizaremos los datos y observaremos algunos fen√≥menos. Adem√°s, veremos c√≥mo afecta la dimensionalidad y el tama√±o de un dataset a estos algoritmos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNRTvwo4ES_2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "id": "AObO3TMQtcSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las escalas de valores de los datos"
      ],
      "metadata": {
        "id": "A3ECdmdyuE0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(x_train.reshape(-1,28*28)).describe()"
      ],
      "metadata": {
        "id": "lbAPv0bztqhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pixels = x_train.reshape(-1,)\n",
        "\n",
        "max = np.max(all_pixels)\n",
        "min = np.min(all_pixels)\n",
        "\n",
        "print(f\"Valor m√°ximo: {max}\")\n",
        "print(f\"Valor m√≠nimo: {min}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(all_pixels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7o87Sosut3Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos preprocesamiento:\n",
        "* Dividimos todo entre 255\n",
        "* Hacemos el reshape adecuado, cada ejemplo es un vector de 784 componentes\n",
        "\n",
        "**Importante**: PCA es muy susceptible a escalas diferentes entre variables"
      ],
      "metadata": {
        "id": "IFXyAOf_xCRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = x_train.reshape(-1,28*28) / 255.\n",
        "X_test = x_test.reshape(-1,28*28) / 255."
      ],
      "metadata": {
        "id": "zv1Zk62hw9x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos algunos ejemplos"
      ],
      "metadata": {
        "id": "ilGXEoCdtc9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random_idxs = np.random.randint(0, x_train.shape[0], 6)\n",
        "\n",
        "fig, axs = plt.subplots(2, 3, figsize=(10, 6))\n",
        "for i, idx in enumerate(random_idxs):\n",
        "    axs[i // 3, i % 3].imshow(x_train[idx].reshape(28, 28), cmap='gray')\n",
        "    axs[i // 3, i % 3].set_title(f\"Label: {y_train[idx]}\")\n",
        "    axs[i // 3, i % 3].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tpxlcMHiri1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlY2HmCXKclu"
      },
      "source": [
        "## 1. PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JOuBSCodYFt"
      },
      "source": [
        "Ahora, apliquemos PCA a los datos. De esta manera, reducimos la dimensi√≥n de los puntos, de 784 a s√≥lo 2.\n",
        "\n",
        "**Importante**:\n",
        "\n",
        "* Observa que entrenamos a PCA con el conjunto de entrenamiento y el conjunto de prueba, s√≥lo lo transformamos con el modelo ya entrenado.\n",
        "* Estamos en aprendizaje no supervisado, aqu√≠, en principio no tenemos etiquetas, o m√°s precisamente, no las usamos para los entrenamientos.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1S9KVyZbkiciIEeC7cLi-epa62gfFM0pi )\n",
        "\n",
        "‚≠ï ¬øPor qu√© no entrenar con todo el conjunto de datos?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mGAro3BEtzu"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_red = pca.fit_transform(x_train)    # S√≥lo entrenamos con el de entrenamiento\n",
        "X_test_red = pca.transform(x_test)          # S√≥lo transformamos el de prueba\n",
        "\n",
        "print(X_train_red.shape)\n",
        "print(X_test_red.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqG0ILHeenod"
      },
      "source": [
        "Ya podemos ver c√≥mo se ve el conjunto de entrenamiento y prueba, separado por clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47MsUtRoE5L1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4),dpi=100)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Training data\")\n",
        "for k in range(10):\n",
        "  plt.scatter(X_train_red[y_train==k, 0], X_train_red[y_train==k, 1], label=k)\n",
        "plt.legend()\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test data\")\n",
        "for k in range(10):\n",
        "  plt.scatter(X_test_red[y_test==k, 0], X_test_red[y_test==k, 1], label=k)\n",
        "plt.legend()\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "X_train_red_3 = pca.fit_transform(x_train)    # S√≥lo entrenamos con el de entrenamiento\n",
        "X_test_red_3 = pca.transform(x_test)          # S√≥lo transformamos el de prueba\n",
        "\n",
        "print(X_train_red.shape)\n",
        "print(X_test_red.shape)"
      ],
      "metadata": {
        "id": "A7DdYy1ThS0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Graficar PCA 3D\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import plotly.io as pio\n",
        "\n",
        "# Crear DataFrame para Plotly\n",
        "df = pd.DataFrame({\n",
        "    'PC1': X_train_red_3[:, 0],\n",
        "    'PC2': X_train_red_3[:, 1],\n",
        "    'PC3': X_train_red_3[:, 2],\n",
        "    'Digit': y_train.astype(str)\n",
        "})\n",
        "\n",
        "# Crear el gr√°fico 3D\n",
        "fig = px.scatter_3d(\n",
        "    df,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    z='PC3',\n",
        "    color='Digit',\n",
        "    color_discrete_sequence=px.colors.qualitative.Light24,\n",
        "    title='MNIST en 3D (PCA)'\n",
        ")\n",
        "\n",
        "# Personalizar el gr√°fico\n",
        "fig.update_layout(\n",
        "    scene=dict(\n",
        "        xaxis=dict(showgrid=False, showbackground=False, showticklabels=True, title='PC1'),\n",
        "        yaxis=dict(showgrid=False, showbackground=False, showticklabels=True, title='PC2'),\n",
        "        zaxis=dict(showgrid=False, showbackground=False, showticklabels=True, title='PC3'),\n",
        "        bgcolor='white'\n",
        "    ),\n",
        "    legend_title_text='D√≠gito',\n",
        "    plot_bgcolor='white',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "# Guardar como HTML\n",
        "pio.write_html(fig, file='mnist_pca_3d.html', auto_open=True)"
      ],
      "metadata": {
        "id": "UvAM3cQvhip5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZcYr6faIn2r"
      },
      "source": [
        "### ‚ö° Varianza explicada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdk4AUOBfENU"
      },
      "source": [
        "Veamos la varianza agregada por cada una de las primeras 20 componentes principales, usando el atributo `explained_variance_ratio_` de la clase PCA.\n",
        "\n",
        "Este es un arreglo que contiene la varianza, como fracci√≥n, asociada a cada componente principal. En el caso de nuestro modelo, son dos componentes principales, por lo que es un arreglo de dos componentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRD1Qoz1EbWE"
      },
      "outputs": [],
      "source": [
        "pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv__ctNIEyJI"
      },
      "source": [
        "Entre estas dos componentes explican alrededor del 16% de la varianza de los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S39tnpVkEx8a"
      },
      "outputs": [],
      "source": [
        "np.sum(pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_2oS4AMFDN3"
      },
      "source": [
        "Veamos como es la varianza explicada considerando 20 componentes principales. Para esto, tenemos primero que entrenar un nuevo modelo de PCA, con 20 componentes principales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f50HdVyFd2k"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=20).fit(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXmDC6GyFfig"
      },
      "source": [
        "Graficamos la contribuci√≥n acumulada de la varianza en cada componente principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOlvv_5xE82l"
      },
      "outputs": [],
      "source": [
        "xs = list(range(1, pca.explained_variance_ratio_.shape[0]+1))\n",
        "\n",
        "plt.bar(xs, pca.explained_variance_ratio_)\n",
        "plt.xticks(xs)\n",
        "plt.plot(xs, np.cumsum(pca.explained_variance_ratio_), marker='o',color='red')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Componentes principales\")\n",
        "plt.ylabel(\"Varianza explicada\")\n",
        "print(f\"Varianza explicada por cada una de las componentes principales:\\n{pca.explained_variance_ratio_}\\n\")\n",
        "print(f\"Varianza acumulada explicada por cada una de las componentes principales:\\n{np.cumsum(pca.explained_variance_ratio_)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7szxyIYffxw"
      },
      "source": [
        "Varianza de todas las componentes principales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PaPM3jBGF_1"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=784).fit(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFVzh4LPIzDK"
      },
      "outputs": [],
      "source": [
        "xs = list(range(1, pca.explained_variance_ratio_.shape[0]+1))\n",
        "\n",
        "plt.figure(figsize=(16, 3))\n",
        "plt.bar(xs, pca.explained_variance_ratio_)\n",
        "plt.plot(xs, np.cumsum(pca.explained_variance_ratio_), '--',color='red')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Componentes principales\")\n",
        "plt.ylabel(\"Varianza explicada\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuX81EmYWncY"
      },
      "source": [
        "¬øCon cu√°ntas componentes principales conseguir√≠amos 95% de la varianza?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ce62q4JKuc1"
      },
      "outputs": [],
      "source": [
        "maxPC = np.where(np.cumsum(pca.explained_variance_ratio_) >= 0.95)[0][0]\n",
        "print(f\"El 95% de la varianza se obtiene con {maxPC} componentes principales\\n\")\n",
        "\n",
        "xs = list(range(1, pca.explained_variance_ratio_.shape[0]+1))\n",
        "\n",
        "plt.figure(figsize=(16, 3))\n",
        "plt.plot(xs, np.cumsum(pca.explained_variance_ratio_), color='red')\n",
        "plt.axhline(0.95,color='gray',linestyle='--')\n",
        "plt.axvline(maxPC,color='gray',linestyle='--')\n",
        "plt.xlabel(\"Componentes principales\")\n",
        "plt.ylabel(\"Varianza explicada\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrM6xyijIP71"
      },
      "source": [
        "üîµ Si queremos entrenar un clasificador, ¬øes mejor entrenar en los datos 2-dimensionales, en el conjunto completo o en un conjunto intermedio?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observaci√≥n importante**: En PCA, es lo mismo hacer reducci√≥n de dimensionalidad con un n√∫mero de componentes $m$, que hacer reducci√≥n de dimensianalidad con un n√∫mero de componentes $n\\geq m$ y luego tomar las primeras $m$.\n",
        "\n",
        "Por ejemplo, podemos hacer reducci√≥n de dimensionalidad con todas las componentes principales y luego tomar solamente las que necesitemos.\n",
        "\n",
        "**Conclusi√≥n**\n",
        "\n",
        "* PCA es r√°pido\n",
        "* Es lineal\n",
        "* Capta la estructura global del conjunto de datos\n",
        "* Podemos medir la varianza acumulada"
      ],
      "metadata": {
        "id": "zQrKxXknkjzY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFbWy5LhLGbr"
      },
      "source": [
        "## 2. t-SNE\n",
        "\n",
        "Ahora, exploremos [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) \"t-distributed Stochastic Neighbor Embedding\".\n",
        "\n",
        "Este m√©todo puede ser m√°s tardado con una cantidad grande de datos y dimensiones, por lo que podemos tomar una muestra de los datos originales. Esta muestra consta de los primeros 5000 datos, el entrenamiento tarda alrededor de 1 minuto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4yYdRzRKl3v"
      },
      "outputs": [],
      "source": [
        "train_sample_size = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFXiY93zLIYX"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "X_train_red = tsne.fit_transform(x_train[:train_sample_size])\n",
        "print(X_train_red.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kMOBfxJjzB"
      },
      "source": [
        "üîµ Observar que t-SNE no tiene un m√©todo `transform`, ¬øqu√© significa esto?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ2RZNS3XJEm"
      },
      "source": [
        "Transformemos el conjunto de datos dos veces, de manera independiente para observar la naturaleza probabil√≠stica del m√©todo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wog06au4OwcY"
      },
      "outputs": [],
      "source": [
        "test_sample_size = 2000\n",
        "\n",
        "X_test_red_1 = TSNE(n_components=2).fit_transform(x_test[:test_sample_size])\n",
        "print(X_test_red_1.shape)\n",
        "\n",
        "X_test_red_2 = TSNE(n_components=2).fit_transform(x_test[:test_sample_size])\n",
        "print(X_test_red_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PINBIHmL5_3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Datos de entrenamiento\")\n",
        "for k in range(10):\n",
        "    plt.scatter(X_train_red[y_train[:train_sample_size]==k, 0], X_train_red[y_train[:train_sample_size]==k, 1], label=k)\n",
        "plt.legend()\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Datos de prueba - Primer entrenamiento\")\n",
        "for k in range(10):\n",
        "    plt.scatter(X_test_red_1[y_test[:test_sample_size]==k, 0],\n",
        "                X_test_red_1[y_test[:test_sample_size]==k, 1],\n",
        "                label=k)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Test data - Segundo entrenamiento\")\n",
        "for k in range(10):\n",
        "    plt.scatter(X_test_red_2[y_test[:test_sample_size]==k, 0],\n",
        "                X_test_red_2[y_test[:test_sample_size]==k, 1],\n",
        "                label=k)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora hagamos reducci√≥n de dimensionalidad a 3 dimensiones con T-SNE\n",
        "\n",
        "‚è± Tarda alrededor de 3-5 minutos"
      ],
      "metadata": {
        "id": "BUsnjcuvkSsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=3)\n",
        "X_train_red_3 = tsne.fit_transform(x_train[:train_sample_size])\n",
        "print(X_train_red_3.shape)"
      ],
      "metadata": {
        "id": "5EFishRIi4KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Graficar PCA 3D\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import plotly.io as pio\n",
        "\n",
        "y_red  = y_train[:train_sample_size]\n",
        "\n",
        "# Crear DataFrame para Plotly\n",
        "df = pd.DataFrame({\n",
        "    'PC1': X_train_red_3[:, 0],\n",
        "    'PC2': X_train_red_3[:, 1],\n",
        "    'PC3': X_train_red_3[:, 2],\n",
        "    'Digit': y_red.astype(str)\n",
        "})\n",
        "\n",
        "# Crear el gr√°fico 3D\n",
        "fig = px.scatter_3d(\n",
        "    df,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    z='PC3',\n",
        "    color='Digit',\n",
        "    color_discrete_sequence=px.colors.qualitative.Light24,\n",
        "    title='MNIST en 3D (PCA)'\n",
        ")\n",
        "\n",
        "# Personalizar el gr√°fico\n",
        "fig.update_layout(\n",
        "    scene=dict(\n",
        "        xaxis=dict(showgrid=False, showbackground=False, showticklabels=True, title='PC1'),\n",
        "        yaxis=dict(showgrid=False, showbackground=False, showticklabels=True, title='PC2'),\n",
        "        zaxis=dict(showgrid=False, showbackground=False, showticklabels=True, title='PC3'),\n",
        "        bgcolor='white'\n",
        "    ),\n",
        "    legend_title_text='D√≠gito',\n",
        "    plot_bgcolor='white',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "# Guardar como HTML\n",
        "pio.write_html(fig, file='mnist_tsne_3d.html', auto_open=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OJlltOGIi3Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusi√≥n**\n",
        "\n",
        "* T-SNE es lento\n",
        "* T-SNE no es lineal\n",
        "* Su principal par√°metros en `perplexity`\n",
        "* No siempre va a dar el mismo resultado\n",
        "* Capta la estructura local y global, es decir, trata de preservar las distribuciones de distancias entre puntos.\n"
      ],
      "metadata": {
        "id": "1Ibwm8cvlUbx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w33lJLwVifh"
      },
      "source": [
        "## ‚≠ï Pr√°ctica\n",
        "\n",
        "Entrenaremos un clasificador para el dataset MNIST de Keras. Es un dataset grande y balanceado. Te enfrentar√°s, por primera vez, al problema de la dimensionalidad y el tama√±o.\n",
        "\n",
        "* Entrena un clasificador con estas representaciones 2-dimensionales de los datos de entrenamiento. Usa PCA.\n",
        "* Reporta las m√©tricas de clasificaci√≥n sobre el conjunto de prueba: Accuracy, y F1-score.\n",
        "* Aumenta el n√∫mero de componentes de la reducci√≥n de dimensionalidad con el objetivo de tener mejores m√©tricas de rendimiento.\n",
        "* Finalmente, realiza la tarea de clasificaci√≥n usando todas las dimensiones.\n",
        "\n",
        "¬øCon qu√© features tuviste mejores desempe√±os? ¬øqu√© reducci√≥n de dimensionalidad funcion√≥ mejor?\n",
        "\n",
        "**Opcional:** Realiza un grid-search para obtener el mejor n√∫mero de dimensiones, de forma que se maximice el accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYkJnF8vYuv1"
      },
      "source": [
        "#### Pasos de la pr√°ctica\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: Observa que, la versi√≥n completa, ya est√° dividida en train/test"
      ],
      "metadata": {
        "id": "PvTTkyWXrFFs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12FTZyqmOrMm"
      },
      "outputs": [],
      "source": [
        "#---- Versi√≥n reducida del dataset ----\n",
        "\n",
        "# from sklearn.datasets import load_digits\n",
        "\n",
        "# digits = load_digits()\n",
        "# X = digits.data\n",
        "# y = digits.target\n",
        "# print(X.shape)\n",
        "# print(y.shape)\n",
        "\n",
        "#----- Versi√≥n completa del dataset ----\n",
        "\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü¢ Preprocesamiento: Reescalamos todo el dataset dividiendo entre 255"
      ],
      "metadata": {
        "id": "jl9zB6WfrYZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(-1,28*28)/255.\n",
        "X_test = X_test.reshape(-1,28*28)/255."
      ],
      "metadata": {
        "id": "7vU7opHzrejq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Realiza PCA para reducir el dataset a 2 dimensiones, es decir a 2 features. Imprime el shape de los conjuntos resultantes para asegurarte del proceso\n",
        "\n",
        "Recuerda:\n",
        "* Entrena y transforma el conjunto de entrenamiento\n",
        "* Transforma solamente el conjunto de prueba"
      ],
      "metadata": {
        "id": "VVCfjkwevE9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n"
      ],
      "metadata": {
        "id": "-DipyZAxvX4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Entrena un clasificador, el que prefieras, con esta versi√≥n de 2 features. Reporta el accuracy y f1-score en el conjunto de entrenamiento y prueba.\n",
        "\n",
        "Recuerda:\n",
        "* SVM es buena opci√≥n pero lento\n",
        "* Decision Tree es r√°pido pero propenso a overfitting\n",
        "* K-NN es r√°pido pero le afecta la alta dimensionalidad\n"
      ],
      "metadata": {
        "id": "nJh_XbGepzj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx_cdkz_Oiva"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "print(f\"Train accuracy: {accuracy_score(y_train,y_pred_train)}\")\n",
        "print(f\"Train f1-score: {f1_score(y_train,y_pred_train,average='micro')}\")  # Observa el hiperpar√°metro 'micro'\n",
        "\n",
        "print(f\"Test accuracy: {accuracy_score(y_test,y_pred)}\")\n",
        "print(f\"Test f1-score: {f1_score(y_test,y_pred,average='micro')}\") # Observa el hiperpar√°metro 'micro'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Ahora:\n",
        "\n",
        "1. Realiza PCA con un n√∫mero mayor de componentes principales $10\\leq d\\leq 100$\n",
        "2. Entrena un modelo usando el mismo algoritmo del paso anterior y vuelve a reportar las m√©tricas, tanto en entrenamiento y prueba."
      ],
      "metadata": {
        "id": "5PcNG7kCpuUE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8YFsCUjTZDf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Ahora, intenta entrenar un modelo con todas las features, es decir, con el dataset original."
      ],
      "metadata": {
        "id": "qGYcSIC10CeR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TFmAEEch0WCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXDoKf4Ed5sf"
      },
      "source": [
        "#### Opcional: Busqueda de hiper-par√°metros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQHnrFICd8zn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hbbXlZ1JDot"
      },
      "source": [
        "‚ö° Dado que no es necesario correr PCA en cada iteraci√≥n y s√≥lo basta con calcular todas las 64 PC, podemos hacer el grid search de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYd5PpU0JM9G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyPcrSUsQry"
      },
      "source": [
        "# Pr√°ctica 2: Documentos de Wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RXeMiZm8knu"
      },
      "source": [
        "Descubrir temas es √∫til para diversos fines, como agrupar documentos, organizar contenido disponible en l√≠nea para recuperar informaci√≥n y hacer recomendaciones.\n",
        "\n",
        "**El modelado de temas** (topic modelling) es una t√©cnica de miner√≠a de texto que proporciona m√©todos para descubrir temas ocultos en el documento, anotar los documentos con estos temas y organizar una gran cantidad de datos no estructurados. Numerosos proveedores de contenido y agencias de noticias est√°n utilizando modelos de temas para recomendar art√≠culos a los lectores.\n",
        "\n",
        "**<h4>Objetivo de la pr√°ctica</h4>**\n",
        "\n",
        "Usar la t√©cnica de reducci√≥n de dimensionalidad PCA con el prop√≥sito de modelar documentos y medir semejanzas entre ellos.\n",
        "\n",
        "Usaremos el modelo de bolsa de palabras (BoW -- Bag of words), que da como resultado una matriz documento-t√©rmino que representa documentos en funci√≥n del conteo de t√©rminos.\n",
        "\n",
        "Tomaremos algunos documentos y recuperaremos los documentos m√°s similares. Evaluaremos esta tarea usando la observaci√≥n directa de los documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnTUAvKz8kn0"
      },
      "source": [
        "## Conjuntos de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7NIj9HL8kpG"
      },
      "source": [
        "Este conjunto de datos completo puede encontrarse [aqu√≠](https://www.cs.upc.edu/~nlp/wikicorpus/). Estos archivos son tipo texto con el contenido raw del texto.\n",
        "\n",
        "*Para prop√≥sitos de esta sesi√≥n, solo usamos un archivo arbitrario de esta colecci√≥n y lo presentamos ya preparado en un dataframe.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTPBt5J_8kod"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAqbXLyaiFfn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/03%20Machine%20Learning/data/spanish-wikipedia-dataframe.csv\"\n",
        "\n",
        "df = pd.read_csv(url, index_col=0)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDeegn_TmTR6"
      },
      "source": [
        "Con este dataset, usaremos la reducci√≥n de dimensionalidad para encontrar similitudes entre documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqtmSm4Y8krL"
      },
      "source": [
        "## **Extracci√≥n de caracter√≠sticas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUEWfzzv8krN"
      },
      "source": [
        "Tratemos de visualizar algunas propiedades de los documentos.\n",
        "\n",
        "Para ello vamos a utilizar un contador ([Counter](https://docs.python.org/2/library/collections.html#collections.Counter)). Un contador es un contenedor que almacena elementos como claves de diccionario, y sus recuentos se almacenan como valores de diccionario.\n",
        "\n",
        "Construiremos una columna con el conteo de palabras por documento y otra con la palabra m√°s frecuente en el documento.\n",
        "\n",
        "**Observaci√≥n**: Podr√≠amos saltarnos esto y hacerlo directamente con CountVectorizer, pero lo haremos para tener una forma de validaci√≥n de los datos. Adem√°s, de mostrar una forma muy b√°sica de tokenizar y realizar algunas tareas b√°sicas de procesamiento de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCyLxlvL8krS"
      },
      "outputs": [],
      "source": [
        "df['Palabras'] = df['Texto'].apply(lambda x: x.split())\n",
        "df['Total'] = df['Palabras'].apply(lambda x: len(x))\n",
        "df.drop(columns=['Palabras'],inplace=True)\n",
        "\n",
        "df = df.sort_values(by=\"Total\",ascending=False)\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5hdGpH28ksK"
      },
      "source": [
        "### Reducci√≥n del tama√±o de las matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfF54MCk8ksR"
      },
      "source": [
        "Para reducir la complejidad espacial de nuestro ejercicio, podemos hacer dos cosas:\n",
        "1. Un muestreo aleatorio de documentos, lo que nos ayudar√≠a a reducir el vocabulario.\n",
        "2. Un recorte en el n√∫mero de documentos por la cantidad de palabras.\n",
        "\n",
        "Usaremos el segundo enfoque."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQNWh_tQ8ksZ"
      },
      "source": [
        "Obtenemos el vocabulario.\n",
        "Para ello vamos a usar el m√©todo de tokenizaci√≥n de NLTK [`word_tokenize`](https://www.nltk.org/api/nltk.tokenize.word_tokenize.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tUgGrr_l7FV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Sqh6jIa8ksd"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "textos = df['Texto'].values\n",
        "textos = \" \".join(textos)\n",
        "vocabulario = list(set(word_tokenize(textos))) # Manera de quitar las repeticiones de una lista\n",
        "print(len(vocabulario),'palabras √∫nicas (tokens)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShkMRM1kx8C4"
      },
      "source": [
        "Veamos la distribuci√≥n de n√∫mero de palabras por documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNveLiogvtb7"
      },
      "outputs": [],
      "source": [
        "from seaborn import histplot\n",
        "import numpy as np\n",
        "\n",
        "longitudes = df['Total'].values\n",
        "promedio = np.mean(longitudes)\n",
        "print(f\"Promedio: {promedio}\")\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.subplot()\n",
        "ax.axvline(promedio,color='black')\n",
        "histplot(longitudes,ax=ax)\n",
        "ax.set_xlabel(\"Longitud del texto\")\n",
        "ax.set_ylabel(\"Conteos\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsXHiOjxwQbC"
      },
      "source": [
        "Ejemplo de transformar los datos con el logaritmo. Esto puede hacerse cuando se tienen datos en diferentes ordenes de longitud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT58u3y1wFvx"
      },
      "outputs": [],
      "source": [
        "log_longs = np.log(longitudes)\n",
        "log_promedio = np.log(promedio)\n",
        "print(f\"Logaritmo del promedio: {np.log(promedio)}\")\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.subplot()\n",
        "histplot(log_longs,ax=ax)\n",
        "ax.axvline(log_promedio,color='black')\n",
        "ax.set_xlabel(\"Logaritmo de la longitud del texto\")\n",
        "ax.set_ylabel(\"Conteos\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP-I4EQp8ks3"
      },
      "source": [
        "Filtramos algunos documentos de acuerdo a la longitud del texto. Nos quedamos con textos que tengan entre 50 y 200 palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74GbUKur8ks6"
      },
      "outputs": [],
      "source": [
        "df = df[(df['Total'] < 200) & (df['Total'] > 50)]\n",
        "print(f\"N√∫mero de documentos con los que nos quedamos: {df.shape[0]}\")\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGsmspwIyMBr"
      },
      "source": [
        "Veamos, otra vez, la distribuci√≥n de n√∫mero de palabras por documentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiXz1sb48ktJ"
      },
      "outputs": [],
      "source": [
        "from seaborn import histplot\n",
        "\n",
        "longitudes = df['Total'].values\n",
        "promedio = np.mean(longitudes)\n",
        "print(f\"Promedio: {promedio}\")\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.subplot()\n",
        "ax.plot([promedio,promedio],[0,200],color='black')\n",
        "histplot(longitudes,ax=ax)\n",
        "ax.set_xlabel(\"Longitud del texto\")\n",
        "ax.set_ylabel(\"Conteos\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWMeMs_w8kta"
      },
      "source": [
        "‚ö° Guardamos el dataframe en formato pickle. Pickle es un formato binario en el cual podemos facilmente recuperar los tipos de cada columna. Se guarda y se recupera el objeto *tal como es*.\n",
        "\n",
        "‚ùó Es importante tener precauci√≥n al usarlo por la compatibilidad entre versiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwCnWC2v8ktb"
      },
      "outputs": [],
      "source": [
        "df.to_pickle('data_frame.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6wUESfg8kts"
      },
      "source": [
        "Leemos el dataframe previamente almacenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2RDSpKV8kt2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle('data_frame.pickle')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yazp3KcJ8kuA"
      },
      "source": [
        "### Modelo *Bag of Words* (BoW)\n",
        "\n",
        "Usaremos el modelo BOW de conteos de ocurrencias de palabras que hemos usado para generar features de texto, al igual que antes, usaremos el `CountVectorizer` de scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcMzhrj1HDA0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "docs_list = df['Texto'].values\n",
        "\n",
        "cv = CountVectorizer(max_features=None)\n",
        "X_bow = cv.fit_transform(docs_list)\n",
        "X_bow.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpWtuMNg8ku-"
      },
      "source": [
        "## PCA\n",
        "\n",
        "Ahora, realizaremos PCA a la matriz de conteos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ3f6HjG8kvC"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(svd_solver='auto')\n",
        "\n",
        "X_pca = pca.fit_transform(np.asarray(X_bow.todense()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsjpGouxaU4k"
      },
      "source": [
        "Observa la forma de la matriz obtenida, ¬øcu√°ntas componentes principales se obtienen? ¬øcu√°ntas *deber√≠amos* de obtener?\n",
        "\n",
        "Esta diferencia tiene que ver con el `svd_solver` usado dado el tama√±o de la matriz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlJWj_RY8kvK"
      },
      "outputs": [],
      "source": [
        "print(X_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7W5w0CEy05C"
      },
      "source": [
        "Construimos un dataframe con la matriz de componentes principales y el *id* de cada documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC45LdF18kv2"
      },
      "outputs": [],
      "source": [
        "Xpca_df = pd.DataFrame(X_pca)\n",
        "Xpca_df['doc_id'] = df['doc_id'].copy()\n",
        "\n",
        "# Reordenamos las columnas, traemos la columna 'id' al principio\n",
        "cols = Xpca_df.columns.tolist()\n",
        "cols = cols[-1:] + cols[:-1]\n",
        "Xpca_df = Xpca_df[cols]\n",
        "\n",
        "print(Xpca_df.shape)\n",
        "Xpca_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwESfdavzPr5"
      },
      "source": [
        "Veamos la proporci√≥n de varianza por cada componente principal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zImm80Mz8kwB"
      },
      "outputs": [],
      "source": [
        "pca_variance_ratio = pca.explained_variance_ratio_\n",
        "print(pca_variance_ratio[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smOgIEko8kwM"
      },
      "outputs": [],
      "source": [
        "D = pca_variance_ratio\n",
        "\n",
        "# Varianza acumulada\n",
        "d = np.cumsum(D)\n",
        "\n",
        "# Graficamos\n",
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "\n",
        "pasos = range(len(D))\n",
        "ax.plot(range(D.shape[0]),d,\n",
        "        ls='--',\n",
        "        color='red',\n",
        "        linewidth=2,\n",
        "        )\n",
        "\n",
        "plt.title('Varianza acumulada por dimensi√≥n')\n",
        "plt.xlabel('Componente',fontsize=14)\n",
        "plt.ylabel('Proporci√≥n',fontsize=14)\n",
        "plt.grid(True)\n",
        "ax.grid(which='major', color='gray', linestyle='-')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfjyAQHp8kwX"
      },
      "source": [
        "### An√°lisis usando PCA\n",
        "\n",
        "En esta secci√≥n, queremos saber que tan bien podemos modelar documentos utilizando estas t√©cnicas de reducci√≥n de dimensionalidad. En la sesi√≥n pasada ya vimos algunas limitantes de modelar texto usando solamente las matrices de conteos.\n",
        "\n",
        "Para nuestro fin, eligiremos un n√∫mero $q$ de componentes principales (que llamaremos _representativas_) para PCA y compara la calidad de los documentos m√°s cercanos (semejantes) a los documentos de an√°lisis mostrados aqu√≠ abajo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvrnX1aM8kwZ"
      },
      "source": [
        "## Modelaci√≥n de documentos utilizando componentes principales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo2zGHCVcfP5"
      },
      "source": [
        "Funciones auxiliares para el manejo del dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN4DHIwu8kwb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title funciones auxiliares\n",
        "\n",
        "def get_documents(df,query_ids:list):\n",
        "    '''\n",
        "    Esta funci√≥n recupera el texto de un documento a partir de la lista de doc_id en el\n",
        "    dataframe del corpus.\n",
        "    Regresa los documentos como una lista\n",
        "    '''\n",
        "    docs = [list(df[df['doc_id']==id]['Texto'].values)[0] for id in query_ids]\n",
        "    return docs\n",
        "\n",
        "def get_index(df,query_ids:list):\n",
        "    '''\n",
        "    Esta funci√≥n recupera el √≠ndice, en el dataframe del corpus, de un documento a partir\n",
        "    de la lista de doc_id.\n",
        "    Regresa los √≠ndices, como una lista\n",
        "    '''\n",
        "    idxs = [df[df['doc_id']==id].index.to_list()[0] for id in query_ids]\n",
        "    return idxs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdpmIh8r8kwq"
      },
      "outputs": [],
      "source": [
        "dim = 20  # elegimos un numero q de componentes principales\n",
        "df_pca = Xpca_df.iloc[:,:dim+1].copy()\n",
        "print(df_pca.shape)\n",
        "df_pca.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wux5gTuaBUPz"
      },
      "source": [
        "Grafiquemos las primeras dos componentes principales del *corpus*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U35uqAEY8kw8"
      },
      "outputs": [],
      "source": [
        "X = df_pca.iloc[:,1:].values  # La matriz con todos los documentos y todas las componentes principales\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1],alpha=0.5,c='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99StnZL88kxa"
      },
      "source": [
        "## Tarea 1. Similitud entre documentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhwDRo9i1l9E"
      },
      "source": [
        "Escojamos los 5 primeros documentos para analizar y encontrar sus vecinos m√°s cercanos.\n",
        "\n",
        "Los agrupamos en un dataframe con el *id* y el *texto*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAIr5Mw8QM0h"
      },
      "source": [
        "Definimos los par√°metros del experimento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG5xgWmjP8e2"
      },
      "outputs": [],
      "source": [
        "n_test_docs = 6\n",
        "# Tomamos los primeros n_test_docs como documentos para experimentar\n",
        "test_idxs = df.index.to_list()[:n_test_docs]\n",
        "\n",
        "n_vecinos_cercanos = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYqq6Vey8kxb"
      },
      "outputs": [],
      "source": [
        "test_docs = df.loc[test_idxs,['doc_id','Texto']]\n",
        "test_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZLAd_wi8kxk"
      },
      "source": [
        "Funciones que calculan el vecino m√°s cercano a cada uno de los documentos de an√°lisis, usando la distancia Euclidiana en las representaciones obtenidas por PCA.\n",
        "\n",
        "Esto lo hacemos usando [NearestNeighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors), el cual es un algoritmo para buscar eficientemente vecinos m√°s cercanos en diversas m√©tricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCejV4Mcnde9"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def k_vecinos_mas_cercanos(docs_df,ids,k=1):\n",
        "    '''\n",
        "    Esta funci√≥n recibe un dataframe con las columnas 'doc_id' y 'vector' de todo el corpus,\n",
        "    'ids' es la lista de doc_id de los documentos de los cuales se obtendran los vecinos m√°s\n",
        "    cercanos.\n",
        "    'k' es el n√∫mero de vecinos cercanos que se buscar√°n\n",
        "    La funci√≥n regresa una lista de tuplas, cada tupla\n",
        "    '''\n",
        "    X = docs_df.iloc[:,1:].values\n",
        "    idxs = docs_df[docs_df['doc_id'].isin(ids)].index.to_list()\n",
        "    knn = NearestNeighbors(n_neighbors=k+1,\n",
        "                           metric='cosine'  # Para tareas de texto la m√©trica coseno suele ser mejor\n",
        "                           )\n",
        "    knn.fit(X)\n",
        "    nns = knn.kneighbors(X[idxs], return_distance=False)\n",
        "    nns_idxs = [pair[1:] for pair in nns]\n",
        "    nns_ids = [(query_id,list(docs_df.loc[idx,'doc_id'].values)) for query_id,idx in\n",
        "               zip(ids,nns_idxs)]\n",
        "    return nns_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WusjgEUD-L_"
      },
      "outputs": [],
      "source": [
        "df_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-bEQ4cNI46_"
      },
      "outputs": [],
      "source": [
        "ids = list(test_docs['doc_id'].values)\n",
        "\n",
        "nns = k_vecinos_mas_cercanos(df_pca,ids,n_vecinos_cercanos)\n",
        "nns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo7hmOMPNUz1"
      },
      "outputs": [],
      "source": [
        "for nn in nns:\n",
        "    test_doc = get_documents(df,[nn[0]])[0]\n",
        "    print(f\"Texto de prueba:\\n\\t{test_doc}\")\n",
        "    nn_docs = get_documents(df,nn[1])\n",
        "    print(f\"{n_vecinos_cercanos} vecinos m√°s cercanos:\")\n",
        "    for nn in nn_docs:\n",
        "        print(f\"\\t{nn}\")\n",
        "    print(20*'-')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiDBrAzBw5Fg"
      },
      "source": [
        "Veamos las parejas de documentos m√°s cercanos, usando las primeras dos PC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dFerxhPfVTcL"
      },
      "outputs": [],
      "source": [
        "#@title Definici√≥n de los colores para mostrar los documentos con colores diferentes\n",
        "\n",
        "n_colors = n_test_docs\n",
        "\n",
        "cm = plt.get_cmap('gist_rainbow')\n",
        "colors = [cm(1.*i/n_colors) for i in range(n_colors)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVrcIuj_S2Fd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = df_pca.iloc[:,1:].values\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(X[:,0],X[:,1], color='gray', alpha=0.25)\n",
        "for j,(color,nn) in enumerate(zip(colors,nns)):\n",
        "    test_idx = get_index(df,[nn[0]])\n",
        "    plt.scatter(X[test_idx,0],X[test_idx,1], color=color,s=60,label=f\"Documento {j+1}\")  # Graficar las primeras dos PC de los documentos de prueba\n",
        "    ns_idxs = get_index(df,nn[1])\n",
        "    plt.scatter(X[ns_idxs,0],X[ns_idxs,1], color=color,s=70,marker='x',label=f'Vecinos del documento {j+1}') # Graficar las primeras dos PC de los vecinos de los documentos de prueba\n",
        "plt.legend()\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J9BAALj5Z6U"
      },
      "source": [
        "‚≠ï Preguntas\n",
        "\n",
        "* ¬øPor qu√© los vecinos no son los m√°s cercanos en la figura?\n",
        "* ¬øQu√© par√°metros podr√≠amos variar para modificar o mejorar estos resultados?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMmporp68kyp"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ Observa las gr√°ficas HTML de reducci√≥n de dimensionalidad de documentos. ¬øObservas similitudes entre documentos *cercanos*?\n",
        "\n",
        "* https://github.com/DCDPUAEM/DCDP/blob/main/03%20Machine%20Learning/figuras/wiki-bow-tsne3d-docs.html\n",
        "* https://github.com/DCDPUAEM/DCDP/blob/main/03%20Machine%20Learning/figuras/wiki-tfidf-umap3d-docs.html"
      ],
      "metadata": {
        "id": "wjye6CU12Lld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjjzBJ-szVOL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>T√©cnicas b√°sicas de Procesamiento de Lenguaje Natural</h1>\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03%20Machine%20Learning/notebooks/10-NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "P-VXkjHfMrcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta notebook aprenderemos algunas t√©cnicas de NLP para lidiar con las tareas de aprendizaje supervisado en el contexto del texto escrito.\n",
        "\n",
        "En el NLP hay dos librerias cl√°sicas principales:\n",
        "\n",
        "* [NLTK](https://www.nltk.org/)\n",
        "* [Spacy](https://spacy.io/)\n",
        "\n",
        "Revisaremos:\n",
        "* Preprocesamiento y limpieza de texto\n",
        "* Tecnicas de representaci√≥n de texto\n",
        "* Algoritmos especializados: Naive Bayes Multinomial, la m√©trica angular\n",
        "\n",
        "Adem√°s, lidiaremos con un problema t√≠pico del aprendizaje supervisado: el desbalanceo de clases.\n",
        "\n",
        "El problema que abordaremos ser√° construir un modelo que identifique mensajes SPAM. Es un problema de clasificaci√≥n binaria. En este contexto, queremos minimizar los falsos positivos, es decir, queremos evitar que marque mensajes no spam como spam. La m√©trica que monitorea los falsos positivos especificamente es el precision, recordar que\n",
        "\n",
        "$$\\text{Precision} = \\frac{TP}{TP+FP}$$\n",
        "\n",
        "Usaremos Accuracy y Precision para este problema, principalmente. Sin embargo, al tratarse de un problema desbalanceado hacia la clase negativa, es importante ver el recall o el F1-score. Muchas de las predicciones exitosas pueden deberse a que se esta apoyando en la clase mayoritaria."
      ],
      "metadata": {
        "id": "gVT-w3PeqdDt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TD8mqvrGX-Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/03%20Machine%20Learning/data/Spam_SMS.csv \"\n",
        "\n",
        "df = pd.read_csv(url, encoding='utf-8')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An√°lisis Exploratorio"
      ],
      "metadata": {
        "id": "y0Dx3lYqVvqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el NLP es importante hacer an√°lisis exploratorio de los datos y determinar par√°metros y caracteristicas importantes, por ejemplo:\n",
        "\n",
        "* Idioma del corpus\n",
        "* Distribuci√≥n de la longitud de los documentos\n",
        "* Perfilado de texto: An√°lisis t√©cnico que examina estructura, metadatos, uso de caracteres/emojis y anomal√≠as\n",
        "* Fuente del corpus\n",
        "* Calidad del corpus"
      ],
      "metadata": {
        "id": "UUrvkHfsV2HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "longitudes = [len(x.split()) for x in df['Message'].values]\n",
        "longitud_promedio = np.mean(longitudes)\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(longitudes)\n",
        "plt.xlabel('Longitud')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.axvline(x=longitud_promedio, color='r', linestyle='--', label=f'Longitud promedio: {longitud_promedio:.1f}')\n",
        "plt.legend()\n",
        "plt.title('Distribuci√≥n de longitudes de mensajes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DBxF7Xn2tjO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos apoyamos de herramientas como *wordclouds* para ver palabras comunes y patrones de vocabulario"
      ],
      "metadata": {
        "id": "U_ew8HNWaYCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq wordcloud"
      ],
      "metadata": {
        "id": "qPAETXw0JZZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "texto = \" \".join(df['Message'])\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(texto)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SKl4fxXmJpbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ ¬øQu√© conclusiones podemos sacar de este gr√°fico?"
      ],
      "metadata": {
        "id": "7kOSNZdGbxTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las **stopwords** son palabras frecuentes pero con bajo valor sem√°ntico (\"el\", \"y\", \"de\", \"en\", etc.).\n",
        "\n",
        "* Es recomendable eliminarlas en tareas de an√°lisis de texto (como clasificaci√≥n, miner√≠a de datos o SEO) para reducir ruido y enfocarse en t√©rminos clave.\n",
        "* No es recomendable elimnarlas cuando se necesita preservar estructura gramatical (en chatbots, traducci√≥n autom√°tica o an√°lisis sint√°ctico), ya que su eliminaci√≥n puede distorsionar el significado.\n",
        "\n",
        "Mientras en modelos de machine learning cl√°sico suelen omitirse para eficiencia, en generaci√≥n de lenguaje natural o modelos m√°s modernos y costosos son esenciales.\n",
        "\n",
        "Adem√°s, su relevancia y listado var√≠a por idioma y dominio (por ejemplo, en textos legales, *art√≠culo* podr√≠a ser stopword)."
      ],
      "metadata": {
        "id": "-8uCHIpALNhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')  # Tambi√©n podemos escoger las stopwords en espa√±ol"
      ],
      "metadata": {
        "id": "7m_Wdm8vLONU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos algunas stopwords:"
      ],
      "metadata": {
        "id": "85mRLkWGWJEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords[:10])"
      ],
      "metadata": {
        "id": "06MlBpy0TcXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una funci√≥n para limpieza de texto.\n",
        "\n",
        "\n",
        "La **tokenizaci√≥n** es el proceso de dividir un texto en unidades m√≠nimas (tokens), como palabras, s√≠mbolos o frases, para su an√°lisis en NLP.\n",
        "\n",
        "    Texto: \"¬°Hola, mundo!\" ‚Üí Tokens: [\"¬°\", Hola\", \",\", \"mundo\", \"!\"]."
      ],
      "metadata": {
        "id": "ReW14KGrav_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = word_tokenize(text)\n",
        "    text = [word for word in text if word not in stopwords]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "df['Clean Message'] = df['Message'].apply(clean_text)\n",
        "df"
      ],
      "metadata": {
        "id": "2S6Mwhd1LSgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora hagamos una nube de palabras con el texto limpio"
      ],
      "metadata": {
        "id": "8JWQJxXeb43e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "texto_limpio = \" \".join(df['Clean Message'])\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(texto_limpio)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pJzv8hLDLwsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hagamos una nube de palabras por cada etiqueta, con los textos limpios"
      ],
      "metadata": {
        "id": "rAeA7dolMOlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textos_limpios_ham = \" \".join(df[df['Class'] == 'ham']['Clean Message'])\n",
        "textos_limpios_spam = \" \".join(df[df['Class'] == 'spam']['Clean Message'])\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.subplot(1, 2, 1)\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(textos_limpios_ham)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title('Ham')\n",
        "plt.subplot(1, 2, 2)\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(textos_limpios_spam)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title('Spam')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vlrjESusMPuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento"
      ],
      "metadata": {
        "id": "EySQLxTjhWp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay t√©cnicas adicionales muy usadas para procesar el texto:\n",
        "\n",
        "- **Expresiones regulares (RegEx)**  \n",
        "  *Definici√≥n:* Patrones de texto utilizados para buscar, coincidir y manipular cadenas de caracteres.  \n",
        "  *Uso:* Validaci√≥n de formatos (ej. emails), extracci√≥n de informaci√≥n, sustituci√≥n de texto y filtrado de datos.  \n",
        "\n",
        "|          |                                                                  |\n",
        "|------------------------|----------------------------------------------------------------------------------------|\n",
        "| **Texto inicial**      | `Contacta a soporte@empresa.com o a ventas@tienda.com. Para errores, escribe a bugs@dev.org. No env√≠es spam a info@dominio.invalido.` |\n",
        "| **Expresi√≥n regular**  | `\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b`                                  |\n",
        "| **Coincidencias**      | 1. `soporte@empresa.com`<br>2. `ventas@tienda.com`<br>3. `bugs@dev.org`\n",
        "\n",
        "- **Lematizar**  \n",
        "  *Definici√≥n:* Proceso ling√º√≠stico que reduce una palabra a su forma base o can√≥nica (lema). Ej: \"corriendo\" ‚Üí \"correr\".  \n",
        "  *Uso:* Normalizaci√≥n de texto en PLN (Procesamiento de Lenguaje Natural) para mejorar an√°lisis de frecuencia o b√∫squedas sem√°nticas.  \n",
        "\n",
        "- **POS Tagging (Etiquetado gramatical)**  \n",
        "  *Definici√≥n:* Asignaci√≥n de categor√≠as gramaticales (sustantivo, verbo, adjetivo, etc.) a cada palabra en un texto.  \n",
        "  *Uso:* An√°lisis sint√°ctico, traducci√≥n autom√°tica, generaci√≥n de texto y sistemas de chatbots para entender la estructura de frases.  \n",
        "\n",
        "\n",
        "No veremos estos puntos."
      ],
      "metadata": {
        "id": "ncEUAHP1M5wI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separaci√≥n de variables"
      ],
      "metadata": {
        "id": "EU_JaUU9WTCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['Class'].values\n",
        "# texts = df['Message'].values\n",
        "texts = df['Clean Message'].values"
      ],
      "metadata": {
        "id": "qKbqwY8f3kWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ratio = y[y == 'spam'].shape[0] / y.shape[0]\n",
        "plt.suptitle(f'Distribuci√≥n de clases\\n{ratio:.2%} spam')\n",
        "sns.countplot(y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kBPg3ZxY4R45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, es un dataset moderadamente desbalanceado. No hay umbrales precisos. Una gu√≠a emp√≠rica es:\n",
        "\n",
        "* Balanceado: La clase minoritaria representa > 30% del total.\n",
        "* Desbalanceado moderado: Clase minoritaria entre 10% y 30%.\n",
        "* Extremadamente desbalanceado: Clase minoritaria < 10%.\n",
        "* Si la clase minoritaria tiene < 5% de los datos, se considera un problema severo (requiere t√©cnicas especiales como oversampling/SMOTE o cost-sensitive learning)."
      ],
      "metadata": {
        "id": "Ob2whOKnI1eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codificaci√≥n de clases"
      ],
      "metadata": {
        "id": "M7aWwuqtWVxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, test_texts, y_train, y_test = train_test_split(texts,\n",
        "                                                            y,\n",
        "                                                            test_size=0.2,\n",
        "                                                            stratify=y,\n",
        "                                                            random_state=1942)"
      ],
      "metadata": {
        "id": "vH-c0gxz4GAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(y_train[:5])\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "print(y_train[:5])\n"
      ],
      "metadata": {
        "id": "qTFZPo5-3079"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracci√≥n de features\n",
        "\n",
        "Hay dos m√©todos cl√°sicos para vectorizar texto. Ambos funcionan bajo el mismo principio: convertir documentos de texto en vectores num√©ricos basados en la frecuencia de las palabras, pero difieren en c√≥mo ponderan estos t√©rminos.\n",
        "\n",
        "* [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).  \n",
        "\n",
        " * Representa cada documento como un vector donde cada componente cuenta la frecuencia absoluta de una palabra en el texto.\n",
        " * No considera la importancia relativa de las palabras en el corpus, solo su ocurrencia.\n",
        "* [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
        "\n",
        " * Ajusta la frecuencia de las palabras ponderando su importancia en el documento y en el corpus. La importancia est√° dada por el producto de dos t√©rminos:\n",
        "\n",
        "   1. TF (Term Frequency): Frecuencia del t√©rmino en el documento.\n",
        "   2. IDF (Inverse Document Frequency): Penaliza t√©rminos comunes (como \"el\", \"y\") y da m√°s peso a palabras relevantes.\n",
        "\n",
        "`CountVectorizer` solo cuenta palabras, mientras que `TfidfVectorizer` ajusta los pesos para reflejar qu√© tan √∫nico o relevante es un t√©rmino en el corpus."
      ],
      "metadata": {
        "id": "2xy3iV3fcQk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîΩ Ejemplo Ilustrativo"
      ],
      "metadata": {
        "id": "qN9aAQwxnYAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"el gato come pescado\", \"el perro come pan\"]\n",
        "for j,doc in enumerate(corpus):\n",
        "    print(f\"documento {j+1}: {doc}\")\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())  # Matriz de conteo\n",
        "print(vectorizer.get_feature_names_out())  # Vocabulario"
      ],
      "metadata": {
        "id": "O-wKIc8hmZdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\"el gato come pescado\", \"el perro come pan\"]\n",
        "for j,doc in enumerate(corpus):\n",
        "    print(f\"documento {j+1}: {doc}\")\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.toarray().round(3))  # Matriz de pesos\n",
        "print(vectorizer.get_feature_names_out())  # Vocabulario"
      ],
      "metadata": {
        "id": "ALk1STzQmntj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento y Evaluaci√≥n"
      ],
      "metadata": {
        "id": "X6cnZCSHhPuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresemos a nuestro ejemplo"
      ],
      "metadata": {
        "id": "nubE_wqGndR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "                            stop_words='english',\n",
        "                            max_features=None,\n",
        "                            lowercase=True\n",
        "                            )\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)"
      ],
      "metadata": {
        "id": "0WJ85I-l4EHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.toarray()[:3,:5]"
      ],
      "metadata": {
        "id": "dOHI9BSsXlWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(f\"Hay {X_train.shape[0]} documentos y {X_train.shape[1]} features (palabras)\")"
      ],
      "metadata": {
        "id": "skSzydC-Uvfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es decir, cada documento est√° representado por un vector de 7493 componentes. ¬°Podemos tener un problema de dimensionalidad!"
      ],
      "metadata": {
        "id": "wlEpJgHfaUsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=10)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = clf.predict(X_train)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "\n",
        "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.2%}\")\n",
        "\n",
        "print(f\"Train precision: {precision_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test precision: {precision_score(y_test, y_pred_test):.2%}\")\n",
        "\n",
        "print(f\"Train recall: {recall_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test recall: {recall_score(y_test, y_pred_test):.2%}\")"
      ],
      "metadata": {
        "id": "tvxPxjOV6Pk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "plt.figure(figsize=(30, 30),dpi=300)\n",
        "plot_tree(clf, filled=True,\n",
        "          feature_names=vectorizer.get_feature_names_out(),\n",
        "          class_names=le.classes_,\n",
        "          impurity=False\n",
        "          )\n",
        "plt.savefig('tree.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xrv0FZEE6jMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = clf.predict(X_train)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "\n",
        "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.2%}\")\n",
        "\n",
        "print(f\"Train precision: {precision_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test precision: {precision_score(y_test, y_pred_test):.2%}\")\n",
        "\n",
        "print(f\"Train recall: {recall_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test recall: {recall_score(y_test, y_pred_test):.2%}\")"
      ],
      "metadata": {
        "id": "6q-HacGhtw5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un clasificador probabilisto ideal para tareas de clasificaci√≥n en NLP: **Naive Bayes** Multinomial\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=164QWAMozr4nyUvU1F05jvEk-9zxP9NmJ\" alt=\"alt text\" width=\"500\">\n",
        "\n",
        "La implementaci√≥n en scikit learn es [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). Su hiperpar√°metro principal es `alpha` que es una constante de suavizado que se usa al estimar probabilidades en t√©rminos de conteos\n",
        "\n",
        "$$P(w) = \\frac{\\text{frecuencia} + \\alpha}{\\text{vocabulario} + \\alpha}$$\n"
      ],
      "metadata": {
        "id": "iY_aDS7B5xQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, recall_score\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_train = clf.predict(X_train)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "\n",
        "print(f'Train accuracy: {accuracy_score(y_train, y_pred_train):.2%}')\n",
        "print(f'Test accuracy: {accuracy_score(y_test, y_pred_test):.2%}')\n",
        "\n",
        "print(f\"Train precision: {precision_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test precision: {precision_score(y_test, y_pred_test):.2%}\")\n",
        "\n",
        "print(f\"Train recall: {recall_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test recall: {recall_score(y_test, y_pred_test):.2%}\")"
      ],
      "metadata": {
        "id": "MtA0cfjE5SQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j6Z9z00a5vZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validemos este modelo con *Cross Validation*"
      ],
      "metadata": {
        "id": "b5Ug9IUkhkCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(np.mean(scores))\n",
        "\n",
        "scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='precision')\n",
        "print(np.mean(scores))"
      ],
      "metadata": {
        "id": "vVsy42W2iqwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearch"
      ],
      "metadata": {
        "id": "dKBZ1MaZhhZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "param_grid = {\n",
        "    'vectorizer__max_features': [None, 5000, 10000],\n",
        "    'selector__k': [100, 500, 1000, 5000],\n",
        "    'classifier__alpha': [0.01, 0.1, 1.0, 10.0]\n",
        "}\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(lowercase=True, stop_words='english')),\n",
        "    ('selector', SelectKBest()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='precision')\n",
        "grid_search.fit(train_texts, y_train)\n",
        "\n",
        "print(\"Mejores par√°metros:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "clf31eoChg4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, recall_score\n",
        "\n",
        "best_nb_model = grid_search.best_estimator_\n",
        "y_pred_test = best_nb_model.predict(test_texts)\n",
        "y_pred_train = best_nb_model.predict(train_texts)\n",
        "\n",
        "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.2%}\")\n",
        "\n",
        "print(f\"Train precision: {precision_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test precision: {precision_score(y_test, y_pred_test):.2%}\")\n",
        "\n",
        "print(f\"Train recall: {recall_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test recall: {recall_score(y_test, y_pred_test):.2%}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iKWwUsITm1Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las palabras que mejor contribuyeron a la tarea"
      ],
      "metadata": {
        "id": "WjFN9lkLskTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selection_mask = best_nb_model['selector'].get_support()  # mascara de selecci√≥n\n",
        "feature_names = best_nb_model['vectorizer'].get_feature_names_out() # palabras del vectorizador\n",
        "selected_features = feature_names[selection_mask]\n",
        "selected_features"
      ],
      "metadata": {
        "id": "pbmK4Ex4sgCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparaci√≥n con otros clasificadores\n",
        "\n",
        "Como podemos ver, la mayoria de las clasificaciones han tenido buen desempe√±o en el accuracy y precision. Comparemos varios clasificadores usando el recall."
      ],
      "metadata": {
        "id": "yKqe9A3j3QO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas\n",
        "\n",
        "\n",
        "clfs = {\n",
        "    'SVC': SVC(),\n",
        "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
        "    'RandomForestClassifier': RandomForestClassifier(),\n",
        "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'DecisionTreeClassifier': DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "accs = []\n",
        "times = []\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)\n",
        "\n",
        "for name, clf in clfs.items():\n",
        "    these_times = []\n",
        "    for i in range(5):\n",
        "        start = time.time()\n",
        "        clf.fit(X_train, y_train)\n",
        "        end = time.time()\n",
        "        these_times.append(end - start)\n",
        "    times.append(np.mean(these_times))\n",
        "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='recall')\n",
        "    accs.append(np.mean(scores))\n",
        "\n",
        "results_df = pandas.DataFrame({'clf': list(clfs.keys()), 'recall': accs, 'time': times})"
      ],
      "metadata": {
        "id": "6LdcS5CusuNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.sort_values('recall', ascending=False)"
      ],
      "metadata": {
        "id": "o8fB8KZ94oF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.sort_values('time')"
      ],
      "metadata": {
        "id": "XKlw322m5AQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, el Naive Bayes es un m√©todo muy efectivo y barato en tareas de NLP."
      ],
      "metadata": {
        "id": "e52Lai_n5sBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizaci√≥n TF-IDF"
      ],
      "metadata": {
        "id": "ypbsD5M-Yg48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(lowercase=True, stop_words='english')),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "pipeline.fit(train_texts, y_train)\n",
        "y_pred_test = pipeline.predict(test_texts)\n",
        "y_pred_train = pipeline.predict(train_texts)\n",
        "\n",
        "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.2%}\")\n",
        "\n",
        "print(f\"Train precision: {precision_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test precision: {precision_score(y_test, y_pred_test):.2%}\")"
      ],
      "metadata": {
        "id": "g1pTGHdqYUxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö°¬øC√≥mo lidiamos con el desbalanceo de clases?\n",
        "\n",
        "Tres estrategias muy usadas son:\n",
        "\n",
        "* **RandomUnderSampler**: Reduce la clase mayoritaria eliminando ejemplos al azar hasta equilibrar las clases. Es r√°pido, pero puede perder informaci√≥n √∫til.\n",
        "\n",
        "* **RandomOverSampler**: Aumenta la clase minoritaria copiando ejemplos existentes al azar. Simple, pero puede causar sobreajuste al repetir los mismos datos.\n",
        "\n",
        "* **SMOTE**: En lugar de copiar datos, crea ejemplos sint√©ticos de la clase minoritaria combinando muestras similares. Mejora la variedad de los datos, pero a veces genera ejemplos poco realistas.\n",
        "\n",
        "Por ahora, probemos *undersampling*. En notebooks posteriores probaremos SMOTE."
      ],
      "metadata": {
        "id": "MjkjjyLs7zuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler(random_state=1992)\n",
        "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "U2ENOEWO5y5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_train_rus shape: {X_train_rus.shape}\")"
      ],
      "metadata": {
        "id": "og-C730mU0Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "qqoHMFczVUe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "original_counts, rus_counts = np.unique(y_train, return_counts=True), np.unique(y_train_rus, return_counts=True)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(original_counts[0], original_counts[1])\n",
        "plt.title('Original')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(rus_counts[0], rus_counts[1])\n",
        "plt.title('Undersampled')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mv5TahvaU7HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ ¬øPor qu√© no hacemos lo mismo con el conjunto de prueba?"
      ],
      "metadata": {
        "id": "9ITQRCfh8InQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "\n",
        "clf = SVC()\n",
        "clf.fit(X_train_rus, y_train_rus)\n",
        "y_pred_train = clf.predict(X_train)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "\n",
        "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.2%}\")\n",
        "\n",
        "print(f\"Train precision: {precision_score(y_train, y_pred_train):.2%}\")\n",
        "print(f\"Test precision: {precision_score(y_test, y_pred_test):.2%}\")"
      ],
      "metadata": {
        "id": "YuOB0FEGtYnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, no ayuda mucho al desempe√±o del modelo. Algunas razones son:\n",
        "\n",
        "* Tama√±o del dataset\n",
        "* Perdida de informaci√≥n relevante"
      ],
      "metadata": {
        "id": "CIMTJbyf9AQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîΩ Representaciones vectoriales de documentos\n",
        "\n",
        "El hecho de usar `CountVectorizer` o el `TfidfVectorizer` para vectorizar los documentos se puede ver de dos maneras:\n",
        "\n",
        "1. **Extracci√≥n de features**: El texto no posee intr√≠nsecamente features que definan al texto, por lo que estas t√©cnicas transforman las palabras en caracter√≠sticas num√©ricas basadas en su frecuencia o importancia.\n",
        "2. **Representaci√≥n vectorial** de documentos: Estas herramientas convierten los textos en vectores num√©ricos, permitiendo su procesamiento matem√°tico por medio de algoritmos de ML. A diferencia de los word embeddings, que capturan significado y contexto, estos m√©todos se centran en la representaci√≥n superficial del texto.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1FxlQrTa2Vg7_QGRVlfYIM5z9tSmAzXZW\" alt=\"alt text\" width=\"500\">\n",
        "\n"
      ],
      "metadata": {
        "id": "hpLaFhr39vi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploremos los vectores de documentos"
      ],
      "metadata": {
        "id": "5M4e9-giTmly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_entradas = 80\n",
        "\n",
        "X_train.toarray()[0,:n_entradas]"
      ],
      "metadata": {
        "id": "OTbt1mDf8hlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que estos vectores son casi puros ceros, es decir, son *sparse*. Tenemos 3 opciones para combatir esto:\n",
        "\n",
        "1. Seleccionar features.\n",
        "2. Combinar features.\n",
        "3. Vectorizar de otra manera (embeddings)."
      ],
      "metadata": {
        "id": "iPmu1t9AT-z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "idx1 = np.random.choice(X_train.shape[0])\n",
        "idx2 = np.random.choice(X_train.shape[0])\n",
        "\n",
        "v1 = X_train.toarray()[idx1,:]\n",
        "v2 = X_train.toarray()[idx2,:]\n",
        "\n",
        "distancia = np.linalg.norm(v1 - v2).round(3)\n",
        "print(f\"Distancia entre los vectores de los documentos {idx1} y {idx2}: {distancia}\")\n",
        "\n",
        "similitud = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "print(f\"Similitud entre los vectores de los documentos {idx1} y {idx2}: {similitud}\")"
      ],
      "metadata": {
        "id": "pHgijV58TkvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zL-ILl43r-t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03%20Machine%20Learning/notebooks/04-RegresionPolinomial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "yHlobrfdVG61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresi√≥n Polinomial\n",
        "\n",
        "En esta notebook exploraremos y experimentaremos con la regresi√≥n polinomial.\n",
        "\n",
        "* üîΩ Esta secci√≥n no forma parte del proceso usual de Machine Learning. Es una exploraci√≥n did√°ctica de alg√∫n aspecto del funcionamiento del algoritmo.\n",
        "* ‚ö° Esta secci√≥n incluye t√©cnicas m√°s avanzadas destinadas a optimizar o profundizar en el uso de los algoritmos.\n",
        "* ‚≠ï Esta secci√≥n contiene un ejercicio o pr√°ctica a realizar. A√∫n si no se establece una fecha de entrega, es muy recomendable realizarla para practicar conceptos clave de cada tema."
      ],
      "metadata": {
        "id": "FkEjx6ZOVQAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Funci√≥n para graficar la regresi√≥n simple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def graficar(estimador,x,y):\n",
        "    xmin = np.min(x)\n",
        "    xmax = np.max(x)\n",
        "    x_values = np.linspace(xmin,xmax,100)\n",
        "    y_values = estimador.predict(x_values.reshape(-1,1))\n",
        "    y_pred = estimador.predict(x)\n",
        "    ymin = min(np.min(y),np.min(y_pred))\n",
        "    ymax = max(np.max(y),np.max(y_pred))\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.scatter(x, y,color='green')\n",
        "    plt.plot(x_values, y_values, color='black')\n",
        "    plt.title('Regresi√≥n')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.scatter(y,y_pred,color='red')\n",
        "    plt.plot(np.linspace(ymin,ymax,100),np.linspace(ymin,ymax,100),color='black')\n",
        "    plt.title('Predicciones vs Valores reales')\n",
        "    plt.xlabel('Valores reales')\n",
        "    plt.ylabel('Valores predichos')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TNYbRP-yADp9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîΩ Ejemplo 1\n",
        "\n",
        "Este primer ejemplo es un ejemplo ilustrativo. Generemos un dataset artificial con una variable predictora y una variable target para una tarea de regresi√≥n."
      ],
      "metadata": {
        "id": "OOGEf45i-Ukr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generar el dataset artificial\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "rng = np.random.RandomState(4595)\n",
        "size = 250 # N√∫mero de puntos\n",
        "\n",
        "#---- Par√°metros de la par√°bola ---\n",
        "b0 = 2\n",
        "b1 = -4\n",
        "b2 = 1\n",
        "\n",
        "#---- Generar el dataset ---\n",
        "x = 4 * rng.rand(size)\n",
        "y = (b0 + (b1*x) + (b2*x**2)) + np.random.normal(0,0.75,size=size) # Agregamos ruido con distribuci√≥n normal\n",
        "\n",
        "#---- Reshape para scikit-learn ---\n",
        "x = x.reshape(-1,1)"
      ],
      "metadata": {
        "id": "pyj1hWzCVGi4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)"
      ],
      "metadata": {
        "id": "sYeAJXv98aQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.8,random_state=2287)"
      ],
      "metadata": {
        "id": "Yhv5PjKv1xp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh5ZQGXQRjKA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.scatter(x,y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî∑ Claramente la relaci√≥n entre la variable predictora $x$ y la variable target $y$ no es lineal, sino cuadr√°tica.\n",
        "\n",
        "‚≠ï Probemos la regresi√≥n lineal cl√°sica. **Completa las lineas que faltan**"
      ],
      "metadata": {
        "id": "JUC8gsicqKlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "#---- Inicializa la clase de Regresi√≥n Lineal: ----\n",
        "\n",
        "\n",
        "\n",
        "#---- Entrena el modelo con el conjunto de entrenamiento: ----\n",
        "\n",
        "\n",
        "\n",
        "#---- Realiza las predicciones en el conjunto de prueba: ----\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#---- Evaluemos el rendimiento del modelo\n",
        "print(f\"R2 en el conjunto de entrenamiento: {lr.score(x_train,y_train)}\")\n",
        "print(f\"MAE en el conjunto de prueba: {np.round(mean_absolute_error(y_test,y_pred),3)}\")\n",
        "print(f\"MAPE en el conjunto de prueba: {np.round(mean_absolute_percentage_error(y_test,y_pred),3)}\")"
      ],
      "metadata": {
        "id": "MErXVWLMrFQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graficar(lr,x,y)"
      ],
      "metadata": {
        "id": "ccNTIK_gu8OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresi√≥n Polinomial"
      ],
      "metadata": {
        "id": "E8Cy3Nv3twJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para crear la nueva feature $x^2$ usaremos la clase [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
        "\n",
        "Esta clase genera una nueva matriz de caracter√≠sticas (*features*) consistente en todas las combinaciones polin√≥micas de las caract√©risticas de grado menor o igual al grado especificado.\n",
        "\n",
        "Por ejemplo, si una muestra de entrada es bidimensional con la forma\n",
        "\n",
        "$$[a, b]$$\n",
        "\n",
        "las caracter√≠sticas polin√≥micas (*polynomial features*) de grado 2 son\n",
        "\n",
        "$$[1, a, b, a^2, ab, b^2].$$\n",
        "\n",
        "**‚ö† Observa el efecto del hiperpar√°metro `include_bias`**"
      ],
      "metadata": {
        "id": "-0iHsOZseA_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# pfeats = PolynomialFeatures(degree=2, include_bias=False)\n",
        "pfeats = PolynomialFeatures(degree=2, include_bias=True)\n",
        "\n",
        "new_x_train = pfeats.fit_transform(x_train)\n",
        "new_x_test = pfeats.transform(x_test)"
      ],
      "metadata": {
        "id": "VUPOjKd4d16Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observar que le indicamos al constructor de la clase que no incluya el `bias` (la columna de 1s al principio de la nueva matriz de caracteristicas). Esto se hace porque pasaremos esta matriz a la regresi√≥n lineal, la cu√°l le agregar√° dicha columna."
      ],
      "metadata": {
        "id": "-efGBEsqiV-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable independiente era:"
      ],
      "metadata": {
        "id": "6nyEQ83qhXlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[:5])"
      ],
      "metadata": {
        "id": "Y_eQXvcPhdAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las nuevas variables independientes son:"
      ],
      "metadata": {
        "id": "NzkCqUzOhxN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_x_train[:5])"
      ],
      "metadata": {
        "id": "PUZAMAgfhhkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(new_x_train,y_train)"
      ],
      "metadata": {
        "id": "JUKAfLxFiE78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos:"
      ],
      "metadata": {
        "id": "1roO_aZizZib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr.score(new_x_train,y_train)"
      ],
      "metadata": {
        "id": "08bj3Xp7jZei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "y_pred = lr.predict(new_x_test)\n",
        "\n",
        "print(f\"MAE en el conjunto de prueba: {np.round(mean_absolute_error(y_test,y_pred),5)}\")\n",
        "print(f\"MAPE en el conjunto de prueba: {np.round(mean_absolute_percentage_error(y_test,y_pred),5)}\")"
      ],
      "metadata": {
        "id": "34nZlwF62V0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Intercepto: {lr.intercept_}\")\n",
        "print(f\"Coeficientes: {lr.coef_}\")"
      ],
      "metadata": {
        "id": "whnSaNJCjbwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraigamos los coeficientes"
      ],
      "metadata": {
        "id": "G95K5kCJ0JYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape de los coeficientes: {lr.coef_.shape}\")\n",
        "\n",
        "b0 = lr.intercept_\n",
        "b1 = lr.coef_[0]\n",
        "b2 = lr.coef_[1]"
      ],
      "metadata": {
        "id": "y4GUQ6XOju4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---- Graficar la par√°bola que obtuvimos ---\n",
        "x_values = np.linspace(x.min(), x.max(), 100)\n",
        "y_hat = b0 + b1*x_values + b2*x_values**2\n",
        "#------------------------------------------\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x, y, alpha=0.7,color='red')\n",
        "plt.plot(x_values, y_hat, color='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "96Y646pBje2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî∑ Veamos una mejor manera de organizar las diferentes partes del proceso de Machine Learning: **Pipelines**."
      ],
      "metadata": {
        "id": "3XQfHZVi0VFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines\n",
        "\n",
        "Un **pipeline** en scikit-learn es una secuencia de transformaciones y un estimador final que se aplican a los datos de manera automatizada. Sirve para encadenar m√∫ltiples pasos de preprocesamiento (como escalado, imputaci√≥n, etc.) y un modelo de machine learning en un √∫nico objeto, garantizando consistencia y evitando fugas de datos durante la validaci√≥n cruzada.  \n",
        "\n",
        "**Ventajas:**  \n",
        "- **Simplifica el c√≥digo:** Combina todos los pasos en un √∫nico objeto.  \n",
        "- **Previene data leakage:** Asegura que las transformaciones se apliquen correctamente durante la validaci√≥n cruzada.  \n",
        "- **Facilita el mantenimiento:** Cambios en el flujo son m√°s f√°ciles de implementar.  \n",
        "- **Reproducibilidad:** Estandariza el proceso de entrenamiento y predicci√≥n.  \n",
        "- **Optimizaci√≥n integrada:** Permite usar `GridSearchCV` o `RandomizedSearchCV` en todos los pasos del pipeline.  \n",
        "\n",
        "Documentaci√≥n: [`sklearn.pipeline.Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).  "
      ],
      "metadata": {
        "id": "Uo4y9jiE3DdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retomemos el dataset anterior"
      ],
      "metadata": {
        "id": "J5A7pkQG0yF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.scatter(x,y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aishVLG_aCgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.8,random_state=2287)\n",
        "\n",
        "print(f\"Tama√±o del conjunto de entrenamiento: {x_train.shape[0]}\")\n",
        "print(f\"Tama√±o del conjunto de prueba: {x_test.shape[0]}\")"
      ],
      "metadata": {
        "id": "l5xIrhe7aGhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el pipeline encapsularemos `PolynomialFeatures` y `LinearRegression`"
      ],
      "metadata": {
        "id": "U6ZI_NVR1E37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "pl = Pipeline([('pf',PolynomialFeatures(degree=2,include_bias=False)),\n",
        "               ('lr',LinearRegression())])"
      ],
      "metadata": {
        "id": "l_w3PaCbj6WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "7uvaHxmj6K35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos evaluar directamente el score, a√∫n cuando el score es un m√©todo de `LinearRegression`"
      ],
      "metadata": {
        "id": "tBrfvdnk7eZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pl.score(x_train,y_train)"
      ],
      "metadata": {
        "id": "He2kprrK6R1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos acceder a los atributos y m√©todos de cada parte del pipeline de la siguiente forma:"
      ],
      "metadata": {
        "id": "ibLGwcvn90Eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coeficientes de la regresi√≥n lineal"
      ],
      "metadata": {
        "id": "7JuzrhaR1evt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pl['lr'].coef_"
      ],
      "metadata": {
        "id": "j1yUYWmS7P40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un atributo del `PolynomialFeatures`"
      ],
      "metadata": {
        "id": "1AVcCJQ_1h0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pl['pf'].n_features_in_"
      ],
      "metadata": {
        "id": "4BuwNTHG96Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las predicciones en el conjunto de prueba"
      ],
      "metadata": {
        "id": "wch6b6BTEobs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graficar(pl,x_train,y_train)"
      ],
      "metadata": {
        "id": "50SVBCrH7ctq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora veamos y evaluemos las predicciones en el conjunto de prueba"
      ],
      "metadata": {
        "id": "ZTBc_fiy-M3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pl.predict(x_test)"
      ],
      "metadata": {
        "id": "WzGD2b0K9K0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl.score(x_test,y_test)"
      ],
      "metadata": {
        "id": "WDvN2RL2aMl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graficar(pl,x_test,y_test)"
      ],
      "metadata": {
        "id": "gbN06Ffv-Rxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambi√©n podemos usar este pipeline para probar r√°pidamente el modelo de regresi√≥n lineal cl√°sico o cualquier otro grado"
      ],
      "metadata": {
        "id": "-ARHSKcw9dS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "grado = 15\n",
        "\n",
        "pl = Pipeline([('pf',PolynomialFeatures(degree=grado,include_bias=False)),\n",
        "               ('lr',LinearRegression())])\n",
        "\n",
        "pl.fit(x_train,y_train)\n",
        "print(f\"R2 en el conjunto de prueba: {pl.score(x_train,y_train)}\")\n",
        "\n",
        "y_pred = pl.predict(x_test)\n",
        "\n",
        "print(f\"MAE en el conjunto de entrenamiento: {np.round(mean_absolute_error(y_train,pl.predict(x_train)),3)}\")\n",
        "print(f\"MAE en el conjunto de prueba: {np.round(mean_absolute_error(y_test,y_pred),3)}\")\n",
        "\n",
        "graficar(pl,x_test,y_test)"
      ],
      "metadata": {
        "id": "MJnv4-Qw9h6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîΩ El problema de la multicolinealidad"
      ],
      "metadata": {
        "id": "JyRG_kUhnLTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sobre la multilinearidad:\n",
        "\n",
        "\n",
        "*   [Estabilidad num√©rica](https://en.wikipedia.org/wiki/Multicollinearity#Consequences_of_multicollinearity)\n",
        "*   [Explicaci√≥n](https://medium.com/@sujathamudadla1213/why-we-have-to-remove-highly-correlated-features-in-machine-learning-9a8416286f18)\n",
        "\n"
      ],
      "metadata": {
        "id": "y-GhjdukdnzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agreguemos una variable con multicolinealidad perfecta\n",
        "\n",
        "$$x_1 = 2x_0$$"
      ],
      "metadata": {
        "id": "PmyTOtLBnXkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "rng = np.random.RandomState(4595)\n",
        "size = 100\n",
        "\n",
        "b0 = 2\n",
        "b1 = -1\n",
        "\n",
        "x = 3 * rng.rand(size)\n",
        "y = (b0 + (b1*x) ) + rng.randn(size) # Agregamos ruido con distribuci√≥n normal\n",
        "\n",
        "X = np.column_stack((x,2*x))"
      ],
      "metadata": {
        "id": "d4AKFnt1nXPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase `LinearRegression` sabe manejar este caso. Sin embargo, tenemos riesgo de inestabilidad num√©rica"
      ],
      "metadata": {
        "id": "Ve6oYePxnPRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X,y)\n",
        "\n",
        "print(lr.coef_)"
      ],
      "metadata": {
        "id": "RFBPomGPQK76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo, si usamos directamente OLS para resolver el problema, tenemos un problema:"
      ],
      "metadata": {
        "id": "Us6FNFuJnyUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.column_stack((np.ones(shape=x.shape),X))\n",
        "\n",
        "beta = (np.linalg.inv(np.transpose(X)@X)@np.transpose(X))@y"
      ],
      "metadata": {
        "id": "o2fxb3A0ke4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AG3YqEq6nGHX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
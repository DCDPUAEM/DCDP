{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IcBBCVOWQDol"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03%20Machine%20Learning/notebooks/09-Regresion-Logistica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "rfXeNiZcRgDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresi√≥n Log√≠stica\n",
        "\n",
        "En esta notebook usaremos la implementaci√≥n de scikit-learn de la regresi√≥n log√≠stica [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Primero veremos algunos ejemplos didacticos para entender el funcionamiento del clasificador. Despu√©s, resolveremos ejemplos usando algunos datasets reales.\n",
        "\n",
        "Recuerda la simbolog√≠a de las secciones:\n",
        "\n",
        "* üîΩ Esta secci√≥n no forma parte del proceso usual de Machine Learning. Es una exploraci√≥n did√°ctica de alg√∫n aspecto del funcionamiento del algoritmo.\n",
        "* ‚ö° Esta secci√≥n incluye t√©cnicas m√°s avanzadas destinadas a optimizar o profundizar en el uso de los algoritmos.\n",
        "* ‚≠ï Esta secci√≥n contiene un ejercicio o pr√°ctica a realizar. A√∫n si no se establece una fecha de entrega, es muy recomendable realizarla para practicar conceptos clave de cada tema."
      ],
      "metadata": {
        "id": "b3V19FWkQQcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Funci√≥n para graficar la frontera de decisi√≥n\n",
        "\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "import numpy as np\n",
        "\n",
        "def graficar_FD(X,y,clf,h=0):\n",
        "    '''\n",
        "    X es todas las instancias las cuales incluiremos en el gr√°fico\n",
        "    '''\n",
        "    assert X.shape[1] == 2   # S√≥lo funciona para datos en dimensi√≥n 2\n",
        "    feature_1, feature_2 = np.meshgrid(\n",
        "    np.linspace(X[:,0].min()-h, X[:, 0].max()+h),\n",
        "    np.linspace(X[:, 1].min()-h, X[:, 1].max()+h)\n",
        "    )\n",
        "    grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
        "    y_grid_pred = clf.predict(grid)\n",
        "    y_grid_pred = y_grid_pred.reshape(feature_1.shape)\n",
        "    display = DecisionBoundaryDisplay(\n",
        "    xx0=feature_1, xx1=feature_2, response=y_grid_pred\n",
        "    )\n",
        "    display.plot()\n",
        "    display.ax_.scatter(\n",
        "        X[:, 0], X[:, 1], c=y, edgecolor=\"black\"\n",
        "    )\n",
        "    plt.show()\n",
        "    return display.ax_"
      ],
      "metadata": {
        "id": "-z5xNkzh0-oH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1(a)\n",
        "\n",
        "En el siguiente ejemplo vemos c√≥mo generar fronteras de decisi√≥n m√°s complejas."
      ],
      "metadata": {
        "id": "nOJSxbuHAG2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://github.com/DCDPUAEM/DCDP/raw/main/03%20Machine%20Learning/data/binary-classification-data.csv'\n",
        "df = pd.read_csv(url,header=None)\n",
        "df"
      ],
      "metadata": {
        "id": "OqWvQhFtAF80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observemos los datos"
      ],
      "metadata": {
        "id": "LkkaTp2VELqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1],c=y,s=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w6J2M-2ZAXJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=7)"
      ],
      "metadata": {
        "id": "5RMwwHseKriM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos una regresi√≥n log√≠stica con los par√°metros por default y vemos su accuracy en el conjunto de entrenamiento y prueba."
      ],
      "metadata": {
        "id": "MpVXzmrJEUzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "print(f\"Accuracy en el entrenamiento: {clf.score(X_train,y_train)}\")\n",
        "print(f\"Accuracy en la prueba: {clf.score(X_test,y_test)}\")"
      ],
      "metadata": {
        "id": "YAa-YvQ2AsRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos una regresi√≥n log√≠stica con polinomial features y vemos su accuracy en el conjunto de entrenamiento. La justificaci√≥n de usar la regresi√≥n log√≠stica lineal, junto con polinomial features, para adaptar el m√≥delo al caso polinomial es similar a la justificaci√≥n en el caso de regresi√≥n lineal."
      ],
      "metadata": {
        "id": "lMtO_fr7Ei8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "p_feats = PolynomialFeatures(2,include_bias=False)\n",
        "log_reg = LogisticRegression(penalty='l2', C=1, solver='newton-cholesky')\n",
        "\n",
        "pl = Pipeline([('pf',p_feats),\n",
        "               ('clf',log_reg)])\n",
        "pl.fit(X,y)\n",
        "pl.score(X,y)"
      ],
      "metadata": {
        "id": "S0YYNWo-CSQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = graficar_FD(X_test,y_test,pl,h=0.5)"
      ],
      "metadata": {
        "id": "I4nbug7M5HNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1(b)\n",
        "\n",
        "En este ejemplo veremos el efecto de la regularizaci√≥n como herramienta para prevenir el *overfitting*. Veremos un dataset con muchas features y varias de ellas correlacionadas.\n",
        "\n",
        "Adem√°s, usaremos la regularizaci√≥n L1 para eliminar features."
      ],
      "metadata": {
        "id": "WG-jLebFE6d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# X, y = make_classification(n_samples=300,\n",
        "#                            n_features=60,\n",
        "#                            n_informative=20,\n",
        "#                            n_repeated=3,\n",
        "#                            n_redundant=20,\n",
        "#                            n_classes=2,\n",
        "#                            n_clusters_per_class=3,\n",
        "#                            class_sep = 0.5,\n",
        "#                            random_state=49)\n",
        "\n",
        "X, y = make_classification(n_samples=500,\n",
        "                           n_features=100,\n",
        "                           n_informative=20,\n",
        "                           n_repeated=3,\n",
        "                           n_redundant=20,\n",
        "                           n_classes=2,\n",
        "                           class_sep = 0.5,\n",
        "                           random_state=57)"
      ],
      "metadata": {
        "id": "Vz9hERUdFDLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento y evaluaci√≥n"
      ],
      "metadata": {
        "id": "t0GY51ESZhe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.75,random_state=1001)"
      ],
      "metadata": {
        "id": "DNmbURE5OhMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realicemos el entrenamiento. El rendimiento parece ser bueno en el conjunto de entrenamiento pero en el conjunto de prueba es malo. **Esta es una se√±al de overfitting**."
      ],
      "metadata": {
        "id": "bo-jhCyjPkwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(penalty=None)\n",
        "lr.fit(X_train,y_train)\n",
        "print(f\"Training score: {lr.score(X_train,y_train)}\")\n",
        "print(f\"Test score: {lr.score(X_test,y_test)}\")"
      ],
      "metadata": {
        "id": "mNR7fsqLFXwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularizaci√≥n"
      ],
      "metadata": {
        "id": "GM5vSAUnZj6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos un clasificador con regularizaci√≥n"
      ],
      "metadata": {
        "id": "ARD6dohrEf9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr2 = LogisticRegression(C=0.1,penalty='l1',solver='liblinear')\n",
        "lr2.fit(X_train,y_train)\n",
        "print(f\"Training score: {lr2.score(X_train,y_train)}\")\n",
        "print(f\"Test score: {lr2.score(X_test,y_test)}\")"
      ],
      "metadata": {
        "id": "lHLUcIizFadi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîΩ Observemosla distribuci√≥n de las magnitudes de los coeficientes"
      ],
      "metadata": {
        "id": "1MpuiWoQa0g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "normas = np.array([np.linalg.norm(x) for x in lr2.coef_[0]])\n",
        "\n",
        "plt.figure()\n",
        "sns.histplot(normas)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AcAkl2JpGzKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que hay varios coeficientes que son exactamente cero. Observar que el n√∫mero es parecido al n√∫mero de features redundantes + n√∫mero de features repetidas"
      ],
      "metadata": {
        "id": "vIekmN0jQj2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coefs = lr2.coef_\n",
        "num_zeros = coefs[coefs==0].shape[0]\n",
        "print(f\"N√∫mero de coeficientes = 0: {num_zeros}\")"
      ],
      "metadata": {
        "id": "IysWsZNVJyWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° ROC-AUC"
      ],
      "metadata": {
        "id": "Yii1KQMBZnUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos el area bajo la curva ROC como m√©trica de rendimiento del clasificador. Para esto necesitamos las probabilidades de las predicciones.\n",
        "\n",
        "Hay varias maneras de calcularlo:\n",
        "\n",
        "* Si el clasificador tiene un m√©todo `predict_proba`, usamos [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html).\n",
        "* Si no, usamos [RocCurveDisplay.from_estimator](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html#sklearn.metrics.RocCurveDisplay.from_estimator)\n",
        "\n",
        "Los siguientes estimadores tienen el m√©todo `predict_proba`: `SVC`, `DecisionTreeClassifier`, `RandomForestClassifier`"
      ],
      "metadata": {
        "id": "5s6usJcrEkc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = lr2.predict_proba(X_test)\n",
        "\n",
        "y_pred_probs.shape"
      ],
      "metadata": {
        "id": "EdfZCTRjZXPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr2.predict(X_test)\n",
        "\n",
        "print(y_pred.shape)\n",
        "\n",
        "print(y_pred_probs[:2])\n",
        "print(y_pred[:2])"
      ],
      "metadata": {
        "id": "_m4_03QjZuXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al ser clasificaci√≥n binaria, observa que s√≥lo necesitamos la primer columna de la matriz `y_pred_probs`"
      ],
      "metadata": {
        "id": "Bnkx6NWoaCZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "score = roc_auc_score(y_test, y_pred_probs[:,1])\n",
        "print(f\"ROC-AUC score: {score}\")"
      ],
      "metadata": {
        "id": "A_1SMbz1EoGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs[:,1])\n",
        "\n",
        "plt.figure()\n",
        "plt.suptitle(\"Curva ROC\")\n",
        "plt.plot(fpr,tpr,color='red')\n",
        "plt.plot([0,1],[0,1],linestyle='--',color='gray')\n",
        "plt.xlabel(\"FPR\")\n",
        "plt.ylabel(\"TPR\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zrim6QaRFXFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3: MNIST"
      ],
      "metadata": {
        "id": "LEMdMkLuQGKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos el dataset de d√≠gitos escritos a mano. Tenemos dos versiones:\n",
        "\n",
        "* Usando `keras`, es el dataset MNIST completo. Son 70,000 im√°genes de $28\\times 28$, divididas en 60,000 de entrenamiento y 10,000 de prueba.\n",
        "* Usando `sklearn`, es una versi√≥n reducida. Son 1797 im√°genes de $8\\times 8$.\n",
        "\n",
        "Por practicidad, usaremos la segunda opci√≥n."
      ],
      "metadata": {
        "id": "zTr3VB0m8i4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Versi√≥n Keras"
      ],
      "metadata": {
        "id": "aIaQfN_FtDnn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtJDTDJOQKXQ"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1,28*28)\n",
        "X_test = X_test.reshape(-1,28*28)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "gNoodzVS82tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[28]"
      ],
      "metadata": {
        "id": "KpoLrFMptONx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(7,2))\n",
        "for idx, (image, label) in enumerate(zip(X_train[:5], y_train[:5])):\n",
        "    plt.subplot(1, 5, idx + 1)\n",
        "    plt.imshow(np.reshape(image, (28,28)), cmap='gray')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(label)"
      ],
      "metadata": {
        "id": "5gSiXODftQIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Versi√≥n Scikit-learn"
      ],
      "metadata": {
        "id": "1g7o0mdBtGwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos el dataset"
      ],
      "metadata": {
        "id": "00M5Gbh99S5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "fyDYWTPZCojo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos c√≥mo se ve una instancia del conjunto de datos"
      ],
      "metadata": {
        "id": "6mi2f4UF-uHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[28]"
      ],
      "metadata": {
        "id": "WB2rc9-m-xMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos algunas instancias de entrenamiento."
      ],
      "metadata": {
        "id": "VkSa9zAz-gt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(7,2))\n",
        "for idx, (image, label) in enumerate(zip(X[:5], y[:5])):\n",
        "    plt.subplot(1, 5, idx + 1)\n",
        "    plt.imshow(np.reshape(image, (8,8)), cmap='gray')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(label)"
      ],
      "metadata": {
        "id": "kdbuOPm5QfCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos la divisi√≥n en entrenamiento y prueba."
      ],
      "metadata": {
        "id": "fWUaSnsE9VJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=128)\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "t_1rKiyCC9UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento y evaluaci√≥n"
      ],
      "metadata": {
        "id": "uNKk6N2iueTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos el entrenamiento y realizamos las predicciones sobre el conjunto de prueba. Reporta las m√©tricas de rendimiento (accuracy, recall, precision) y la m√°triz de confusi√≥n con el conjunto de prueba."
      ],
      "metadata": {
        "id": "W8klQizkRUlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "clf = LogisticRegression(solver='liblinear')\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred,average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred,average='macro'),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3rfi1u-gQYpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° Validaci√≥n cruzada\n",
        "\n",
        "Usemos validaci√≥n cruzada para evaluar el entrenamiento de nuestro modelo, [documentaci√≥n](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score).\n",
        "\n",
        "La validaci√≥n cruzada es una t√©cnica de validaci√≥n de nuestro entrenamiento. La usamos cuando queremos evaluar el desempe√±o del modelo usando s√≥lo el conjunto de entrenamiento."
      ],
      "metadata": {
        "id": "Pzc7nPcNV44K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(clf, X_train, y_train, cv=5)"
      ],
      "metadata": {
        "id": "CS0mrtlRVUr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores)\n",
        "print(np.mean(scores))"
      ],
      "metadata": {
        "id": "h6bnb-9sVgGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy en el conjunto de entrenamiento: {clf.score(X_test,y_test)}\")\n",
        "print(f\"Accuracy CV en el conjunto de entrenamiento: {np.mean(scores)}\")\n",
        "print(f\"Accuracy en el conjunto de prueba: {accuracy_score(y_test,y_pred)}\")"
      ],
      "metadata": {
        "id": "YCREDP6qTsqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° ROC-AUC\n"
      ],
      "metadata": {
        "id": "oN6YUnqff8UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = clf.predict_proba(X_test)\n",
        "y_pred_probs.shape"
      ],
      "metadata": {
        "id": "bsBk1CLIA6j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las probabilidades predichas y la etiqueta predicha"
      ],
      "metadata": {
        "id": "PflvKULISe5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred_probs[0])\n",
        "print(y_pred[0])"
      ],
      "metadata": {
        "id": "QidU_EliExcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como son probabilidades, la suma de las componentes es 1"
      ],
      "metadata": {
        "id": "bM7DKxy2Sij9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(y_pred_probs[0])"
      ],
      "metadata": {
        "id": "I5u5QDKFE5A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuando se trata de un problema multiclase podemos escoger el enfoque `ovr` (one-vs-rest) o `ovo` (one-vs-one). En este caso, no se puede graficar *directamente* la curva ROC."
      ],
      "metadata": {
        "id": "XTkP52QLDr_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "score = roc_auc_score(y_test, y_pred_probs,multi_class='ovr')\n",
        "print(f\"ROC-AUC score: {score}\")"
      ],
      "metadata": {
        "id": "u3V16eCtDFLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚≠ï Pr√°ctica *final* de clasificaci√≥n\n",
        "\n",
        "Vamos a retomar el dataset de la sesi√≥n pasada (dataset PIMA). El objetivo es tener el mejor modelo posible con cada uno de los siguientes algoritmos:\n",
        "\n",
        "* SVM\n",
        "* Decision Tree\n",
        "* Random Forest\n",
        "* Regresi√≥n Log√≠stica\n",
        "\n",
        "Con esto haremos una comparaci√≥n entre ellos.\n",
        "\n",
        "Los pasos a seguir son:\n",
        "\n",
        "1. Prepara el dataset para los algoritmo, recuerda que hay algunos valores faltantes. Adem√°s, recuerda reescalar los datos apropiadamente.\n",
        "\n",
        "2. Usando como dataset el dataset preprocesado del paso anterior, realiza una busqueda de par√°metros con cada algoritmo de acuerdo a las siguientes opciones:\n",
        "\n",
        "* SVM\n",
        " * C: 0.1,1,10,100\n",
        " * kernel: lineal, polinomial, rbf\n",
        " * grados (polinomial): 2,3,5\n",
        "* Decision Tree\n",
        " * criterion: gini, entropy, log_loss\n",
        " * max_depth: None, 10, 20,\n",
        " * min_samples_split: 2, 3, 5\n",
        "* Random Forest\n",
        " * criterion: gini, entropy, log_loss\n",
        " * max_depth: None, 10, 20,\n",
        " * min_samples_split: 2, 3, 5\n",
        "* Regresi√≥n Log√≠stica\n",
        " * C: 0.1,1,10\n",
        " * penalty: l1, l2, elasticnet, None\n",
        "\n",
        "3. Considerando los 4 mejores modelos anteriores. ¬øQu√© clasificador tiene mejor rendimiento en este dataset? Para esto toma en cuenta el accuracy en el conjunto de prueba."
      ],
      "metadata": {
        "id": "TY0QzMvrSpds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://github.com/DCDPUAEM/DCDP/raw/main/03%20Machine%20Learning/data/diabetes.csv'\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "df"
      ],
      "metadata": {
        "id": "l4aC-ulJE-sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "---------------\n",
        "    TU C√ìDIGO\n",
        "--------------\n",
        "'''"
      ],
      "metadata": {
        "id": "aHaVyS3tOg_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay m√°s clasificadores que es importante revisar. Con las herramientas que ya cuentas, ya puedes revisarlos por tu cuenta:\n",
        "\n",
        "* [K-nearest neighbors classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
        "* [Quadratic Discriminant Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)\n",
        "* [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)\n",
        "* [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes)\n",
        "* ...\n",
        "\n",
        "[M√°s informaci√≥n](https://scikit-learn.org/stable/supervised_learning.html), [comparaci√≥n](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)."
      ],
      "metadata": {
        "id": "IegLWougNyi_"
      }
    }
  ]
}
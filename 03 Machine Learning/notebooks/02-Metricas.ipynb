{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03%20Machine%20Learning/notebooks/02-Metricas.ipynb)"
      ],
      "metadata": {
        "id": "kqAFGSYu55TU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métricas de Rendimiento"
      ],
      "metadata": {
        "id": "fWoR7uPEld_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta notebook comenzaremos a usar métricas de rendimiento para cuantificar el rendimiento de predicciones hechas sobre una pequeña sección de un dataset de correo SPAM.\n",
        "\n",
        "El objetivo es visualizar la función de las métricas de rendimiento por sí mismas, independientemente de cualquier algoritmo de Machine Learning.\n",
        "\n",
        "Además, practicaremos la importación de métricas de evaluación de `scikit learn`."
      ],
      "metadata": {
        "id": "DVyD03jKlhCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "\n",
        "url = 'https://github.com/DCDPUAEM/DCDP/raw/main/03%20Machine%20Learning/data/sample-spam.csv'\n",
        "sample_df = pd.read_csv(url,index_col=0)\n",
        "sample_df.reset_index(inplace=True,drop=True)\n",
        "sample_df"
      ],
      "metadata": {
        "id": "tA2WSDsujttB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭕ Crea una lista con predicciones sobre cada texto de este dataframe, ¿es SPAM o HAM?\n",
        "\n",
        "*   HAM (No SPAM): 0\n",
        "*   SPAM: 1\n",
        "\n",
        "Esto se acostumbra así porque la tarea es detección de SPAM, entonces la clase positiva es 1 y la clase negativa es 0.\n",
        "\n",
        "Esta lista de predicciones serán las predicciones del modelo de Machine Learning (tú eres el modelo de Machine Learning).\n",
        "\n"
      ],
      "metadata": {
        "id": "NQ0UFAXCoNrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicciones =\n",
        "\n",
        "assert len(predicciones) == 10"
      ],
      "metadata": {
        "id": "tHhTlyBddQPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leemos las etiquetas reales donde se indica si cada texto es o no SPAM."
      ],
      "metadata": {
        "id": "0McgoYrOx6I0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFYdePM_d09i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora evaluaremos las predicciones usando el porcentaje de aciertos visto en la sesión pasada. Esta métrica de rendimiento se llama `Accuracy` y, al igual que muchas métricas de evaluación, se encuentra ya implementada en el módulo [`sklearn.metrics`](https://scikit-learn.org/stable/modules/model_evaluation.html) de scikit-learn.\n",
        "\n",
        "¿Qué tan buena fué tu predicción?\n",
        "\n",
        "**Observación**: En este caso, `accuracy_score` es una función."
      ],
      "metadata": {
        "id": "CqCFPs1bygxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(reales,predicciones)"
      ],
      "metadata": {
        "id": "uZ-t1GV-o_1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probemos otras métricas, [`recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) y [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.htm).\n",
        "\n",
        "⭕ Calcula e imprime ambas métricas comparando tus predicciones con las predicciones reales."
      ],
      "metadata": {
        "id": "nlS_7z9Y3ook"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score, f1_score\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "0yvbn6Qryixh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
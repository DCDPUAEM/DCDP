{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03%20Machine%20Learning/notebooks/09-CrossValidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "6f81gAT1ly6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation\n",
        "\n",
        "En esta notebook practicaremos el uso de la validaci贸n cruzada. La validaci贸n cruzada es una t茅cnica fundamental en machine learning para evaluar el rendimiento de un modelo de manera robusta y evitar el overfitting. En lugar de dividir los datos una sola vez en conjuntos de entrenamiento y prueba (como en el train-test split), la validaci贸n cruzada repite este proceso m煤ltiples veces, dividiendo los datos en *k* particiones (folds) y rotando cu谩l se usa para validaci贸n. Esto permite:\n",
        "\n",
        "* Estimar mejor la generalizaci贸n del modelo, calculando m茅tricas promedio sobre todas las iteraciones.\n",
        "* Reducir la dependencia de una divisi贸n aleatoria espec铆fica, especialmente 煤til en datasets peque帽os.\n",
        "* Optimizar hiperpar谩metros (por ejemplo, GridSearch**CV**), garantizando que la configuraci贸n elegida sea m谩s estable.\n"
      ],
      "metadata": {
        "id": "F0yVpN9naCR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=19vqfNoSDSmG4TygQRoSE-WGceThEtgWm\" alt=\"alt text\" width=\"500\">\n"
      ],
      "metadata": {
        "id": "hE5TOsIoH0fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos la implementaci贸n [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)."
      ],
      "metadata": {
        "id": "FyRQAotylhLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo de uso"
      ],
      "metadata": {
        "id": "HVYQ6mFBeEFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "wYqsxDRIHuSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.feature_names"
      ],
      "metadata": {
        "id": "8vjUViIufRWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos la descripci贸n del dataset"
      ],
      "metadata": {
        "id": "OGD-0mPcfyBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.DESCR)"
      ],
      "metadata": {
        "id": "3v-xjZKOfXee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, es un dataset ligeramente desvalanceado. No hay umbrales precisos. Una gu铆a emp铆rica es:\n",
        "\n",
        "* Balanceado: La clase minoritaria representa > 30% del total.\n",
        "* Desbalanceado moderado: Clase minoritaria entre 10% y 30%.\n",
        "* Extremadamente desbalanceado: Clase minoritaria < 10%.\n",
        "* Si la clase minoritaria tiene < 5% de los datos, se considera un problema severo (requiere t茅cnicas especiales como oversampling/SMOTE o cost-sensitive learning)."
      ],
      "metadata": {
        "id": "PzDXuvQUo5mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "ratio = np.unique(y,return_counts=True)[1][0]/len(y)\n",
        "\n",
        "labels, countings = np.unique(y,return_counts=True)\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(labels,countings)\n",
        "plt.xticks(labels)\n",
        "plt.title(f\"Distribuci贸n de clases\\n Ratio:{np.round(ratio,2)}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gPJzxAhdmU8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    stratify=y, # IMPORTANTE!\n",
        "                                                    random_state=842\n",
        "                                                    )"
      ],
      "metadata": {
        "id": "yUCpLTGYjAxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Suponiendo que y_train y y_test son tus datos\n",
        "train_labels, train_counts = np.unique(y_train, return_counts=True)\n",
        "test_labels, test_counts = np.unique(y_test, return_counts=True)\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "\n",
        "# Subplot 1: Train\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(train_labels, train_counts, color='blue', label='Train')\n",
        "plt.xticks(train_labels)\n",
        "plt.title(\"Distribuci贸n de clases - Train\")\n",
        "plt.xlabel(\"Clases\")\n",
        "plt.ylabel(\"Conteo\")\n",
        "\n",
        "# Subplot 2: Test\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(test_labels, test_counts, color='orange', label='Test')\n",
        "plt.xticks(test_labels)\n",
        "plt.title(\"Distribuci贸n de clases - Test\")\n",
        "plt.xlabel(\"Clases\")\n",
        "plt.ylabel(\"Conteo\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "shM6tMjImqD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
        "\n",
        "print(f\"Train F1 Score: {f1_score(y_train,y_train_pred)}\")\n",
        "print(f\"Test F1 Score: {f1_score(y_test, y_test_pred)}\")"
      ],
      "metadata": {
        "id": "CKbIl90mmPpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(f\"Scores: {scores}\")\n",
        "print(f\"Promedio: {np.mean(scores)}\")"
      ],
      "metadata": {
        "id": "AIAoFhofotCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_depth()"
      ],
      "metadata": {
        "id": "vD_Nl8G6qR29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifiquemos el modelo:"
      ],
      "metadata": {
        "id": "oC1YW8wzqGL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=3)\n",
        "model.fit(X_train, y_train)\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
        "\n",
        "print(f\"Train F1 Score: {f1_score(y_train,y_train_pred)}\")\n",
        "print(f\"Test F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
        "\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(f\"Scores: {scores}\")\n",
        "print(f\"Promedio: {np.mean(scores)}\")"
      ],
      "metadata": {
        "id": "T8QIspQyqJVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Como podemos ver, el cross validation exhibe el problema de overfitting.**"
      ],
      "metadata": {
        "id": "CbO6Ep6tpwyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=None)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
        "print(f\"Train F1 Score: {f1_score(y_train,y_train_pred)}\")\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=5)\n",
        "print(f\"Accuracy Promedio CV: {np.mean(scores)}\")"
      ],
      "metadata": {
        "id": "vqhsyHp-tZVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Observar c贸mo podemos hacer un entrenamiento con todo el conjunto de datos y tener una buena estimaci贸n del rendimiento del modelo, a煤n sin tener un conjunto de prueba."
      ],
      "metadata": {
        "id": "IhVJ6B03uPDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation y Overfitting\n",
        "\n",
        "En este experimento compararemos el rendimiento de dos modelos de regresi贸n (uno simple, lineal, y otro complejo, polin贸mico de grado alto) para exhibir el fen贸meno de *overfitting* y c贸mo la validaci贸n cruzada ayuda a detectarlo.\n",
        "\n",
        "**Lo mostraremos usando un problema de regresi贸n**, para hacer enfasis que no es una herramienta exclusiva de la clasificaci贸n.\n",
        "\n",
        "Generamos datos sint茅ticos basados en una funci贸n seno con ruido, y se eval煤a:\n",
        "\n",
        "* Train-Test Split: Midiendo el MAPE (Error Porcentual Absoluto Medio) en entrenamiento y prueba, donde el modelo complejo suele tener bajo error en entrenamiento pero alto en prueba (se帽al de overfitting).\n",
        "\n",
        "* 5-Fold Cross-Validation: Calculando el MAPE promedio en m煤ltiples particiones, revelando que el modelo complejo tiene mayor variabilidad y peor generalizaci贸n.\n",
        "\n",
        "* Visualizaci贸n: Gr谩ficos muestran c贸mo el modelo complejo sigue el ruido (izquierda) y su alta dispersi贸n de errores en validaci贸n cruzada (derecha).\n",
        "\n",
        "**La validaci贸n cruzada proporciona una evaluaci贸n m谩s confiable que una sola divisi贸n train-test, exponiendo la inestabilidad de modelos sobreajustados.**"
      ],
      "metadata": {
        "id": "50R5ULnVIFhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_absolute_percentage_error, make_scorer\n",
        "\n",
        "\n",
        "# Generamos datos sint茅ticos con ruido\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 1, 30)\n",
        "y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.2, 30)  # Funci贸n seno + ruido\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fPeu1mYWj9y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj0BYPI8BnbH"
      },
      "outputs": [],
      "source": [
        "grado = 12\n",
        "\n",
        "X = X.reshape(-1, 1)  # Formato correcto para estimadores de scikit-learn\n",
        "\n",
        "# Dividir en train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Definir modelos\n",
        "modelo_simple = LinearRegression()\n",
        "modelo_complejo = make_pipeline(\n",
        "    PolynomialFeatures(degree=grado),  # Polinomio de grado alto (overfitting)\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "# Entrenar y evaluar con train-test split\n",
        "modelo_simple.fit(X_train, y_train)\n",
        "modelo_complejo.fit(X_train, y_train)\n",
        "\n",
        "# Predecir\n",
        "y_pred_simple = modelo_simple.predict(X_test)\n",
        "y_pred_complejo = modelo_complejo.predict(X_test)\n",
        "\n",
        "y_pred_simple_train = modelo_simple.predict(X_train)\n",
        "y_pred_complejo_train = modelo_complejo.predict(X_train)\n",
        "\n",
        "# Calcular errores\n",
        "mse_simple = mean_absolute_percentage_error(y_test, y_pred_simple)\n",
        "mse_complejo = mean_absolute_percentage_error(y_test, y_pred_complejo)\n",
        "\n",
        "print(f\"MAPE (Simple - Train): {mean_absolute_percentage_error(y_train, y_pred_simple_train):.2f}\")\n",
        "print(f\"MAPE (Simple - Test): {mse_simple:.2f}\",end=\"\\n\"+50*\"-\"+\"\\n\")\n",
        "\n",
        "print(f\"MAPE (Complejo - Train): {mean_absolute_percentage_error(y_train, y_pred_complejo_train):.2f}\")\n",
        "print(f\"MAPE (Complejo - Test): {mse_complejo:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la m茅trica MAPE como scorer personalizado\n",
        "mape_scorer = make_scorer(\n",
        "    mean_absolute_percentage_error,\n",
        "    greater_is_better=False  # Porque el MAPE es un error (menor es mejor)\n",
        ")\n",
        "\n",
        "# Evaluar con Cross-Validation (5-Fold)\n",
        "cv_scores_simple = cross_val_score(modelo_simple, X, y, cv=5, scoring=mape_scorer)\n",
        "cv_scores_complejo = cross_val_score(modelo_complejo, X, y, cv=5, scoring=mape_scorer)\n",
        "\n",
        "mape_simple = -cv_scores_simple.mean()\n",
        "mape_complejo = -cv_scores_complejo.mean()\n",
        "\n",
        "print(f\"MAPE Promedio CV (Simple): {mape_simple:.2f}\")\n",
        "print(f\"MAPE Promedio CV (Complejo): {mape_complejo:.2f}\")"
      ],
      "metadata": {
        "id": "uDZQ3kBPBobv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gr谩fico de los datos y modelos\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X, y, s=20, label=\"Datos reales\")\n",
        "plt.plot(X, modelo_simple.predict(X), color='red', label=\"Modelo simple (grado 1)\")\n",
        "plt.plot(X, modelo_complejo.predict(X), color='green', label=\"Modelo complejo (grado 15)\")\n",
        "plt.title(\"Comparaci贸n de modelos\")\n",
        "plt.legend()\n",
        "\n",
        "# Gr谩fico de errores en CV\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(\n",
        "    [-cv_scores_simple, -cv_scores_complejo],\n",
        "    tick_labels=[\"Modelo Simple\", \"Modelo Complejo\"]\n",
        ")\n",
        "plt.title(\"Distribuci贸n del MAPE  en 5-Fold CV\")\n",
        "plt.ylabel(\"MAPE\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gEpTTdXCjthS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03%20Machine%20Learning/notebooks/09-CrossValidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "6f81gAT1ly6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation\n",
        "\n",
        "En esta notebook practicaremos el uso de la validación cruzada. La validación cruzada es una técnica fundamental en machine learning para evaluar el rendimiento de un modelo de manera robusta y evitar el overfitting. En lugar de dividir los datos una sola vez en conjuntos de entrenamiento y prueba (como en el train-test split), la validación cruzada repite este proceso múltiples veces, dividiendo los datos en *k* particiones (folds) y rotando cuál se usa para validación. Esto permite:\n",
        "\n",
        "* Estimar mejor la generalización del modelo, calculando métricas promedio sobre todas las iteraciones.\n",
        "* Reducir la dependencia de una división aleatoria específica, especialmente útil en datasets pequeños.\n",
        "* Optimizar hiperparámetros (por ejemplo, GridSearch**CV**), garantizando que la configuración elegida sea más estable.\n"
      ],
      "metadata": {
        "id": "F0yVpN9naCR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=19vqfNoSDSmG4TygQRoSE-WGceThEtgWm\" alt=\"alt text\" width=\"500\">\n"
      ],
      "metadata": {
        "id": "hE5TOsIoH0fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos la implementación [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)."
      ],
      "metadata": {
        "id": "FyRQAotylhLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo de uso"
      ],
      "metadata": {
        "id": "HVYQ6mFBeEFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "wYqsxDRIHuSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.feature_names"
      ],
      "metadata": {
        "id": "8vjUViIufRWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos la descripción del dataset"
      ],
      "metadata": {
        "id": "OGD-0mPcfyBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.DESCR)"
      ],
      "metadata": {
        "id": "3v-xjZKOfXee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, es un dataset ligeramente desvalanceado. No hay umbrales precisos. Una guía empírica es:\n",
        "\n",
        "* Balanceado: La clase minoritaria representa > 30% del total.\n",
        "* Desbalanceado moderado: Clase minoritaria entre 10% y 30%.\n",
        "* Extremadamente desbalanceado: Clase minoritaria < 10%.\n",
        "* Si la clase minoritaria tiene < 5% de los datos, se considera un problema severo (requiere técnicas especiales como oversampling/SMOTE o cost-sensitive learning)."
      ],
      "metadata": {
        "id": "PzDXuvQUo5mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "ratio = np.unique(y,return_counts=True)[1][0]/len(y)\n",
        "\n",
        "labels, countings = np.unique(y,return_counts=True)\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(labels,countings)\n",
        "plt.xticks(labels)\n",
        "plt.title(f\"Distribución de clases\\n Ratio:{np.round(ratio,2)}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gPJzxAhdmU8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    stratify=y, # IMPORTANTE!\n",
        "                                                    random_state=842\n",
        "                                                    )"
      ],
      "metadata": {
        "id": "yUCpLTGYjAxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Suponiendo que y_train y y_test son tus datos\n",
        "train_labels, train_counts = np.unique(y_train, return_counts=True)\n",
        "test_labels, test_counts = np.unique(y_test, return_counts=True)\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "\n",
        "# Subplot 1: Train\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(train_labels, train_counts, color='blue', label='Train')\n",
        "plt.xticks(train_labels)\n",
        "plt.title(\"Distribución de clases - Train\")\n",
        "plt.xlabel(\"Clases\")\n",
        "plt.ylabel(\"Conteo\")\n",
        "\n",
        "# Subplot 2: Test\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(test_labels, test_counts, color='orange', label='Test')\n",
        "plt.xticks(test_labels)\n",
        "plt.title(\"Distribución de clases - Test\")\n",
        "plt.xlabel(\"Clases\")\n",
        "plt.ylabel(\"Conteo\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "shM6tMjImqD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
        "\n",
        "print(f\"Train F1 Score: {f1_score(y_train,y_train_pred)}\")\n",
        "print(f\"Test F1 Score: {f1_score(y_test, y_test_pred)}\")"
      ],
      "metadata": {
        "id": "CKbIl90mmPpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(f\"Scores: {scores}\")\n",
        "print(f\"Promedio: {np.mean(scores)}\")"
      ],
      "metadata": {
        "id": "AIAoFhofotCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_depth()"
      ],
      "metadata": {
        "id": "vD_Nl8G6qR29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifiquemos el modelo:"
      ],
      "metadata": {
        "id": "oC1YW8wzqGL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=3)\n",
        "model.fit(X_train, y_train)\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
        "\n",
        "print(f\"Train F1 Score: {f1_score(y_train,y_train_pred)}\")\n",
        "print(f\"Test F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
        "\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(f\"Scores: {scores}\")\n",
        "print(f\"Promedio: {np.mean(scores)}\")"
      ],
      "metadata": {
        "id": "T8QIspQyqJVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Como podemos ver, el cross validation exhibe el problema de overfitting.**"
      ],
      "metadata": {
        "id": "CbO6Ep6tpwyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=None)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
        "print(f\"Train F1 Score: {f1_score(y_train,y_train_pred)}\")\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=5)\n",
        "print(f\"Accuracy Promedio CV: {np.mean(scores)}\")"
      ],
      "metadata": {
        "id": "vqhsyHp-tZVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Observar cómo podemos hacer un entrenamiento con todo el conjunto de datos y tener una buena estimación del rendimiento del modelo, aún sin tener un conjunto de prueba."
      ],
      "metadata": {
        "id": "IhVJ6B03uPDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation y Overfitting\n",
        "\n",
        "En este experimento compararemos el rendimiento de dos modelos de regresión (uno simple, lineal, y otro complejo, polinómico de grado alto) para exhibir el fenómeno de *overfitting* y cómo la validación cruzada ayuda a detectarlo.\n",
        "\n",
        "**Lo mostraremos usando un problema de regresión**, para hacer enfasis que no es una herramienta exclusiva de la clasificación.\n",
        "\n",
        "Generamos datos sintéticos basados en una función seno con ruido, y se evalúa:\n",
        "\n",
        "* Train-Test Split: Midiendo el MAPE (Error Porcentual Absoluto Medio) en entrenamiento y prueba, donde el modelo complejo suele tener bajo error en entrenamiento pero alto en prueba (señal de overfitting).\n",
        "\n",
        "* 5-Fold Cross-Validation: Calculando el MAPE promedio en múltiples particiones, revelando que el modelo complejo tiene mayor variabilidad y peor generalización.\n",
        "\n",
        "* Visualización: Gráficos muestran cómo el modelo complejo sigue el ruido (izquierda) y su alta dispersión de errores en validación cruzada (derecha).\n",
        "\n",
        "**La validación cruzada proporciona una evaluación más confiable que una sola división train-test, exponiendo la inestabilidad de modelos sobreajustados.**"
      ],
      "metadata": {
        "id": "50R5ULnVIFhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_absolute_percentage_error, make_scorer\n",
        "\n",
        "\n",
        "# Generamos datos sintéticos con ruido\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 1, 30)\n",
        "y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.2, 30)  # Función seno + ruido\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fPeu1mYWj9y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj0BYPI8BnbH"
      },
      "outputs": [],
      "source": [
        "grado = 12\n",
        "\n",
        "X = X.reshape(-1, 1)  # Formato correcto para estimadores de scikit-learn\n",
        "\n",
        "# Dividir en train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Definir modelos\n",
        "modelo_simple = LinearRegression()\n",
        "modelo_complejo = make_pipeline(\n",
        "    PolynomialFeatures(degree=grado),  # Polinomio de grado alto (overfitting)\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "# Entrenar y evaluar con train-test split\n",
        "modelo_simple.fit(X_train, y_train)\n",
        "modelo_complejo.fit(X_train, y_train)\n",
        "\n",
        "# Predecir\n",
        "y_pred_simple = modelo_simple.predict(X_test)\n",
        "y_pred_complejo = modelo_complejo.predict(X_test)\n",
        "\n",
        "y_pred_simple_train = modelo_simple.predict(X_train)\n",
        "y_pred_complejo_train = modelo_complejo.predict(X_train)\n",
        "\n",
        "# Calcular errores\n",
        "mse_simple = mean_absolute_percentage_error(y_test, y_pred_simple)\n",
        "mse_complejo = mean_absolute_percentage_error(y_test, y_pred_complejo)\n",
        "\n",
        "print(f\"MAPE (Simple - Train): {mean_absolute_percentage_error(y_train, y_pred_simple_train):.2f}\")\n",
        "print(f\"MAPE (Simple - Test): {mse_simple:.2f}\",end=\"\\n\"+50*\"-\"+\"\\n\")\n",
        "\n",
        "print(f\"MAPE (Complejo - Train): {mean_absolute_percentage_error(y_train, y_pred_complejo_train):.2f}\")\n",
        "print(f\"MAPE (Complejo - Test): {mse_complejo:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la métrica MAPE como scorer personalizado\n",
        "mape_scorer = make_scorer(\n",
        "    mean_absolute_percentage_error,\n",
        "    greater_is_better=False  # Porque el MAPE es un error (menor es mejor)\n",
        ")\n",
        "\n",
        "# Evaluar con Cross-Validation (5-Fold)\n",
        "cv_scores_simple = cross_val_score(modelo_simple, X, y, cv=5, scoring=mape_scorer)\n",
        "cv_scores_complejo = cross_val_score(modelo_complejo, X, y, cv=5, scoring=mape_scorer)\n",
        "\n",
        "mape_simple = -cv_scores_simple.mean()\n",
        "mape_complejo = -cv_scores_complejo.mean()\n",
        "\n",
        "print(f\"MAPE Promedio CV (Simple): {mape_simple:.2f}\")\n",
        "print(f\"MAPE Promedio CV (Complejo): {mape_complejo:.2f}\")"
      ],
      "metadata": {
        "id": "uDZQ3kBPBobv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gráfico de los datos y modelos\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X, y, s=20, label=\"Datos reales\")\n",
        "plt.plot(X, modelo_simple.predict(X), color='red', label=\"Modelo simple (grado 1)\")\n",
        "plt.plot(X, modelo_complejo.predict(X), color='green', label=\"Modelo complejo (grado 15)\")\n",
        "plt.title(\"Comparación de modelos\")\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico de errores en CV\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(\n",
        "    [-cv_scores_simple, -cv_scores_complejo],\n",
        "    tick_labels=[\"Modelo Simple\", \"Modelo Complejo\"]\n",
        ")\n",
        "plt.title(\"Distribución del MAPE  en 5-Fold CV\")\n",
        "plt.ylabel(\"MAPE\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gEpTTdXCjthS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03%20Machine%20Learning/notebooks/07-Clasificacion-Otros.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "n3cv9-x_gKat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Otros clasificadores\n",
        "\n",
        "<h2>√Årboles de decisi√≥n, Vecinos m√°s cercanos, Random Forest y Regresi√≥n Log√≠stica</h2>"
      ],
      "metadata": {
        "id": "o6TaZpW8qma6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lo largo de esta notebook compararemos varios clasificadores, viendo su funcionamiento en un dataset artificial y uno real. Adem√°s, veremos atributos y m√©todos importantes de cada uno.\n",
        "\n",
        "Veremos:\n",
        "\n",
        "* √Årboles de decisi√≥n\n",
        "* Bosques aleatorios\n",
        "* Vecinos m√°s cercanos\n",
        "* Regresi√≥n Log√≠stica\n",
        "\n",
        "En cada uno de los clasificadores experimentaremos con algunos de sus hiperpar√°metros principales y derivaremos conclusiones usando los atributos de los modelos entrenados\n",
        "\n",
        "---\n",
        "\n",
        "Recuerda la simbolog√≠a de las notebooks:\n",
        "\n",
        "* üîΩ Esta secci√≥n no forma parte del proceso usual de Machine Learning. Es una exploraci√≥n did√°ctica de alg√∫n aspecto del funcionamiento del algoritmo.\n",
        "* ‚ö° Esta secci√≥n incluye t√©cnicas m√°s avanzadas destinadas a optimizar o profundizar en el uso de los algoritmos.\n",
        "* ‚≠ï Esta secci√≥n contiene un ejercicio o pr√°ctica a realizar. A√∫n si no se establece una fecha de entrega, es muy recomendable realizarla para practicar conceptos clave de cada tema."
      ],
      "metadata": {
        "id": "t1DZOGMrQDix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Los datasets"
      ],
      "metadata": {
        "id": "ONC5NoYhRFQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Dataset linealmente separable"
      ],
      "metadata": {
        "id": "rNgvbD62RVO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Conjunto de datos\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=600,centers=2,random_state=31)\n",
        "\n",
        "theta = np.pi/4 # √Ångulo de rotaci√≥n\n",
        "R = np.array([[np.cos(theta),-np.sin(theta)],[np.sin(theta),np.cos(theta)]]) # Matriz de rotaci√≥n\n",
        "\n",
        "Xr = np.transpose(R@np.transpose(X)) # Rotamos el dataset\n",
        "\n",
        "idxs = np.where(y==1)[0]   # Obtenemos los √≠ndices donde y=1\n",
        "\n",
        "Xr[idxs,:] = Xr[idxs,:] + np.array([-1,-2])\n",
        "\n",
        "X1, y1 = Xr, y\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(X1[:,0],X1[:,1],c=y1)\n",
        "plt.axis('off')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UUxuqZnCTEb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels, counts = np.unique(y1,return_counts=True)\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(labels,counts)\n",
        "plt.suptitle(f\"Conteo de las clases\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R1Fex7MWKLv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST lite\n",
        "\n",
        "Esta versi√≥n del dataset MNIST solamente tiene 1797 im√°genes y cada imagen es de $8\\times 8$."
      ],
      "metadata": {
        "id": "n7WyzpRinMxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "X_mnist = digits.data\n",
        "y_mnist = digits.target\n",
        "\n",
        "print(f\"Tama√±o del dataset: {X_mnist.shape}\")\n",
        "\n",
        "idx = 45\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(X_mnist[idx].reshape(8,8),cmap='gray')\n",
        "plt.suptitle(f\"D√≠gito: {y_mnist[idx]}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7nQe2VjYnPIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el balanceo de las clases:"
      ],
      "metadata": {
        "id": "Rk-RIme-IPor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels, counts = np.unique(y_mnist,return_counts=True)\n",
        "\n",
        "min, max = np.min(counts), np.max(counts)\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(labels,counts)\n",
        "plt.axhline(min,color='black',linestyle='--')\n",
        "plt.axhline(max,color='black',linestyle='--')\n",
        "plt.suptitle(f\"Conteo de las clases\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iQBsbK73G61v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificador 1: √Årboles de decisi√≥n"
      ],
      "metadata": {
        "id": "WkEmgr9yQA48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta notebook usaremos el clasificador [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) de scikit-learn.\n",
        "\n",
        "Primero, observaremos algunas caracteristicas generales del m√©todo y despu√©s, lo usaremos en un problema de clasificaci√≥n con un dataset cl√°sico del machine learning.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DHVlH2Bud4Da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El m√≥dulo `dtreeviz` es √∫til para la visualizaci√≥n de √°rboles de decisi√≥n y la interpretaci√≥n de modelos ([documentaci√≥n](https://github.com/parrt/dtreeviz))."
      ],
      "metadata": {
        "id": "h6vhe29d5uVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq dtreeviz"
      ],
      "metadata": {
        "id": "YKhWGzhP5hNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 1: Ejemplo ilustrativo con datos linealmente separables\n",
        "\n",
        "Con este ejemplo, exploraremos el uso b√°sico del √°lgoritmo y observaremos las caracter√≠sticas y particularidades del clasificador DT."
      ],
      "metadata": {
        "id": "hHm65FMfuY4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retomamos el dataset"
      ],
      "metadata": {
        "id": "RRvwyIhEZwWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(X1[:,0],X1[:,1],c=y1)\n",
        "plt.axis('off')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AsmIGzTx6TmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=101) # 70% training and 30% test\n",
        "\n",
        "print(f\"Tama√±o del conjunto de entrenamiento: {X_train.shape[0]}\")\n",
        "print(f\"Tama√±o del conjunto de prueba: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "id": "ehKSVwpH_r0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrenamiento y evaluaci√≥n"
      ],
      "metadata": {
        "id": "v4Mgg6M1NZ9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier()  # Create Decision Tree classifier object\n",
        "clf = clf.fit(X_train,y_train)  # Train Decision Tree Classifier\n",
        "y_pred = clf.predict(X_test)    # Predict the response for test dataset"
      ],
      "metadata": {
        "id": "BwjCPJ_6NgoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dados que los datos siguen siendo linealmente separables, seguimos obteniendo el 100% en todas las m√©tricas."
      ],
      "metadata": {
        "id": "TYrghH9rC-F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred),5)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred),3)}\")\n",
        "print(f\"F1 score: {round(f1_score(y_test,y_pred),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1lSQ8DO4_zwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizaci√≥n del √°rbol"
      ],
      "metadata": {
        "id": "jOcQyoF-NomE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el √°rbol de decisi√≥n usando [`export_text`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html) del m√≥dulo `tree` de scikit-learn."
      ],
      "metadata": {
        "id": "VU2uE9kFaGCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_text\n",
        "\n",
        "text_representation = export_text(decision_tree=clf,feature_names=['x','y'])\n",
        "print(text_representation)"
      ],
      "metadata": {
        "id": "eK2bQQc5_4yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualicemos la frontera de decisi√≥n usando [`DecisionBoundaryDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html)\n",
        "\n",
        "La frontera de decisi√≥n podr√≠a no ser la que esperar√≠amos:"
      ],
      "metadata": {
        "id": "M0smc2XnR1F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X1[:, 0], X1[:, 1], c=y1) # Hacemos el scatter con los puntos que queremos mostrar\n",
        "DecisionBoundaryDisplay.from_estimator(clf, X1,\n",
        "                                      ax=ax, alpha=0.5) # Mostramos la frontera de decisi√≥n encima\n",
        "ax.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mx8i-5bGCUUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, este clasificador no separa con una l√≠nea en general, a√∫n si los datos son linealmente separables. **Los √°rboles de decisi√≥n obtienen una FD compuesta de segmentos de l√≠nea verticales y horizontales.**"
      ],
      "metadata": {
        "id": "fMZFacpJqyQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Efecto de perturbaciones"
      ],
      "metadata": {
        "id": "11W12kCidBep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, observemos el efecto de perturbar levemente el conjunto de datos. ¬øQu√© le pasa al arbol de decisi√≥n?\n",
        "\n",
        "Este tipo de perturbaciones pueden ocurrir como resultado de errores de medici√≥n o de la presencia de outliers.\n",
        "\n",
        "Movemos un par de puntos cerca de la FD.\n",
        "\n",
        "**Conclusi√≥n:** Un √°rbol de decisi√≥n es sensible a outliers y ruido."
      ],
      "metadata": {
        "id": "Mr0pJ-nKSfQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset con perturbaci√≥n\n",
        "\n",
        "Xrp = X1.copy()\n",
        "Xrp[192] = Xrp[192] + np.array([-1,-2])\n",
        "Xrp[486] = Xrp[486] + np.array([2,1])\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(9,5),sharey=True)\n",
        "axs[0].scatter(Xr[:,0],Xr[:,1],c=y1)\n",
        "axs[0].set_title(\"Dataset Original\")\n",
        "axs[1].scatter(Xrp[:,0],Xrp[:,1],c=y1)\n",
        "axs[1].set_title(\"Dataset Perturbado\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "8hUiUrdOdH9b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Xrp, y, test_size=0.3, random_state=101) # 70% training and 30% test\n",
        "\n",
        "clf = DecisionTreeClassifier()  # Create Decision Tree classifier object\n",
        "clf = clf.fit(X_train,y_train)  # Train Decision Tree Classifier\n",
        "y_pred = clf.predict(X_test)    # Predict the response for test dataset"
      ],
      "metadata": {
        "id": "PiWGKgphQJFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred),5)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dl_M9fD2QJFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_text\n",
        "\n",
        "text_representation = export_text(decision_tree=clf,feature_names=['x','y'])\n",
        "print(text_representation)"
      ],
      "metadata": {
        "id": "L5f2V5DEQJFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "fig = plt.figure()   # Definimos una figura m√°s grande para que quepa\n",
        "_ = plot_tree(clf, feature_names=['x','y'],\n",
        "                   class_names=['0','1'],\n",
        "                   filled=True)"
      ],
      "metadata": {
        "id": "rhTzYqn3R2bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(Xrp[:, 0], Xrp[:, 1], c=y1) # Hacemos el scatter con los puntos que queremos mostrar\n",
        "DecisionBoundaryDisplay.from_estimator(clf, Xrp,\n",
        "                                      ax=ax, alpha=0.5) # Mostramos la frontera de decisi√≥n encima\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fbmVDMdXQJFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 2: MNIST"
      ],
      "metadata": {
        "id": "BOHwPFOmX7zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "X = digits.data\n",
        "y = digits.target"
      ],
      "metadata": {
        "id": "Le5lbPpXX_oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos los rangos de las variables"
      ],
      "metadata": {
        "id": "xb2S12haAej9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(f\"M√°ximo: {np.max(X)}, M√≠nimo: {np.min(X)}\")\n",
        "pd.DataFrame(X).describe()"
      ],
      "metadata": {
        "id": "pKXZ7DeSANVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenemos el modelo sin normalizar"
      ],
      "metadata": {
        "id": "svZyfphEAKLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "JG9-Y5TVYWxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy entrenamiento: {clf.score(X_train,y_train)}\")\n",
        "print(f\"Accuracy prueba: {clf.score(X_test,y_test)}\")"
      ],
      "metadata": {
        "id": "Lpuc3WXfSJFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos otra herramienta de evaluaci√≥n en la clasificaci√≥n: La funci√≥n [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html), que muestra un reporte de varias m√©tricas de clasificaci√≥n en cada una de las etiquetas, adem√°s de varios promedios."
      ],
      "metadata": {
        "id": "HOXBaAOvnnxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "nk19hw3snUzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viendo las m√©tricas, ¬øhay se√±ales de *overfitting*?"
      ],
      "metadata": {
        "id": "4gWKa8KHS99Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred, average='macro'),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fMzvZFqlYwRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el √°rbol de decisi√≥n"
      ],
      "metadata": {
        "id": "dUf4C5dqns2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "from sklearn.tree import export_text\n",
        "\n",
        "text_representation = export_text(decision_tree=clf,feature_names=[f'pixel_{j+1}' for j in range(X.shape[1])])\n",
        "print(text_representation)"
      ],
      "metadata": {
        "id": "5CIDRJC9Y7NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos la profundidad del √°rbol"
      ],
      "metadata": {
        "id": "Uewv4TomqpHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Profundidad del √°rbol: {clf.get_depth()}\")"
      ],
      "metadata": {
        "id": "ZCe_KpB0qicA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analicemos el comportamiento del *accuracy* en t√©rminos de la profundidad m√°xima"
      ],
      "metadata": {
        "id": "sgqVN64CqE88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depths = list(range(2,16))\n",
        "\n",
        "accs_train = []\n",
        "accs_test = []\n",
        "\n",
        "for d in depths:\n",
        "    clf = DecisionTreeClassifier(max_depth=d)\n",
        "    clf = clf.fit(X_train,y_train)\n",
        "    accs_train.append(clf.score(X_train,y_train))\n",
        "    accs_test.append(clf.score(X_test,y_test))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(depths,accs_train,label='Entrenamiento')\n",
        "plt.plot(depths,accs_test,label='Prueba')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qWdAVZUWpwBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploremos el efecto de los principales hiperpar√°metros"
      ],
      "metadata": {
        "id": "v6G5DAjGpW3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "depths = list(range(2,10)) + [None]\n",
        "\n",
        "@interact(max_depth=depths)\n",
        "def train_rf(max_depth=5):\n",
        "    model = DecisionTreeClassifier(max_depth=max_depth)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    plt.figure(figsize=(3,3))\n",
        "    cm = confusion_matrix(y_test,y_pred)\n",
        "    s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qb2bhaahoL5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importancia de las features\n",
        "\n",
        "Un √°rbol de decisi√≥n nos permite conocer la importancia de cada feature. Se encuentran en el atributo `feature_importances_`, el cual es un arreglo.\n",
        "\n",
        "üîµ Reflexionemos en estas preguntas:\n",
        "\n",
        "* ¬øQu√© son las features en este problema?\n",
        "* ¬øCu√°les esperamos que sean las features m√°s importantes?\n",
        "\n",
        "Estas importancias las determina en funci√≥n de la separaci√≥n de clases que induce en los nodos donde aparecen estas features."
      ],
      "metadata": {
        "id": "7d1YrYeIrd5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ö° Veamos los pixeles que m√°s discriminan entre clases"
      ],
      "metadata": {
        "id": "hnbHMqQ-AyFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "print(f\"Tama√±o del conjunto de entrenamiento: {X_train.shape}\")\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "\n",
        "importancias = clf.feature_importances_\n",
        "print(f\"Importancia de las features (shape): {importancias.shape}\")\n",
        "importancias_matriz = importancias.reshape(8,8)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "im = plt.imshow(importancias_matriz, cmap='viridis', interpolation='nearest')\n",
        "plt.colorbar(im, label='Importancia del p√≠xel')\n",
        "plt.title('Importancia de los p√≠xeles seg√∫n el √Årbol de Decisi√≥n')\n",
        "plt.axis('off')  # Opcional: oculta los ejes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4P6VyEPPrgcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos ejemplos de c√≥mo son las im√°genes en esos pixeles. Nos restringimos a importancias mayores a cierto umbral  "
      ],
      "metadata": {
        "id": "eY-WRsHFBDpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 942 # 897 159\n",
        "ejemplo = X_train[idx].reshape(8, 8)\n",
        "\n",
        "filtered_importancias = np.where(importancias_matriz>0.065,importancias_matriz,np.zeros_like(importancias_matriz))\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9, 3))\n",
        "\n",
        "# Mapa de importancia\n",
        "ax1.imshow(filtered_importancias, cmap='viridis', interpolation='nearest')\n",
        "ax1.set_title('Importancia de p√≠xeles')\n",
        "ax1.axis('off')\n",
        "\n",
        "# D√≠gito de ejemplo\n",
        "ax2.imshow(ejemplo, cmap='gray_r', interpolation='nearest')\n",
        "ax2.set_title('D√≠gito de ejemplo')\n",
        "ax2.axis('off')\n",
        "\n",
        "# Superposici√≥n\n",
        "ax3.imshow(filtered_importancias, cmap='plasma', alpha=0.75, interpolation='nearest')\n",
        "ax3.imshow(ejemplo, cmap='gray_r', alpha=0.65, interpolation='nearest')\n",
        "ax3.set_title('Superposici√≥n')\n",
        "ax3.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "42yeXIWLwluO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Efecto de la normalizaci√≥n"
      ],
      "metadata": {
        "id": "6uP__ydLXPNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "# ----- Normalizando ----\n",
        "pl = Pipeline([('scl',MinMaxScaler()),\n",
        "               ('clf',DecisionTreeClassifier())])\n",
        "pl.fit(X_train,y_train)\n",
        "print(f\"Accuracy de prueba (Normalizando): {pl.score(X_test,y_test)}\")\n",
        "\n",
        "# ---- Sin normalizar ----\n",
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "print(f\"Accuracy de prueba (Sin normalizar): {clf.score(X_test,y_test)}\")\n"
      ],
      "metadata": {
        "id": "bquVwsUweOax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la ayuda de gridsearch, veamos cu√°l es la mejor profundidad que podemos obtener"
      ],
      "metadata": {
        "id": "XY2wWtUTWT_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "gs = GridSearchCV(estimator = DecisionTreeClassifier(),\n",
        "                  param_grid = {'max_depth': depths})\n",
        "gs.fit(X_train,y_train)\n",
        "print(gs.best_params_)\n",
        "print(f\"Accuracy de prueba con el mejor clasificador: {gs.best_estimator_.score(X_test,y_test)}\")"
      ],
      "metadata": {
        "id": "YKOmR3XTUzuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparaci√≥n con SVM"
      ],
      "metadata": {
        "id": "_0uXYModWQsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "import time\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)\n",
        "\n",
        "num_pruebas = 7\n",
        "\n",
        "dt_times = []\n",
        "for j in range(num_pruebas):\n",
        "    start = time.time()\n",
        "    clf = DecisionTreeClassifier(max_depth=None)\n",
        "    clf = clf.fit(X_train,y_train)\n",
        "    end = time.time()\n",
        "    dt_times.append(end-start)\n",
        "print(f\"Tiempo promedio de ejecuci√≥n DT: {np.mean(dt_times)}\")\n",
        "\n",
        "svm_times = []\n",
        "for j in range(num_pruebas):\n",
        "    start = time.time()\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf = clf.fit(X_train,y_train)\n",
        "    end = time.time()\n",
        "    svm_times.append(end-start)\n",
        "print(f\"Tiempo promedio de ejecuci√≥n SVM: {np.mean(svm_times)}\")\n"
      ],
      "metadata": {
        "id": "vX-9U_75WS-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬øC√≥mo se compara el rendimiento con un SVM?"
      ],
      "metadata": {
        "id": "YiHH6O5dbgM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "clf = SVC()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "print(f\"Accuracy de prueba (SVM): {clf.score(X_test,y_test)}\")\n",
        "\n",
        "print(f\"Accuracy de prueba (DT): {gs.best_estimator_.score(X_test,y_test)}\")"
      ],
      "metadata": {
        "id": "08bgv1Y0bmTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusi√≥n\n",
        "\n",
        "Ventajas:\n",
        "\n",
        "* Es robusto ante la falta de normalizaci√≥n\n",
        "* Es r√°pido\n",
        "\n",
        "Desventajas:\n",
        "\n",
        "* Es susceptible a perturbaciones (ruido)\n",
        "* Es susceptible a overfitting"
      ],
      "metadata": {
        "id": "IA4BlWG15RWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificador 2: Random Forest"
      ],
      "metadata": {
        "id": "rz-7yl_OB8RI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1vR5_xEzC4l3aWURPCNkwlceDzJ7cbh-s\" alt=\"random forest\" width=\"500\">\n",
        "\n",
        "Ahora experimentemos con un clasificador [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Este es un clasificador **ensamble**, es decir, se compone de varios clasificadores √°rbol de decisi√≥n, cada uno emitiendo un voto sobre la clasificaci√≥n.\n"
      ],
      "metadata": {
        "id": "Ex0o1fb9fxoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 1"
      ],
      "metadata": {
        "id": "da1F_6qFzQPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(X1[:,0],X1[:,1],c=y1)\n",
        "plt.axis('off')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fKJ4CcS2zTdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos y evaluamos un modelo de Random Forest, dado que los datos son linealmente separables, esperamos obtener 100% en las m√©tricas"
      ],
      "metadata": {
        "id": "AeEU_BInzj-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=101) # 70% training and 30% test\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"F1 score: {round(f1_score(y_test,y_pred, average='macro'),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z4WYsZP0zeyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adem√°s, visalicemos la frontera de decisi√≥n"
      ],
      "metadata": {
        "id": "8JL6YQV00NhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X1[:, 0], X1[:, 1], c=y1) # Hacemos el scatter con los puntos que queremos mostrar\n",
        "DecisionBoundaryDisplay.from_estimator(clf, X1,\n",
        "                                      ax=ax, alpha=0.5) # Mostramos la frontera de decisi√≥n encima\n",
        "ax.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gHXBub-2z6LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 2"
      ],
      "metadata": {
        "id": "fW7rs6h7zSSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero, hagamos la clasificaci√≥n incial, con los hiperpar√°metros por defecto"
      ],
      "metadata": {
        "id": "b57ltaP7k0YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=50)\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"F1 score: {round(f1_score(y_test,y_pred, average='macro'),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZF72KyQGCBkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ ¬øQu√© pasa si lo corremos otra vez? ¬øObtenemos el mismo resultado? Cambia si fijamos el `random_state`?"
      ],
      "metadata": {
        "id": "6VsQ8_FhGfm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el efecto de la normalizaci√≥n (re-escalamiento)"
      ],
      "metadata": {
        "id": "kF_Ey_Dm5e0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "pl = Pipeline([\n",
        "            ('scl',StandardScaler()),\n",
        "            # ('scl',MinMaxScaler()),\n",
        "            ('clf',RandomForestClassifier(n_estimators=50))])\n",
        "pl.fit(X_train,y_train)\n",
        "y_pred = pl.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"F1 score: {round(f1_score(y_test,y_pred, average='macro'),3)}\")\n",
        "\n",
        "# plt.figure(figsize=(3,3))\n",
        "# cm = confusion_matrix(y_test,y_pred)\n",
        "# s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "ueY51wzJ5jj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observemos que tenemos una mayor *estabilidad* ante los hiperpar√°metros. Esto se debe a la naturaleza colectiva de la predicci√≥n."
      ],
      "metadata": {
        "id": "5i_efgXEqKKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "depths = list(range(2,14)) + [None]\n",
        "\n",
        "@interact(n_estimators=[10, 50, 100, 200], max_depth=depths)\n",
        "def train_rf(n_estimators=100, max_depth=5):\n",
        "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    plt.figure(figsize=(3,3))\n",
        "    cm = confusion_matrix(y_test,y_pred)\n",
        "    s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RGGO99S8mjeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el comportamiento del accuracy en el conjunto de prueba, vemos que se observa un comportamiento m√°s robusto ante el overfitting. Ahora el par√°metro dominante respecto a la complejidad es el `n_estimators`"
      ],
      "metadata": {
        "id": "uIyrMwIw0otU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Configuraci√≥n de la figura\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# --- Gr√°fico 1: Accuracy vs max_depth (original) ---\n",
        "depths = list(range(2, 16))\n",
        "accs_train = []\n",
        "accs_test = []\n",
        "\n",
        "for d in depths:\n",
        "    clf = RandomForestClassifier(max_depth=d, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    accs_train.append(clf.score(X_train, y_train))\n",
        "    accs_test.append(clf.score(X_test, y_test))\n",
        "\n",
        "ax1.plot(depths, accs_train, label='Entrenamiento', marker='o')\n",
        "ax1.plot(depths, accs_test, label='Prueba', marker='o')\n",
        "ax1.set_xlabel('max_depth')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Accuracy vs. Profundidad del √Årbol')\n",
        "ax1.legend()\n",
        "\n",
        "# --- Gr√°fico 2: Accuracy vs n_estimators (nuevo) ---\n",
        "n_estimators = [10, 50, 100, 200, 300, 400, 500]\n",
        "accs_train_est = []\n",
        "accs_test_est = []\n",
        "\n",
        "for n in n_estimators:\n",
        "    clf = RandomForestClassifier(n_estimators=n, random_state=42, max_depth=5)\n",
        "    clf.fit(X_train, y_train)\n",
        "    accs_train_est.append(clf.score(X_train, y_train))\n",
        "    accs_test_est.append(clf.score(X_test, y_test))\n",
        "\n",
        "ax2.plot(n_estimators, accs_train_est, label='Entrenamiento', marker='o')\n",
        "ax2.plot(n_estimators, accs_test_est, label='Prueba', marker='o')\n",
        "ax2.set_xlabel('n_estimators')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Accuracy vs. N√∫mero de √Årboles')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hPYXzw7d0Zqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora veamos la importancia de las features"
      ],
      "metadata": {
        "id": "eP-OmUqP2JU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "print(f\"Tama√±o del conjunto de entrenamiento: {X_train.shape}\")\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "\n",
        "# ------ IMPORTANCIA DE LAS FEATURES ---------\n",
        "importancias = clf.feature_importances_\n",
        "# ---------------------------------------------\n",
        "print(f\"Importancia de las features (shape): {importancias.shape}\")\n",
        "importancias_matriz = importancias.reshape(8,8)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "im = plt.imshow(importancias_matriz, cmap='viridis', interpolation='nearest')\n",
        "plt.colorbar(im, label='Importancia del p√≠xel')\n",
        "plt.title('Importancia de los p√≠xeles seg√∫n el √Årbol de Decisi√≥n')\n",
        "plt.axis('off')  # Opcional: oculta los ejes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ysKw72s9qVrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.random.choice(X_train.shape[0],size=1)\n",
        "ejemplo = X_train[idx].reshape(8, 8)\n",
        "\n",
        "k = 6\n",
        "topk_values = np.partition(importancias_matriz.flatten(), -k)[-k:]\n",
        "umbral = np.min(topk_values)\n",
        "filtered_importancias = np.where(importancias_matriz>umbral,importancias_matriz,np.zeros_like(importancias_matriz))\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9, 3))\n",
        "\n",
        "# Mapa de importancia\n",
        "ax1.imshow(filtered_importancias, cmap='viridis', interpolation='nearest')\n",
        "ax1.set_title('Importancia de p√≠xeles')\n",
        "ax1.axis('off')\n",
        "\n",
        "# D√≠gito de ejemplo\n",
        "ax2.imshow(ejemplo, cmap='gray_r', interpolation='nearest')\n",
        "ax2.set_title(f'D√≠gito de ejemplo: {y_train[idx]}')\n",
        "ax2.axis('off')\n",
        "\n",
        "# Superposici√≥n\n",
        "ax3.imshow(filtered_importancias, cmap='plasma', alpha=0.75, interpolation='nearest')\n",
        "ax3.imshow(ejemplo, cmap='gray_r', alpha=0.65, interpolation='nearest')\n",
        "ax3.set_title('Superposici√≥n')\n",
        "ax3.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9A-I3mpe2Vfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusi√≥n\n",
        "\n",
        "Ventajas:\n",
        "\n",
        "* Es robusto ante la falta de normalizaci√≥n\n",
        "* Es robusto ante el overfitting\n",
        "* Maneja bien features categ√≥ricas y num√©ricas sin preprocesamiento complejo\n",
        "* Funciona bien con datasets grandes y alta dimensionalidad\n",
        "* Proporciona importancia de features incorporada\n",
        "* Maneja autom√°ticamente valores faltantes\n",
        "\n",
        "Desventajas:\n",
        "\n",
        "* Es lento (especialmente con muchos √°rboles o datos de alta dimensi√≥n)\n",
        "* Requiere m√°s memoria que modelos lineales\n",
        "* Menos interpretable que modelos simples\n",
        "* Puede ser demasiado conservativo con datos muy raros o novedosos\n",
        "* Dif√≠cil de implementar en sistemas en tiempo real estricto"
      ],
      "metadata": {
        "id": "zPQVLEFh5S0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificador 3: Regresi√≥n Log√≠stica\n",
        "\n",
        "\n",
        "Ahora exploremos el clasificador [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
      ],
      "metadata": {
        "id": "mlMM0aQY3e-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 1\n",
        "\n",
        "Una vez m√°s, en este ejemplo veremos el uso general y la forma de la frontera de decisi√≥n."
      ],
      "metadata": {
        "id": "zIWVV6vL56rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(X1[:,0],X1[:,1],c=y1)\n",
        "plt.axis('off')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jGozdUiU56rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos y evaluamos un modelo de Regresi√≥n Log√≠stica, dado que los datos son linealmente separables, esperamos obtener 100% en las m√©tricas"
      ],
      "metadata": {
        "id": "1O0Km_7o56rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=101) # 70% training and 30% test\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"F1 score: {round(f1_score(y_test,y_pred, average='macro'),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QW4NDeyJ56rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X1[:, 0], X1[:, 1], c=y1) # Hacemos el scatter con los puntos que queremos mostrar\n",
        "DecisionBoundaryDisplay.from_estimator(clf, X1,\n",
        "                                      ax=ax, alpha=0.5) # Mostramos la frontera de decisi√≥n encima\n",
        "ax.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UKhonJyi7C3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 2"
      ],
      "metadata": {
        "id": "DGEn7LhT7Lpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero, hagamos la clasificaci√≥n incial, con los hiperpar√°metros por defecto"
      ],
      "metadata": {
        "id": "s6O9hj587N5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"F1 score: {round(f1_score(y_test,y_pred, average='macro'),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jRsvG9iv7N5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En general, en la regresi√≥n log√≠stica, es importante el re-escalamiento"
      ],
      "metadata": {
        "id": "FeHemwbq84s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "pl = Pipeline([\n",
        "            # ('scl',StandardScaler()),\n",
        "            ('scl',MinMaxScaler()),\n",
        "            ('clf',LogisticRegression())])\n",
        "pl.fit(X_train,y_train)\n",
        "y_pred = pl.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"F1 score: {round(f1_score(y_test,y_pred, average='macro'),3)}\")\n",
        "\n",
        "# plt.figure(figsize=(3,3))\n",
        "# cm = confusion_matrix(y_test,y_pred)\n",
        "# s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "DG_QLavO7Tnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "pens = ['l1', 'l2', None]\n",
        "\n",
        "@interact(C=Cs, penalty=pens)\n",
        "def train_rf(penalty=None, C=1):\n",
        "    model = LogisticRegression(penalty=penalty,\n",
        "                               C=C,\n",
        "                               solver='saga')\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    plt.figure(figsize=(3,3))\n",
        "    cm = confusion_matrix(y_test,y_pred)\n",
        "    s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FmvXbeYX7cJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler  # ¬°Importante para LogReg!\n",
        "\n",
        "# Generar datos de ejemplo (si no los tienes ya)\n",
        "# X, y = ...\n",
        "\n",
        "# Estandarizar features (crucial para regularizaci√≥n)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Rango de valores de C (usamos escala logar√≠tmica)\n",
        "C_values = np.logspace(-3, 3, 20)  # De 0.001 a 1000\n",
        "\n",
        "accs_train = []\n",
        "accs_test = []\n",
        "\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(C=C, penalty='l2', solver='lbfgs', max_iter=1000, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    accs_train.append(model.score(X_train_scaled, y_train))\n",
        "    accs_test.append(model.score(X_test_scaled, y_test))\n",
        "\n",
        "# Gr√°fico\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.semilogx(C_values, accs_train, label='Entrenamiento', marker='o')\n",
        "plt.semilogx(C_values, accs_test, label='Prueba', marker='o')\n",
        "plt.xlabel('C')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Efecto de C en Regresi√≥n Log√≠stica')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ff7JfOLh8fLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "\n",
        "pl = Pipeline([\n",
        "            ('scl',StandardScaler()),\n",
        "            ('clf',LogisticRegression(C=0.001))])\n",
        "pl.fit(X_train,y_train)\n",
        "\n",
        "# ------ IMPORTANCIA DE LAS FEATURES ---------\n",
        "importancias = np.abs(clf.coef_[0])  # Tomamos los valores absolutos de los coeficientes\n",
        "# ---------------------------------------------\n",
        "print(f\"Importancia de las features (shape): {importancias.shape}\")\n",
        "importancias_matriz = importancias.reshape(8,8)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "im = plt.imshow(importancias_matriz, cmap='viridis', interpolation='nearest')\n",
        "plt.colorbar(im, label='Importancia del p√≠xel')\n",
        "plt.title('Importancia de los p√≠xeles seg√∫n Regresi√≥n Log√≠stica')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5UGtE8Kn9KeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusi√≥n\n",
        "\n",
        "Ventajas:\n",
        "\n",
        "- **Interpretabilidad**: Los coeficientes tienen una explicaci√≥n clara (cambio en el log-odds por unidad de cambio en el feature).\n",
        "- **Eficiencia computacional**: R√°pido en entrenamiento y predicci√≥n, incluso con datasets grandes.\n",
        "- **Probabilidades naturales**: Devuelve probabilidades calibradas (√∫til para decisiones con umbrales).\n",
        "- **Regularizaci√≥n incorporada**: Evita overfitting con L1/L2 (controlable con `C`).\n",
        "- **Funciona bien con datos linealmente separables**: √ìptimo cuando la relaci√≥n es aproximadamente lineal.\n",
        "\n",
        "Desventajas:\n",
        "\n",
        "- **Sensibilidad a multicolinealidad**: Los coeficientes se vuelven inestables o poco interpretables.\n",
        "- **No captura relaciones no lineales**: Requiere transformaciones manuales de features (ej.: polinomios).\n",
        "- **Supuesto linealidad**: Asume una relaci√≥n lineal entre features y log-odds (limitante en problemas complejos).\n",
        "- **Dependencia del escalado**: Requiere features estandarizadas para regularizaci√≥n efectiva.\n",
        "- **Problemas con clases desbalanceadas**: Tiende a favorecer la clase mayoritaria (necesita ajustes como `class_weight`)."
      ],
      "metadata": {
        "id": "F-msuCF69LTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificador 4: K-Vecinos m√°s cercanos\n",
        "\n",
        "[KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
      ],
      "metadata": {
        "id": "ufTcqnSF_eWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 1"
      ],
      "metadata": {
        "id": "8VWS2RlfALOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(X1[:,0],X1[:,1],c=y1)\n",
        "plt.axis('off')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ElC6bdFX9roI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=101) # 70% training and 30% test\n",
        "\n",
        "clf = KNeighborsClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"F1 score: {round(f1_score(y_test,y_pred, average='macro'),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MqKnhjbF_pgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X1[:, 0], X1[:, 1], c=y1) # Hacemos el scatter con los puntos que queremos mostrar\n",
        "DecisionBoundaryDisplay.from_estimator(clf, X1,\n",
        "                                      ax=ax, alpha=0.5) # Mostramos la frontera de decisi√≥n encima\n",
        "ax.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dbZOdlTi_ziU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 2"
      ],
      "metadata": {
        "id": "RNMqX-QuAHxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "clf = KNeighborsClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"F1 score: {round(f1_score(y_test,y_pred, average='macro'),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SxZlbBaK_z14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le afectan las escalas diferentes entre features, en este caso no tenemos ese fenomeno"
      ],
      "metadata": {
        "id": "gW79Rl5fAY-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "pl = Pipeline([\n",
        "            ('scl',StandardScaler()),\n",
        "            # ('scl',MinMaxScaler()),\n",
        "            ('clf',LogisticRegression())])\n",
        "pl.fit(X_train,y_train)\n",
        "y_pred = pl.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {round(accuracy_score(y_test,y_pred),3)}\")\n",
        "print(f\"Recall: {round(recall_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"Precision: {round(precision_score(y_test,y_pred, average='macro'),3)}\")\n",
        "print(f\"F1 score: {round(f1_score(y_test,y_pred, average='macro'),3)}\")\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "s_cm = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8uNX01lZAT_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Escalar los datos (importante para KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Configuraci√≥n de la figura\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# --- Gr√°fico 1: Accuracy vs n√∫mero de vecinos (k) ---\n",
        "k_values = list(range(1, 31))  # Probamos k de 1 a 30\n",
        "accs_train_k = []\n",
        "accs_test_k = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    accs_train_k.append(knn.score(X_train_scaled, y_train))\n",
        "    accs_test_k.append(knn.score(X_test_scaled, y_test))\n",
        "\n",
        "ax1.plot(k_values, accs_train_k, label='Entrenamiento', marker='o')\n",
        "ax1.plot(k_values, accs_test_k, label='Prueba', marker='o')\n",
        "ax1.set_xlabel('N√∫mero de vecinos (k)')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Accuracy vs. N√∫mero de Vecinos')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# --- Gr√°fico 2: Accuracy vs leaf_size ---\n",
        "leaf_sizes = list(range(2, 101, 5))  # De 5 a 100 en pasos de 5\n",
        "accs_train_leaf = []\n",
        "accs_test_leaf = []\n",
        "\n",
        "for leaf in leaf_sizes:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, leaf_size=leaf)  # k fijo=5 para este an√°lisis\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    accs_train_leaf.append(knn.score(X_train_scaled, y_train))\n",
        "    accs_test_leaf.append(knn.score(X_test_scaled, y_test))\n",
        "\n",
        "ax2.plot(leaf_sizes, accs_train_leaf, label='Entrenamiento', marker='o')\n",
        "ax2.plot(leaf_sizes, accs_test_leaf, label='Prueba', marker='o')\n",
        "ax2.set_xlabel('leaf_size')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Accuracy vs. Tama√±o de Hoja')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K_XVWv8gBJYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚ö† ¬°Ahora no tenemos importancia de features!**"
      ],
      "metadata": {
        "id": "NHpzdReuBS5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusi√≥n\n",
        "\n",
        "Ventajas:\n",
        "\n",
        "* Simple de entender\n",
        "* No requiere fase de entrenamiento (modelo \"perezoso\")\n",
        "* Adaptable f√°cilmente a nuevos datos\n",
        "* Funciona bien con datos no lineales\n",
        "* Pocos hiperpar√°metros a ajustar (principalmente k y m√©trica de distancia)\n",
        "\n",
        "Desventajas:\n",
        "\n",
        "* Sensible a la escala de los features (requiere normalizaci√≥n)\n",
        "* Costoso computacionalmente en predicci√≥n (especialmente con muchos datos)\n",
        "* Sensible a features irrelevantes o ruidosos\n",
        "* Dificultad para elegir k √≥ptimo\n",
        "* Problemas con datos de alta dimensionalidad (maldici√≥n de la dimensionalidad)\n",
        "* No proporciona importancia de features intr√≠nseca"
      ],
      "metadata": {
        "id": "blbZjZSB_90G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "IPython.notebook.save_notebook()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ObH5vvHpAGr1",
        "outputId": "7acbc7e9-c289-46ab-a05d-2292d9e39ac4"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "IPython.notebook.save_notebook()\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
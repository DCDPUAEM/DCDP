{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5y98twIUC-T"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP_2022/blob/main/03%20Machine%20Learning/notebooks/13-Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdQaBQt7UC-X"
      },
      "source": [
        "<h1>Clustering</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpuaRVvnUC-c"
      },
      "source": [
        "* El an√°lisis de agrupamiento, o agrupamiento, es una tarea de aprendizaje autom√°tico no supervisada.\n",
        "\n",
        "* Implica descubrir autom√°ticamente la agrupaci√≥n natural de los datos. A diferencia del aprendizaje supervisado (como el modelado predictivo), los algoritmos de agrupaci√≥n solo interpretan los datos de entrada y encuentran grupos o agrupaciones naturales en el espacio de caracter√≠sticas.\n",
        "\n",
        "* Un grupo, o cluster, es un en el espacio de caracter√≠sticas donde las instancias est√°n m√°s cerca del grupo que de otros grupos.\n",
        "\n",
        "* Es probable que estos grupos reflejen alg√∫n mecanismo en funcionamiento en el dominio del que se extraen las instancias, un mecanismo que hace que algunas instancias tengan un parecido m√°s fuerte entre s√≠ que con las instancias restantes.\n",
        "\n",
        "* La agrupaci√≥n en cl√∫sters puede ser √∫til como actividad de an√°lisis de datos para obtener m√°s informaci√≥n sobre el dominio del problema, el llamado descubrimiento de patrones o descubrimiento de conocimiento.\n",
        "\n",
        "* El agrupamiento tambi√©n puede ser √∫til como un tipo de ingenier√≠a de caracter√≠sticas, donde los ejemplos existentes y nuevos se pueden mapear y etiquetar como pertenecientes a uno de los grupos identificados en los datos.\n",
        "\n",
        "* La evaluaci√≥n de los grupos identificados es subjetiva y puede requerir un experto en el dominio, aunque existen muchas medidas cuantitativas espec√≠ficas de los grupos.\n",
        "\n",
        "&#128214; <u>Referencias bibliogr√°ficas</u>:\n",
        "* Flach, Peter (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of Data. Cambridge University Press.\n",
        "\n",
        "[Algoritmos de clustering en scikit-learn](https://scikit-learn.org/stable/modules/clustering.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy_dshVLUC-g"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recuerda la simbolog√≠a de las secciones:\n",
        "\n",
        "* üîΩ Esta secci√≥n no forma parte del proceso usual de Machine Learning. Es una exploraci√≥n did√°ctica de alg√∫n aspecto del funcionamiento del algoritmo.\n",
        "* ‚ö° Esta secci√≥n incluye t√©cnicas m√°s avanzadas destinadas a optimizar o profundizar en el uso de los algoritmos.\n",
        "* ‚≠ï Esta secci√≥n contiene un ejercicio o pr√°ctica a realizar. A√∫n si no se establece una fecha de entrega, es muy recomendable realizarla para practicar conceptos clave de cada tema."
      ],
      "metadata": {
        "id": "Aow9Mtnl20yW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrhsqwc9UC-j"
      },
      "source": [
        "# Demostraci√≥n ilustrativa de algunos algoritmos de agrupamiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN"
      ],
      "metadata": {
        "id": "bMNIcQ3lhySs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets artificiales"
      ],
      "metadata": {
        "id": "KaAMr5amOxB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Dataset 1: Clusters esf√©ricos\n",
        "X1, y1 = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.scatter(X1[:, 0], X1[:, 1], c=y1, cmap='viridis', s=10)\n",
        "plt.title(\"Clusters esf√©ricos\", fontsize=12)\n",
        "plt.axis('off')\n",
        "\n",
        "# Dataset 2: Medialunas\n",
        "X2, y2 = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(X2[:, 0], X2[:, 1], c=y2, cmap='viridis', s=10)\n",
        "plt.title(\"Clusters no convexos\", fontsize=12)\n",
        "plt.axis('off')\n",
        "\n",
        "# Dataset 3: Densidad variable + ruido\n",
        "X3_1, _ = make_blobs(n_samples=100, centers=1, cluster_std=0.3, random_state=42)\n",
        "X3_2, _ = make_blobs(n_samples=200, centers=1, cluster_std=1.5, random_state=42)\n",
        "noise = np.random.uniform(low=-6, high=6, size=(20, 2))\n",
        "X3 = np.vstack([X3_1, X3_2, noise])\n",
        "y3 = np.concatenate([np.zeros(100), np.ones(200), np.full(20, -1)])  # -1 = ruido\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.scatter(X3[:, 0], X3[:, 1], c=y3, cmap='viridis', s=10)\n",
        "plt.title(\"Densidad variable + ruido\", fontsize=12)\n",
        "plt.axis('off')\n",
        "\n",
        "# Dataset 4: Clusters anisotr√≥picos\n",
        "X4, y4 = make_blobs(n_samples=300, random_state=42)\n",
        "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "X4 = np.dot(X4, transformation)\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.scatter(X4[:, 0], X4[:, 1], c=y4, cmap='viridis', s=10)\n",
        "plt.title(\"Clusters anisotr√≥picos\", fontsize=12)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ontbd7-5Oz_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5cjPkORUC-k"
      },
      "source": [
        "Generaci√≥n de datos sint√©ticos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIrvqFYVUC-k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# X, y = make_blobs(n_samples=500,centers=3, random_state=24)\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
        "axs[0].scatter(X[:, 0], X[:, 1],c='black')\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "axs[0].set_title(\"Datos para clustering\")\n",
        "axs[1].scatter(X[:, 0], X[:, 1],c=y)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "axs[1].set_title(\"Datos etiquetados\")\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferencia del aprendizaje supervisado, en los m√©todos de clustering (implementados en scikit-learn) el proceso es de la siguiente manera:\n",
        "\n",
        "1. Inicializar el objeto, por ejemplo `modelo = KMeans(n_clusters=3)`.\n",
        "2. Hacer fit: `modelo.fit(X)`. Hay dos opciones:\n",
        "    * Obtener la lista de etiquetas de clusters como `y_clusters = modelo.predict(X)`.\n",
        "    * Obtener la lista de etiquetas de clusters usando el atributo `labels_` como `y_clusters = modelo.labels_`.\n",
        "\n",
        "Tambi√©n pueden obtenerse las etiquetas de los clusters directamente con el m√©todo `fit_transform()`.\n",
        "\n",
        "En esta notebook estaremos usando indistintamente los 3 m√©todos para ejemplificar su uso."
      ],
      "metadata": {
        "id": "-HKVaJOGLe8e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XewRUibwUC-u"
      },
      "source": [
        "## [K-MEANS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definamos el dataset sint√©tico a usar"
      ],
      "metadata": {
        "id": "aY_L3Z5QQFuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X1.copy()\n",
        "y = y1.copy()"
      ],
      "metadata": {
        "id": "bRSzUrfDQFhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probemos varios valores de `n_clusters`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s_I4uuysgJbU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diS9kzsmUC-u"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Definir el n√∫mero de clusters a buscar:\n",
        "num_clusters = 4\n",
        "\n",
        "modelo = KMeans(n_clusters=num_clusters)\n",
        "modelo.fit(X)\n",
        "y_clusters = modelo.predict(X)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0], X[:,1], c=y_clusters)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos la tarea de clustering usando la m√©trica de [score de silueta](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html):"
      ],
      "metadata": {
        "id": "hdI7D6OnzY0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(silhouette_score(X, y_clusters))"
      ],
      "metadata": {
        "id": "OzgZ1rbSzYfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos la tarea de clustering usando la m√©trica de [adjusted mutual information (AMI)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html).\n",
        "\n",
        "‚ùó Recuerda que, para esta m√©trica, necesitamos dos clusterings que comparar. Idealmente, uno es el *ground-truth clustering*. Para esto usemos las etiquetas `y`."
      ],
      "metadata": {
        "id": "Gq3rmqrG16JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_mutual_info_score\n",
        "\n",
        "print(adjusted_mutual_info_score(y, y_clusters))"
      ],
      "metadata": {
        "id": "G0ni5_5g2Hgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estamos comparando estos clusterings:"
      ],
      "metadata": {
        "id": "gjE9jegq2Xt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Estamos comparando estos dos clusterings\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
        "axs[0].scatter(X[:, 0], X[:, 1],c=y)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "axs[0].set_title(\"Ground truth\")\n",
        "axs[1].scatter(X[:, 0], X[:, 1],c=y_clusters)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "axs[1].set_title(\"K-means\")\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Tr__5Fob2ZzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîΩ Veamos los centroides de cada cluster"
      ],
      "metadata": {
        "id": "PyyDVjX57DQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centers = modelo.cluster_centers_\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1],c=y_clusters)\n",
        "plt.scatter(centers[:,0],centers[:,1],color='black', marker='x',s=80)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2N1VQf9z6ZUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° ¬øC√≥mo sabemos cu√°ntos clusters buscar?\n",
        "\n",
        "Cuando los m√©todos de clustering requieren, como hiperpar√°metro, el n√∫mero de clusters a encontrar, podemos realizar cualquiera de los siguientes an√°lisis."
      ],
      "metadata": {
        "id": "NMispnH_B-24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Elbow value\n",
        "\n",
        "‚ùó S√≥lo es para K-Means"
      ],
      "metadata": {
        "id": "olDjYLtJ07XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_num_clusters = 20\n",
        "\n",
        "inertias = []\n",
        "k_values = list(range(1,max_num_clusters))\n",
        "for k in k_values:\n",
        "    modelo = KMeans(n_clusters=k, n_init='auto')\n",
        "    modelo.fit(X)\n",
        "    inertias.append(modelo.inertia_)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,inertias,color='red')\n",
        "plt.axvline(x=3,linestyle='dashed',color='gray')\n",
        "plt.ylabel(\"Inertia\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.xticks(k_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ugk8MAJT7IuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### An√°lisis de silueta\n",
        "\n",
        "**Funciona para cualquier m√©todo**\n",
        "\n",
        "Tambi√©n podemos usar el **score de silueta**, el cual es un valor $-1\\leq s\\leq 1$ que mide que tan coherente son los puntos dentro de sus propios clusters, en t√©rminos de las distancias a los dem√°s clusters. Entre m√°s alto el valor, la configuraci√≥n del cl√∫ster es apropiada. **Este score es intr√≠nseco del clustering**.\n",
        "\n",
        "Usaremos la implementaci√≥n de [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)."
      ],
      "metadata": {
        "id": "XIArcI2_ECzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "best_num_clusters = 4\n",
        "\n",
        "siluetas = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto').fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    siluetas.append(silhouette_score(X, labels, metric='euclidean'))\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,siluetas,color='red')\n",
        "plt.axvline(x=best_num_clusters,linestyle='dashed',color='gray')\n",
        "plt.xticks(k_values)\n",
        "plt.ylabel(\"Silhoutte Scores\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A7mTsL6mDQXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### An√°isis de Adjusted Mutual Information (AMI)\n",
        "\n",
        "Buscamos el clustering con el mayor valor de AMI.\n",
        "\n",
        "Es un score que asigna una puntuaci√≥n a la comparaci√≥n entre dos clusterings. La Informaci√≥n Mutua Ajustada mide que tanta informaci√≥n comparten dos clusterings en t√©rminos de los elementos que comparten, es decir, del tama√±o de la intersecci√≥n.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2A2Vmdrozb63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_mutual_info_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "best_num_clusters = 4\n",
        "\n",
        "scores = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto')\n",
        "    kmeans.fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    scores.append(adjusted_mutual_info_score(y, labels))\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,scores,color='red')\n",
        "plt.axvline(x=best_num_clusters,linestyle='dashed',color='gray')\n",
        "plt.xticks(k_values)\n",
        "plt.ylabel(\"Adjusted Mutual Information\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dfYSiusEy5_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, en los tres casos se valida la hip√≥tesis de que el mejor valor para $K$ es $K=3$. Esta hip√≥tesis tiene mayor peso por el conocimiento previo del problema, es decir, al generar los datos sabiamos que teniamos 3 grupos de puntos."
      ],
      "metadata": {
        "id": "AYVmcBtfEq1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîΩ K-Means es sensible a outliers"
      ],
      "metadata": {
        "id": "9RH5O2sb2awy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Efecto de outliers en K-Means\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "# Generamos datos normales (3 clusters compactos)\n",
        "np.random.seed(42)\n",
        "X_inicial = np.vstack([\n",
        "    np.random.normal(loc=[5, 0], scale=0.5, size=(100, 2)),  # Cluster 1\n",
        "    np.random.normal(loc=[7, 4], scale=0.5, size=(100, 2)),  # Cluster 2\n",
        "    np.random.normal(loc=[8, 0], scale=0.5, size=(95, 2))  # Cluster 3\n",
        "])\n",
        "\n",
        "# A√±adimos outliers\n",
        "outliers = np.array([ [25, -10], [20,-15],[22,-12],[23,-14],[24,-16],[24.5,-15.5]])\n",
        "X_with_outliers = np.vstack([X_inicial, outliers])\n",
        "\n",
        "# Aplicamos K-means a ambos conjuntos\n",
        "kmeans_normal = KMeans(n_clusters=3, random_state=42).fit(X_inicial)\n",
        "kmeans_outliers = KMeans(n_clusters=3, random_state=42).fit(X_with_outliers)\n",
        "\n",
        "# --- Paso clave: Reordenar etiquetas para que coincidan los colores ---\n",
        "distance_matrix = cdist(kmeans_normal.cluster_centers_, kmeans_outliers.cluster_centers_)\n",
        "row_ind, col_ind = linear_sum_assignment(distance_matrix)\n",
        "\n",
        "# Mapeamos las etiquetas del modelo con outliers para que coincidan con las originales\n",
        "labels_outliers_reordered = np.zeros_like(kmeans_outliers.labels_)\n",
        "for original_label, outlier_label in zip(row_ind, col_ind):\n",
        "    labels_outliers_reordered[kmeans_outliers.labels_ == outlier_label] = original_label\n",
        "\n",
        "# Graficamos\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Sin outliers\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_inicial[:, 0], X_inicial[:, 1], c=kmeans_normal.labels_,\n",
        "            cmap='viridis',\n",
        "            alpha=0.6)\n",
        "plt.scatter(\n",
        "    kmeans_normal.cluster_centers_[:, 0],\n",
        "    kmeans_normal.cluster_centers_[:, 1],\n",
        "    c='red', marker='X', s=50, label='Centroides'\n",
        ")\n",
        "plt.title(\"K-means SIN outliers\")\n",
        "plt.xlim(-2, 30)\n",
        "plt.ylim(-20, 10)\n",
        "plt.legend()\n",
        "\n",
        "# Con outliers\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_with_outliers[:, 0], X_with_outliers[:, 1],\n",
        "            c=labels_outliers_reordered,\n",
        "            cmap='viridis',\n",
        "            alpha=0.6)\n",
        "plt.scatter(\n",
        "    kmeans_outliers.cluster_centers_[:, 0],\n",
        "    kmeans_outliers.cluster_centers_[:, 1],\n",
        "    c='red', marker='X', s=50, label='Centroides'\n",
        ")\n",
        "plt.title(\"K-means CON outliers\")\n",
        "plt.xlim(-2, 30)\n",
        "plt.ylim(-20, 10)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w1-hva7K2FYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT3pieSEUC-p"
      },
      "source": [
        "## [Clustering Jer√°rquico](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
        "\n",
        "* La agrupaci√≥n jer√°rquica es una familia general de algoritmos de agrupaci√≥n que crean agrupaciones anidadas fusion√°ndolas o dividi√©ndolas sucesivamente. Esta jerarqu√≠a de grupos se representa como un √°rbol (o dendrograma). La ra√≠z del √°rbol es el grupo √∫nico que re√∫ne todas las muestras, siendo las hojas los grupos con una sola muestra.\n",
        "* La implementaci√≥n [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) de scikit-learn realiza una agrupaci√≥n jer√°rquica utilizando un enfoque ascendente (bottom-up): cada observaci√≥n comienza en su propio grupo, y los grupos se fusionan sucesivamente. Los criterios de vinculaci√≥n (_linkage_) determinan la m√©trica utilizada para la estrategia de fusi√≥n:\n",
        "  - _Ward_ minimiza la suma de las diferencias al cuadrado dentro de todos los grupos. Es un enfoque que minimiza la varianza y, en este sentido, es similar a la funci√≥n objetivo de k-means pero se aborda con un enfoque jer√°rquico aglomerativo.\n",
        "  - _Maximum_ o _complete Linkage_ minimiza la distancia m√°xima entre observaciones de pares de grupos.\n",
        "  - _Average linkage_ minimiza el promedio de las distancias entre todas las observaciones de pares de grupos.\n",
        "  - _Single linkage_ minimiza la distancia entre las observaciones m√°s cercanas de pares de grupos.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Se puede especificar, ya sea el n√∫mero de clusters o el umbral de distancia m√°xima:\n",
        "\n",
        "* Si `n_clusters`$\\geq2$ entonces nos regresa ese n√∫mero de clusters.\n",
        "* Si `n_clusters`=None, hay que especificar un `distance_threshold`.\n",
        "* Si `distance_threshold`$\\neq$None, `n_clusters` debe ser None y `compute_full_tree` debe ser True.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escoger el dataset"
      ],
      "metadata": {
        "id": "3UPl6ZB7Q4Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X1.copy()\n",
        "y = y1.copy()"
      ],
      "metadata": {
        "id": "F4siQodDQ4GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIZ_4p5HUC-q"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "modelo = AgglomerativeClustering(distance_threshold=5.0,\n",
        "                                 n_clusters=None,\n",
        "                                 compute_full_tree=True)\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "num_clusters_encontrados = np.unique(yhat).shape[0]\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:, 0], X[:, 1], c=yhat)\n",
        "plt.title(f\"{num_clusters_encontrados} clusters encontrados.\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambi√©n podemos usar criterios externos para escoger un n√∫mero de clusters adecuado. Por ejemplo, podemos usar el score de silueta. La conclusi√≥n es usar `__` clusters."
      ],
      "metadata": {
        "id": "6SeUcJL4H6VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "num_optimo_clusters = 4\n",
        "\n",
        "siluetas = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    ac = AgglomerativeClustering(n_clusters=k)\n",
        "    ac.fit(X)\n",
        "    labels = ac.labels_\n",
        "    siluetas.append(silhouette_score(X, labels, metric='euclidean'))\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,siluetas,color='red')\n",
        "plt.axvline(x=num_optimo_clusters,linestyle='dashed',color='black')\n",
        "plt.xticks(k_values)\n",
        "plt.ylabel(\"Silhoutte Scores\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "okjT7LBkHfle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Graficar dendograma\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
        "                                      counts]).astype(float)\n",
        "\n",
        "    # Calculate the number of clusters at each distance level\n",
        "    unique_distances = np.unique(linkage_matrix[:, 2])\n",
        "    cluster_counts = []\n",
        "    for d in unique_distances:\n",
        "        cluster_counts.append(np.sum(linkage_matrix[:, 2] >= d) + 1)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendro = dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "    # Modify y-axis to show number of clusters\n",
        "    ax = plt.gca()\n",
        "    yticks = ax.get_yticks()\n",
        "    new_labels = []\n",
        "    for y in yticks:\n",
        "        # Find how many clusters exist at this distance level\n",
        "        n_clusters = np.sum(linkage_matrix[:, 2] >= y) + 1\n",
        "        new_labels.append(f\"{y:.2f}\\n({n_clusters} clusters)\")\n",
        "    ax.set_yticklabels(new_labels)\n",
        "\n",
        "    return dendro\n",
        "\n",
        "\n",
        "plt.figure(dpi=120)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plot_dendrogram(modelo, truncate_mode='level', p=3)\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.ylabel(\"Distance (Number of clusters)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ImqNm7LnMlVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JogoUSPjUC-t"
      },
      "source": [
        "## [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
        "* [Martin Ester, Hans-Peter Kriegel, J√∂rg Sander, Xiaowei Xu (1996). _A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise_. Proceedings of Knowledge Discovery and Databases - The International Conference on Knowledge Discovery & Data Mining.](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)\n",
        "* DBSCAN recurre a una noci√≥n de c√∫mulos basada en la densidad de los mismos, que est√° dise√±ada para descubrir grupos de formas arbitrarias. DBSCAN requiere solo un par√°metro de entrada $\\varepsilon$, el cual determina la distancia m√°xima entre dos puntos para considerarse cercanos. El otro par√°metro importante es el `min_samples` el cual representa el n√∫mero m√≠nimo de puntos que puede haber en un cluster.\n",
        "\n",
        "* DBSCAN reconoce ruido. Es decir, puede dejar puntos sin asignar a ning√∫n cluster.\n",
        "\n",
        "* \"_La raz√≥n principal por la que reconocemos los grupos, es que dentro de cada grupo tenemos una densidad t√≠pica de puntos que es considerablemente m√°s alta que fuera del grupo. Adem√°s, la densidad dentro de las √°reas de ruido es menor que la densidad en cualquiera de los grupos._\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escoger dataset"
      ],
      "metadata": {
        "id": "dPLW1v1uRbjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X1.copy()\n",
        "y = y1.copy()"
      ],
      "metadata": {
        "id": "PLPI_dwrRbZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1LX9VnhUC-t"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "modelo = DBSCAN(eps=0.530, min_samples=9)\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = [j for j in np.unique(yhat) if j!=-1]    # Todos los clusters que no son ruido\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[yhat==-1,0],X[yhat==-1,1],marker='x',color='gray',label='ruido')\n",
        "for cluster in clusters:\n",
        "    filas = np.where(yhat == cluster)\n",
        "    plt.scatter(X[filas, 0], X[filas, 1])\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬øC√≥mo denota DBSCAN a los puntos que no pertenecen a ning√∫n cluster (ruido)?"
      ],
      "metadata": {
        "id": "KpTEpVasivPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(yhat)"
      ],
      "metadata": {
        "id": "ps_FP5NHivAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una particularidad de DBSCAN es la susceptibilidad de los resultados en funci√≥n del principal par√°metro $\\varepsilon$."
      ],
      "metadata": {
        "id": "Ldbjw07gv4sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valores_eps = np.linspace(0.0001,10,100)\n",
        "num_clusters = []\n",
        "\n",
        "for eps in valores_eps:\n",
        "    modelo = DBSCAN(eps=eps, min_samples=5)\n",
        "    yhat = modelo.fit_predict(X)\n",
        "    clusters = np.unique(yhat)\n",
        "    num_clusters.append(len(clusters))\n",
        "\n",
        "plt.plot(valores_eps,num_clusters)\n",
        "plt.suptitle(\"N√∫mero de clusters en funci√≥n del par√°metro eps\")\n",
        "plt.xlabel(\"eps\",fontsize=15)\n",
        "plt.ylabel(\"N√∫mero de clusters\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "etCJvkj_Rcc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîΩ Comparaci√≥n de K-Means, AgglomerativeClustering y DBSCAN en datasets grandes"
      ],
      "metadata": {
        "id": "OgQLbw2a4cp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=15000,centers=3, random_state=174)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1],c='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dJOUl_US4kOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "\n",
        "modelos = [KMeans(n_clusters=3),AgglomerativeClustering(n_clusters=3),DBSCAN()]\n",
        "\n",
        "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
        "\n",
        "for modelo,ax in zip(modelos,axs):\n",
        "    inicio = time.time()\n",
        "    modelo.fit(X)\n",
        "    final = time.time()\n",
        "    print(f\"{modelo.__class__.__name__}\\n\\tTiempo de ejecuci√≥n: {round(final-inicio,2)} segundos\")\n",
        "    clusters = modelo.labels_\n",
        "    ax.scatter(X[:,0],X[:,1],c=clusters)\n",
        "    ax.set_title(f\"{modelo.__class__.__name__}\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "AIMIZCoziRUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones**\n",
        "\n",
        "* KMeans es bueno con datasets grandes.\n",
        "* AgglomerativeClustering tarda con datasets grandes.\n",
        "* DBSCAN es el m√°s flexible."
      ],
      "metadata": {
        "id": "vfQcOuopkXq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Comparando todos los datasets y m√©todos\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "\n",
        "\n",
        "# Configuraci√≥n de clustering\n",
        "def apply_clustering(X):\n",
        "    # K-Means\n",
        "    kmeans = KMeans(n_clusters=2, random_state=42).fit(X)\n",
        "\n",
        "    # Agglomerative (Ward linkage)\n",
        "    agg = AgglomerativeClustering(n_clusters=2).fit(X)\n",
        "\n",
        "    # DBSCAN (par√°metros ajustables)\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X)\n",
        "\n",
        "    return [kmeans.labels_, agg.labels_, dbscan.labels_]\n",
        "\n",
        "\n",
        "datasets = [(X1, \"Clusters esf√©ricos\"),\n",
        "            (X2, \"Medialunas\"),\n",
        "            (X3, \"Densidad variable + ruido\"),\n",
        "            (X4, \"Clusters anisotr√≥picos\")]\n",
        "methods = [\"K-Means\", \"Agglomerative\", \"DBSCAN\"]\n",
        "\n",
        "plt.figure(figsize=(10, 13))\n",
        "\n",
        "for i, (X, title) in enumerate(datasets):\n",
        "    clusters = apply_clustering(X)\n",
        "\n",
        "    for j, (labels, method) in enumerate(zip(clusters, methods)):\n",
        "        plt.subplot(4, 3, i*3 + j + 1)\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=10)\n",
        "\n",
        "        # Solo mostrar t√≠tulo en la primera fila\n",
        "        if i == 0:\n",
        "            plt.title(method, fontsize=12)\n",
        "\n",
        "        # Solo mostrar etiqueta en la primera columna\n",
        "        if j == 0:\n",
        "            plt.ylabel(title, fontsize=12)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qEKfG96mZOnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Otros m√©todos"
      ],
      "metadata": {
        "id": "68s4pOjOw8mF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PeZ1RewUC-m"
      },
      "source": [
        "### Affinity Propagation\n",
        "\n",
        "[`Affinity Propagation`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html) es un algoritmo de clustering que agrupa datos mediante el intercambio de mensajes entre puntos, representando su idoneidad para ser ejemplares (centros de clusters). A diferencia de otros m√©todos, determina autom√°ticamente el n√∫mero de clusters en funci√≥n de dos par√°metros clave:\n",
        "\n",
        "* `preference`: Controla cu√°ntos puntos se eligen como ejemplares (centros de clusters). Valores m√°s altos generan m√°s clusters.\n",
        "* `damping` (factor de amortiguaci√≥n, 0.5 a 1): Estabiliza las iteraciones evitando oscilaciones num√©ricas\n",
        "\n",
        "*Frey, B. J., & Dueck, D. (2007). Clustering by passing messages between data points. Science, 315(5814), 972-976*. [Fuente](https://www.science.org/doi/10.1126/science.1136800)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escogemos un dataset"
      ],
      "metadata": {
        "id": "qDm-J1IHcRpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X3.copy()\n",
        "y = y3.copy()"
      ],
      "metadata": {
        "id": "PPOwo7EEcQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEEowNZbUC-o"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "modelo = AffinityPropagation(damping=0.5,max_iter=500,random_state=None)\n",
        "\n",
        "modelo.fit(X)\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "print(f\"Se encontraron {clusters.shape[0]} clusters\")\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ywQ1YzUC-r"
      },
      "source": [
        "### BIRCH\n",
        "\n",
        "[`BIRCH`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html) es un algoritmo de clustering jer√°rquico bottom-up eficiente para grandes datasets, dise√±ado para agrupar datos m√©tricos multidimensionales de forma incremental, optimizando el uso de memoria y tiempo. Utiliza Clustering Features (CF) ‚Äîtripletas que resumen la informaci√≥n de los clusters‚Äî para construir iterativamente un √°rbol con dos par√°metros clave:\n",
        "\n",
        "* `branching_factor`: Controla el n√∫mero m√°ximo de subclusters por nodo\n",
        "* `threshold`: Determina el radio m√°ximo de los subclusters (controla la compactaci√≥n)\n",
        "\n",
        "Fue el primer m√©todo en manejar efectivamente el ruido en datasets.\n",
        "\n",
        "*Tian Zhang, Raghu Ramakrishnan and Miron Livny (1996). _BIRCH: An Efficient Data Clustering Method for Very Large Databases_. ACM SIGMOD Record.* [DOI:10.1145/235968.233324](https://dl.acm.org/doi/10.1145/235968.233324).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onKzt5_aUC-s"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import Birch\n",
        "\n",
        "modelo = Birch(threshold=0.01, n_clusters=3)\n",
        "\n",
        "modelo.fit(X)\n",
        "yhat = modelo.predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OPTICS\n",
        "\n",
        "[`OPTICS`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) es un algoritmo de clustering relacionado con DBSCAN que identifica puntos *n√∫cleo* y expande clusters a partir de ellos. A diferencia de DBSCAN, mantiene una jerarqu√≠a de clusters para diferentes valores de radio, siendo una alternativa m√°s eficiente para datasets grandes. Sus par√°metros clave son:\n",
        "\n",
        "* `min_samples`: Controla el n√∫mero m√≠nimo de puntos para considerar una regi√≥n densa\n",
        "* `max_eps`: Establece el radio m√°ximo de b√∫squeda de vecindarios\n",
        "\n",
        "*Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, J√∂rg Sander (1999). OPTICS: Ordering Points To Identify the Clustering Structure. ACM SIGMOD Record.* [DOI:10.1145/304181.304187](https://dl.acm.org/doi/10.1145/304181.304187)"
      ],
      "metadata": {
        "id": "Yk-Fo_ZkRGLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "modelo = OPTICS(max_eps=200, min_samples=9)\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    filas = np.where(yhat == cluster)\n",
        "    plt.scatter(X[filas, 0], X[filas, 1])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bDeYrKNhRFbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = []\n",
        "valores_eps = np.linspace(0.0001,200,100)\n",
        "\n",
        "for eps in valores_eps:\n",
        "    modelo = OPTICS(eps=eps, min_samples=5)\n",
        "    yhat = modelo.fit_predict(X)\n",
        "    clusters = np.unique(yhat)\n",
        "    num_clusters.append(len(clusters))\n",
        "\n",
        "plt.plot(valores_eps,num_clusters)\n",
        "plt.suptitle(\"N√∫mero de clusters en funci√≥n del par√°metro eps\")\n",
        "plt.xlabel(\"eps\",fontsize=15)\n",
        "plt.ylabel(\"N√∫mero de clusters\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e0BzdpuQSVXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY5OUxZbUC-u"
      },
      "source": [
        "### Spectral Clustering\n",
        "\n",
        "[`SpectralClustering`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) es un m√©todo avanzado de clustering basado en t√©cnicas de √°lgebra lineal que utiliza los vectores propios de una matriz de similitud para identificar estructuras en los datos. Su principal hiperpar√°metro es:\n",
        "\n",
        "* `n_clusters`: Define el n√∫mero estimado de clusters a identificar\n",
        "\n",
        "*Referencias:*\n",
        "1. *Ng, A. Y., Jordan, M. I., & Weiss, Y. (2002). On Spectral Clustering: Analysis and an algorithm. NIPS.* [PDF](https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf)\n",
        "2. *Von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and Computing.* [DOI:10.1007/s11222-007-9033-z](https://link.springer.com/article/10.1007/s11222-007-9033-z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knNzMY8vUC-u"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "modelo = SpectralClustering(n_clusters=2)\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## M√°s m√©tricas"
      ],
      "metadata": {
        "id": "woMLwJCXiqFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay m√°s m√©tricas que pueden usarse para el clustering. Las hay de dos tipos:\n",
        "\n",
        "1. Cuando no se conoce un clustering *ground truth*, en este caso se evalua el m√≥delo unicamente con la informaci√≥n de √©l mismo. Por ejemplo:\n",
        "\n",
        "    * [Silhoutte score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\n",
        "    * [Calinski-Harabasz Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score)\n",
        "    * [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score)\n",
        "    * ...\n",
        "\n",
        "2. Cuando se conoce un clustering *ground truth* o se quieren comparar dos clusterings. Por ejemplo:\n",
        "    * [AMI](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score)\n",
        "    * [V-measure](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score)\n",
        "    * [Rand index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html#sklearn.metrics.rand_score)\n",
        "    * ...\n"
      ],
      "metadata": {
        "id": "TwZ6xR8Ditvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚≠ï Pr√°ctica"
      ],
      "metadata": {
        "id": "xd9VdumvETie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬øPuedes encontrar buenos clusterings para los siguientes datasets?\n",
        "\n",
        "Considera dos datasets DS1 y DS2/DS3. En cada uno de ellos, prueba los siguientes m√©todos:\n",
        "\n",
        "1. K-Means\n",
        "2. AgglomerativeClustering\n",
        "3. DBSCAN\n",
        "\n",
        "Tareas a realizar:\n",
        "\n",
        "* Realiza una busqueda de hiperpar√°metros bas√°ndote en alguna m√©trica como AMI, Silhoutte, Elbow Value (para el caso de K-Means) o alguna otra.\n",
        "* Con estos hiperpar√°metros, escoge el mejor clustering de cada uno de los tres m√©todos.\n",
        "* Compara visualmente los 3 clusterings obtenidos.\n",
        "* Usando el score de silueta y el √≠ndice Calinski-Harabasz, ¬øcu√°l de los tres clusterings fue mejor?\n",
        "* En el caso del dataset DS1, tienes un *ground truth* clustering. Reporta el valor de la m√©trica [AMI](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score) y [ARI](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score)."
      ],
      "metadata": {
        "id": "U4pvJ24QjBeT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqCnMpbRUC-z"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons, make_blobs\n",
        "import numpy as np\n",
        "\n",
        "n_samples = 500\n",
        "\n",
        "DS1 = make_moons(n_samples=n_samples, noise=.05)\n",
        "DS2 = np.random.rand(n_samples, 2)\n",
        "DS3 = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=170)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.scatter(DS1[0][:,0],DS1[0][:,1])\n",
        "plt.title(\"Dataset DS1\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,2)\n",
        "plt.scatter(DS2[:,0],DS2[:,1])\n",
        "plt.title(\"Dataset DS2\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,3)\n",
        "plt.scatter(DS3[0][:,0],DS3[0][:,1])\n",
        "plt.title(\"Dataset DS3\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GK6IZ8wOntck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficar los clusters:"
      ],
      "metadata": {
        "id": "Su_f19cQnuMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zepYvT6PUC-z"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1,2,figsize=(9,5),sharey=True)\n",
        "axs[0].scatter(X[:,0],X[:,1], c=y)\n",
        "axs[0].set_title(\"Original dataset\")\n",
        "axs[1].scatter(X[:,0],X[:,1], c=y_clusters)\n",
        "axs[1].set_title(\"Clustering\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 1: Documentos de Wikipedia\n",
        "\n",
        "Usaremos otra vez el dataset de documentos de Wikipedia, tomaremos la versi√≥n parcial y preprocesada de la sesi√≥n pasada.\n",
        "\n",
        "El objetivo de la pr√°ctica es hacer **Topic Modelling**, es decir segmentar los documentos en grupos con tem√°ticas similares. Para esto, usaremos algoritmos de clustering aplicados a representaciones vectoriales de los documentos. Si las representaciones vectoriales de los documentos son *buenas*, lograremos este objetivo.\n",
        "\n",
        "Al final evaluaremos usamos m√©tricas de clustering y visualizando t√≥picos manualmente\n",
        "\n",
        "**El dataset no tiene una variable target** Estamos en aprendizaje no supervisado"
      ],
      "metadata": {
        "id": "baK_yZkgTibB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud -qq"
      ],
      "metadata": {
        "id": "vy_MugDLg2Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/03%20Machine%20Learning/data/spanish-wikipedia-dataframe.csv\"\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "df.drop(columns=['doc_id'],inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "ON51TIO0Tj6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construimos la matriz BOW de los documentos y hacemos PCA para obtener una matriz de caracteristicas (num√©ricas continuas).\n",
        "\n",
        "Con `max_features=10000` y todas las componentes principales tarda alrededor de 2 minutos"
      ],
      "metadata": {
        "id": "I6zM-rCnhQ3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "docs_list = df['Texto'].values\n",
        "\n",
        "# cv = CountVectorizer(max_features=10000)\n",
        "cv = TfidfVectorizer(max_features=10000)\n",
        "X_bow = cv.fit_transform(docs_list)\n",
        "print(X_bow.shape)\n",
        "\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(np.asarray(X_bow.todense()))\n",
        "print(X_pca.shape)"
      ],
      "metadata": {
        "id": "g1r0VLlGUjN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tomamos las primeras `n_dim` componentes principales como representaci√≥n vectorial de cada documento."
      ],
      "metadata": {
        "id": "65nedGJjhcnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_dim = 300\n",
        "# n_dim = X_pca.shape[1]\n",
        "\n",
        "X_pca_dim = X_pca[:,:n_dim]\n",
        "print(X_pca_dim.shape)"
      ],
      "metadata": {
        "id": "v1EwVtj_VY1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos clustering a las representaciones vectoriales."
      ],
      "metadata": {
        "id": "3Mtik-oKirjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# num_clusters = 13\n",
        "num_clusters = 6\n",
        "\n",
        "clustering = KMeans(n_clusters=num_clusters)\n",
        "clustering.fit(X_pca_dim)\n",
        "clusters = clustering.labels_"
      ],
      "metadata": {
        "id": "Pr19w-riUksN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idxs_per_cluster = {j:np.where(clusters==j)[0] for j in np.unique(clusters)}\n",
        "documents_per_cluster = {j:df.loc[idxs_per_cluster[j],'Texto'].values for j in np.unique(clusters)}\n",
        "\n",
        "# documents_per_cluster"
      ],
      "metadata": {
        "id": "clyEjbRtVprv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la finalidad de explorar el contenido de los textos de cada cluster, hacemos una nube de palabras de los documentos de cada cluster. Para esto, usamos el m√≥dulo [wordcloud](https://pypi.org/project/wordcloud/).\n",
        "\n",
        "Aqu√≠ puedes ver [ejemplos de su uso](https://github.com/amueller/word_cloud/tree/main)"
      ],
      "metadata": {
        "id": "_RQzX5gShqcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=num_clusters,figsize=(5*num_clusters,5),dpi=100)\n",
        "for k,ax in enumerate(axs):\n",
        "    wordcloud = WordCloud().generate(\" \".join(documents_per_cluster[k]))\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(f\"Cluster {k}\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "WCmQvHOdd2WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(f\"Score de silueta: {silhouette_score(X_pca_dim,clusters)}\")"
      ],
      "metadata": {
        "id": "bHXfJgDrjXg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualicemos el valor de codo"
      ],
      "metadata": {
        "id": "iDT-VcHloush"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_clusters = range(2,20)\n",
        "inercias = []\n",
        "\n",
        "for k in n_clusters:\n",
        "    clustering = KMeans(n_clusters=k)\n",
        "    clustering.fit(X_pca_dim)\n",
        "    inercias.append(clustering.inertia_)\n",
        "\n",
        "plt.plot(range(1,len(inercias)+1),inercias)\n",
        "plt.xlabel(\"N√∫mero de clusters\")\n",
        "plt.ylabel(\"Inercia\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3SY9R6ZxossC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprimamos algunos documentos de cada cluster:"
      ],
      "metadata": {
        "id": "A5E304ohm1EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for j,docs in enumerate(documents_per_cluster.values()):\n",
        "    print(f\"Cluster {j}:\")\n",
        "    for doc in docs[:5]:\n",
        "        print(f\"\\t{doc[:100]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "jxeMnQjAm06V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generar un HTML para visualizar los clusters\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "# Configuraci√≥n de colores por cluster\n",
        "colors = np.random.rand(len(np.unique(clusters)), 3) * 255\n",
        "colors = [f'rgb({r},{g},{b})' for r, g, b in colors]\n",
        "\n",
        "# Crear figura 3D\n",
        "fig = go.Figure()\n",
        "\n",
        "# A√±adir puntos para cada cluster\n",
        "for cluster_id in np.unique(clusters):\n",
        "    # Filtrar puntos del cluster actual\n",
        "    mask = clusters == cluster_id\n",
        "    x, y, z = X_pca_dim[mask, 0], X_pca_dim[mask, 1], X_pca_dim[mask, 2]\n",
        "\n",
        "    # Obtener fragmentos de texto (primeros 100 caracteres)\n",
        "    text_samples = [f\"<b>Cluster {cluster_id}</b><br>{doc[:100]}...\"\n",
        "                   for doc in df.loc[mask, 'Texto'].values]\n",
        "\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=x,\n",
        "        y=y,\n",
        "        z=z,\n",
        "        mode='markers',\n",
        "        name=f'Cluster {cluster_id}',\n",
        "        marker=dict(\n",
        "            size=5,\n",
        "            color=colors[cluster_id],\n",
        "            opacity=0.8\n",
        "        ),\n",
        "        text=text_samples,\n",
        "        hoverinfo='text'\n",
        "    ))\n",
        "\n",
        "# Configuraci√≥n del layout\n",
        "fig.update_layout(\n",
        "    title='PCA 3D de Documentos Clusterizados con K-Means',\n",
        "    scene=dict(\n",
        "        xaxis=dict(visible=False, showticklabels=False),\n",
        "        yaxis=dict(visible=False, showticklabels=False),\n",
        "        zaxis=dict(visible=False, showticklabels=False),\n",
        "        bgcolor='rgba(0,0,0,0)'\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=30),\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=12,\n",
        "        font_family=\"Arial\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Guardar como HTML\n",
        "fig.write_html(\"Wikipedia_tfidf_kmeans_6.html\")"
      ],
      "metadata": {
        "id": "ZAHTzsrSn5Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ Reflexiona sobre las siguientes preguntas:\n",
        "\n",
        "* ¬øPuedes ver de qu√© tratan los documentos de cada cluster?\n",
        "* Prueba a cambiar el n√∫mero de clusters.\n",
        "* Prueba a cambiar el n√∫mero de dimensiones de las representaciones vectoriales.\n",
        "* Prueba a cambiar el m√©todo de clustering\n",
        "* En cada tarea de clustering, mide el coeficiente de silueta."
      ],
      "metadata": {
        "id": "HtwoFKxDiL15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 2: Canciones de Spotify"
      ],
      "metadata": {
        "id": "NrEN69yjS9Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este conjunto de datos contiene estad√≠sticas de audio de las 2.000 canciones top de Spotify. Los datos contienen alrededor de 15 columnas que describen la canci√≥n y algunas de sus cualidades. Se incluyen canciones publicadas desde 1956 hasta 2019 de algunos artistas notables y famosos. Estos datos contienen caracter√≠sticas de audio como Danceability, BPM, Liveness, Valence(Positivity) y algunas m√°s:\n",
        "\n",
        "* √çndice: ID\n",
        "* T√≠tulo: Nombre de la pista\n",
        "* Artista: Nombre del artista\n",
        "* G√©nero superior: G√©nero de la pista\n",
        "* A√±o: A√±o de lanzamiento de la pista\n",
        "* Pulsaciones por minuto (BPM): El tempo de la canci√≥n\n",
        "* Energy: La energ√≠a de una canci√≥n: cuanto m√°s alto sea el valor, m√°s energ√©tica ser√° la canci√≥n.\n",
        "* Danceability: Cuanto m√°s alto sea el valor, m√°s f√°cil ser√° bailar esta canci√≥n.\n",
        "* Loudness: Cuanto m√°s alto sea el valor, m√°s fuerte ser√° la canci√≥n.\n",
        "* Liveness: ...\n",
        "* Valence: Cuanto m√°s alto sea el valor, m√°s positivo ser√° el estado de √°nimo de la canci√≥n.\n",
        "* Duraci√≥n: La duraci√≥n de la canci√≥n.\n",
        "* Acousticness: Cuanto m√°s alto sea el valor, m√°s ac√∫stica ser√° la canci√≥n.\n",
        "* Speechiness: Cuanto m√°s alto sea el valor, m√°s palabras habladas contiene la canci√≥n.\n",
        "* Popularity: Cuanto m√°s alto sea el valor, m√°s popular es la canci√≥n.\n",
        "\n",
        "Este dataset se encuentra en [Kaggle](https://www.kaggle.com/datasets/iamsumat/spotify-top-2000s-mega-dataset)\n",
        "\n",
        "Vamos a hacer clustering como estrategia para agrupar canciones por grupos similares con base en sus features num√©ricas.\n",
        "\n",
        "**El dataset no tiene una variable target** Estamos en aprendizaje no supervisado"
      ],
      "metadata": {
        "id": "YUSnLgDc6yxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'https://github.com/DCDPUAEM/DCDP/raw/main/03%20Machine%20Learning/data/spotify-2000.csv'\n",
        "df = pd.read_csv(url,index_col=0,thousands=',')\n",
        "df"
      ],
      "metadata": {
        "id": "2p5rJTnaEKAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hagamos un breve analisis exploratorio"
      ],
      "metadata": {
        "id": "yGU6ZIFh5OyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "AuqCCslwj11Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos los g√©neros"
      ],
      "metadata": {
        "id": "vEA4ixrd5tUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generos = df['Top Genre'].unique()\n",
        "print(f\"Hay {len(generos)} g√©neros √∫nicos:\")\n",
        "print(generos)"
      ],
      "metadata": {
        "id": "FUbx_iOg_c7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos los rangos de las variables"
      ],
      "metadata": {
        "id": "uLb7QDal5yO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "oRqQgbox70gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A manera de an√°lisis exploratorio, veamos las correlaciones entre variables, ¬øqu√© observamos?"
      ],
      "metadata": {
        "id": "U4-A0tlA9kRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from seaborn import heatmap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "correlaciones = df.iloc[:,3:].corr()\n",
        "heatmap(correlaciones)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5of0zRUs9I24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que algunos m√©todos de clustering son susceptibles a la escala de valores, hacemos un escalamiento de las variables num√©ricas.\n",
        "\n",
        "**Haremos clustering con s√≥lo estas variables num√©ricas**"
      ],
      "metadata": {
        "id": "_hud5RYP7zhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "df2 = df[[\"Beats Per Minute (BPM)\", \"Loudness (dB)\",\n",
        "              \"Liveness\", \"Valence\", \"Acousticness\",\n",
        "              \"Speechiness\"]].copy()\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df2[df2.columns] = scaler.fit_transform(df2[df2.columns])\n",
        "X = df2.values\n",
        "\n",
        "df2.head(3)"
      ],
      "metadata": {
        "id": "N_Tb3EgWTljS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.describe()"
      ],
      "metadata": {
        "id": "Cdq-yECD-rBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos K-means para segmentar en 10 grupos"
      ],
      "metadata": {
        "id": "cK5a1QqrB03t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "modelo = KMeans(n_clusters=10, n_init='auto')\n",
        "\n",
        "modelo.fit(X)\n",
        "clusters = modelo.labels_\n",
        "\n",
        "print(f\"Las primeras 10 etiquetas: {clusters[:10]}\")"
      ],
      "metadata": {
        "id": "BC2tD2N28yVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integramos la informaci√≥n de los clusters al dataframe original."
      ],
      "metadata": {
        "id": "QGUA-MzoCCDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Music Segments\"] = clusters\n",
        "df[\"Music Segments\"] = df[\"Music Segments\"].map({0: \"Cluster 1\", 1:\n",
        "    \"Cluster 2\", 2: \"Cluster 3\", 3: \"Cluster 4\", 4: \"Cluster 5\",\n",
        "    5: \"Cluster 6\", 6: \"Cluster 7\", 7: \"Cluster 8\",\n",
        "    8: \"Cluster 9\", 9: \"Cluster 10\"})\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "rhHEN2zCUo0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observemos un cluster"
      ],
      "metadata": {
        "id": "dn_xNOGSl4vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster = 'Cluster 3'\n",
        "\n",
        "df[df['Music Segments']==cluster][['Artist','Title','Top Genre','Year']]"
      ],
      "metadata": {
        "id": "MCCBqzzVl6-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver los artistas en este cluster"
      ],
      "metadata": {
        "id": "YueDpOXR59u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Music Segments']==cluster]['Artist'].unique()"
      ],
      "metadata": {
        "id": "zEI8jGX86AUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficamos usando solamente 3 features. Usamos el m√≥dulo [plotly](https://plotly.com/python/) para gr√°ficas interactivas.\n",
        "\n",
        "Otra alternativa es [Bokeh](https://bokeh.org/)."
      ],
      "metadata": {
        "id": "wcoMmKJDCHfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "PLOT = go.Figure()\n",
        "\n",
        "for i in list(df[\"Music Segments\"].unique()):\n",
        "    PLOT.add_trace(go.Scatter3d(x = df[df[\"Music Segments\"]==i]['Beats Per Minute (BPM)'],\n",
        "                                    y = df[df[\"Music Segments\"] ==i]['Energy'],\n",
        "                                    z = df[df[\"Music Segments\"] ==i]['Danceability'],\n",
        "                                    mode = 'markers',marker_size = 6, marker_line_width = 1,\n",
        "                                    name = str(i)))\n",
        "PLOT.update_traces(hovertemplate='Beats Per Minute (BPM): %{x} <br>Energy: %{y} <br>Danceability: %{z}')\n",
        "\n",
        "PLOT.update_layout(width = 800, height = 800, autosize = True, showlegend = True,\n",
        "                   scene = dict(xaxis=dict(title = 'Beats Per Minute (BPM)', titlefont_color = 'black'),\n",
        "                                yaxis=dict(title = 'Energy', titlefont_color = 'black'),\n",
        "                                zaxis=dict(title = 'Danceability', titlefont_color = 'black')),\n",
        "                   font = dict(family = \"Arial\", color  = 'black', size = 12))\n",
        "\n",
        "PLOT.show()"
      ],
      "metadata": {
        "id": "m1dXHIGFUG04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando s√≥lo dos dimensiones:"
      ],
      "metadata": {
        "id": "7okOAKI4GRMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(dpi=120)\n",
        "for segment in df[\"Music Segments\"].unique():\n",
        "    plt.scatter(x = df[df[\"Music Segments\"]==segment]['Beats Per Minute (BPM)'],\n",
        "                y = df[df[\"Music Segments\"] ==segment]['Energy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2VhzrHq0FYFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(dpi=120)\n",
        "for segment in df[\"Music Segments\"].unique():\n",
        "    plt.scatter(x = df[df[\"Music Segments\"]==segment]['Danceability'],\n",
        "                y = df[df[\"Music Segments\"] ==segment]['Energy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B5nypRH6GVDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(f\"Score de silueta: {silhouette_score(X,clusters)}\")"
      ],
      "metadata": {
        "id": "EKMSEGpXgP5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï Preguntas:\n",
        "* Siendo K-Means, ¬øpor qu√© se no se ve la separaci√≥n perfecta?\n",
        "\n",
        "‚≠ï Ejercicio 1. Continuando con este m√©todo de K-Means:\n",
        "* ¬øQu√© valor de K es mejor? Puedes usar cualquiera de los 3 criteros de arriba, empezando por el *elbow value*.\n",
        "* Una vez que hayas escogido un valor para $K$, reportar los valores de las m√©tricas de clustering: score de Silueta, [Calinski-Harabasz Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score) y [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
        "\n",
        "‚≠ï Ejercicio 2:\n",
        "\n",
        "* Repetir el experimento, ahora usando Agglomerative Clustering y DBSCAN.\n",
        "* ¬øPuedes elevar las m√©tricas de clustering? Considera las m√©tricas score de Silueta y [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
        "* Explora visualmente algunas canciones de los clusters, ¬øtiene sentido el agrupamiento?"
      ],
      "metadata": {
        "id": "ACYNyhKPefTU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2gwkihWG6Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 3: `20newsgroups`"
      ],
      "metadata": {
        "id": "0saXLbUX3uh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retomamos el corpus de documentos `20newsgroups`. Realizaremos un clustering en las representaciones de los documentos para ver qu√© documentos se agrupan juntos.\n",
        "\n",
        "Este dataset se encuentra disponible en sklearn, [documentaci√≥n](https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)."
      ],
      "metadata": {
        "id": "TvuZKGxMXqlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Descargamos el dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "data_full = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),\n",
        "                               categories=['soc.religion.christian','sci.space','rec.autos'])"
      ],
      "metadata": {
        "id": "-1ujSDe-4xPx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = data_full.data\n",
        "print(f\"N√∫mero de documentos: {len(docs)}\")\n",
        "topics = data_full.target\n",
        "print(f\"N√∫mero de t√≥picos: {np.unique(topics).shape[0]}\")"
      ],
      "metadata": {
        "id": "Y5Z3lysw4-D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:2]"
      ],
      "metadata": {
        "id": "TfektfBKjNd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Representaciones *term-document* y *tf-idf*\n",
        "\n",
        "Hemos visto que los documentos se pueden representar por medio de vectores *sparse* cuyas componentes son conteos de las apariciones de ciertas palabras. Es decir, el modelo **BOW** (bag of words).\n",
        "\n",
        "Otra forma parecida de representar documentos es por medio de vectores cuyas componentes ahora representan √≠ndices *tf-idf* (term frequency - inverse document frequency). Estas tienen la ventaja de restar importancia a palabras que aparecen en muchos documentos.\n",
        "\n",
        "Para un corpus $D$ de documentos, el √≠ndice $tfidf$ de un t√©rmino $t$ en un documento $d$ se calcula como\n",
        "\n",
        "$$tfidf(t,d,D)=tf(t,d)idf(t,D)$$\n",
        "\n",
        "donde\n",
        "\n",
        "$$tf(t,d)=\\frac{f_{t,d}}{\\sum_{s\\in d}f_{s,d}}$$\n",
        "\n",
        "$$idf(t,D)=\\log\\frac{N}{|\\{d\\in D : t\\in d\\}|}$$\n",
        "\n",
        "$f_{t,d}$ es el n√∫mero de ocurrencias de un t√©rmino $t$ en un documento $d$, $N$ es el n√∫mero de documentos en $D$."
      ],
      "metadata": {
        "id": "pJ2riMEvdhLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "corpus = [\n",
        "        'This is the first document.',\n",
        "        'This document is the second document.',\n",
        "        'And this is the third one.',\n",
        "        'Is this the first document?',\n",
        "        ]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(f\"Vocabulario:\\n{tfidf_vectorizer.get_feature_names_out()}\")\n",
        "print(f\"Es una matriz sparse:\\n{X_tfidf}\")\n",
        "print(f\"Primeras tres columnas:\\n{X_tfidf.todense()[:,:3]}\")"
      ],
      "metadata": {
        "id": "s1X2lUBMfxFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos el modelo **BOW**"
      ],
      "metadata": {
        "id": "YqaG2PNowXpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_counts = count_vectorizer.fit_transform(corpus)\n",
        "print(f\"El vocabulario:\\n{count_vectorizer.get_feature_names_out()}\")\n",
        "print(f\"Las primeras tres columnas:\\n{X_counts.todense()[:,:3]}\")"
      ],
      "metadata": {
        "id": "9eph-7ZsglIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ],
      "metadata": {
        "id": "qe3PsrBrdqKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traemos la funci√≥n para limpiar texto de hace algunas sesiones"
      ],
      "metadata": {
        "id": "nelHZO35lj9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/03%20Machine%20Learning/data/limpiador_texto.py\"\n",
        "!wget --no-cache --backups=1 {url}"
      ],
      "metadata": {
        "id": "O8hfDrERlecE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download\n",
        "\n",
        "download('stopwords')\n",
        "download('punkt')"
      ],
      "metadata": {
        "id": "sm99y0I5lqlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from limpiador_texto import preprocesar_textos\n",
        "\n",
        "clean_docs = preprocesar_textos(docs)"
      ],
      "metadata": {
        "id": "MO7c9yE4l0iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos el clustering con las dos representaciones y calculamos las m√©tricas de rendimiento del clustering."
      ],
      "metadata": {
        "id": "WwvyWWKmlw3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                             max_features=200)\n",
        "X_tfidf = vectorizer.fit_transform(docs)\n",
        "# X_tfidf = vectorizer.fit_transform(clean_docs)\n",
        "print(X_tfidf.shape)\n",
        "\n",
        "clustering = KMeans(n_clusters=3, n_init='auto')\n",
        "clustering.fit(X_tfidf)\n",
        "clusters = clustering.labels_\n",
        "\n",
        "print(f\"AMI: {adjusted_mutual_info_score(topics,clusters)}\")\n",
        "print(f\"AR: {adjusted_rand_score(topics,clusters)}\")\n",
        "print(f\"Silhoutte del clustering obtenido: {silhouette_score(X_tfidf,clusters)}\")\n",
        "print(f\"Silhoutte de los t√≥picos: {silhouette_score(X_tfidf,topics)}\")"
      ],
      "metadata": {
        "id": "0CTGrbYo6BwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english',\n",
        "                             max_features=200)\n",
        "# X_counts = vectorizer.fit_transform(docs)\n",
        "X_counts = vectorizer.fit_transform(clean_docs)\n",
        "print(X_counts.shape)\n",
        "\n",
        "clustering = KMeans(n_clusters=3, n_init='auto',random_state=57)\n",
        "clustering.fit(np.asarray(X_counts.todense()))\n",
        "clusters_counts = clustering.labels_\n",
        "\n",
        "print(f\"AMI: {adjusted_mutual_info_score(topics,clusters_counts)}\")\n",
        "print(f\"AR: {adjusted_rand_score(topics,clusters_counts)}\")\n",
        "print(f\"Silhoutte del clustering obtenido: {silhouette_score(np.asarray(X_counts.todense()),clusters_counts)}\")\n",
        "print(f\"Silhoutte de los t√≥picos: {silhouette_score(np.asarray(X_counts.todense()),topics)}\")"
      ],
      "metadata": {
        "id": "nKV0jP58FYYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english',\n",
        "                             max_features=200)\n",
        "# X_counts = vectorizer.fit_transform(docs)\n",
        "X_counts = vectorizer.fit_transform(clean_docs)\n",
        "print(X_counts.shape)\n",
        "\n",
        "pca = PCA(svd_solver='auto',n_components=30)\n",
        "X_pca = pca.fit_transform(np.asarray(X_counts.todense()))\n",
        "print(X_pca.shape)\n",
        "\n",
        "clustering = KMeans(n_clusters=3, n_init='auto',random_state=57)\n",
        "clustering.fit(np.asarray(X_pca))\n",
        "clusters_counts = clustering.labels_\n",
        "\n",
        "print(f\"AMI: {adjusted_mutual_info_score(topics,clusters_counts)}\")\n",
        "print(f\"AR: {adjusted_rand_score(topics,clusters_counts)}\")\n",
        "print(f\"Silhoutte del clustering obtenido: {silhouette_score(X_pca,clusters_counts)}\")\n",
        "print(f\"Silhoutte de los t√≥picos: {silhouette_score(X_pca,topics)}\")"
      ],
      "metadata": {
        "id": "-ujg9kANLDN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podr√≠amos explorar el comportamiento del par√°metro `n_clusters` usando los criterios de intertia y silhoutte."
      ],
      "metadata": {
        "id": "xBGsfhT5hXVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sils = []\n",
        "inertias = []\n",
        "\n",
        "for n in range(2,20):\n",
        "    clustering = KMeans(n_clusters=n, n_init='auto')\n",
        "    clustering.fit(np.asarray(X_counts.todense()))\n",
        "    # sil = silhouette_score(np.asarray(X_counts.todense()),clustering.labels_)\n",
        "    # sils.append(sil)\n",
        "    inertias.append(clustering.inertia_)\n",
        "\n",
        "\n",
        "plt.plot(list(range(2,20)),inertias)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bpecWbud1DOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploraci√≥n de los clusters"
      ],
      "metadata": {
        "id": "ukpKNSfcdsad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos probar algunas t√©cnicas adicionales para explorar los clusters"
      ],
      "metadata": {
        "id": "VQz3WBlK5nuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq wordcloud"
      ],
      "metadata": {
        "id": "XkLYugf8Vn1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=3,figsize=(15,10),dpi=100)\n",
        "for k,ax in enumerate(axs):\n",
        "    cluster_text = \" \".join([clean_docs[j] for j,cluster in enumerate(clusters_counts) if cluster==k])\n",
        "    wc = WordCloud().generate(cluster_text)\n",
        "    ax.set_title(f\"Cluster {k}\")\n",
        "    ax.imshow(wc, interpolation='bilinear')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "whFCsDYgYwp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprimimos algunos documentos en cada cluster"
      ],
      "metadata": {
        "id": "6E8NOEer7UEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(43)\n",
        "\n",
        "for k in range(3):\n",
        "    docs_in_cluster = [clean_docs[j] for j,cluster in enumerate(clusters_counts) if cluster==k]\n",
        "    print(f\"Cluster {k} {20*'-'}\")\n",
        "    print(f\"N√∫mero de documentos en el cluster: {clusters_counts[clusters_counts==k].shape[0]}\")\n",
        "    some_docs = np.random.choice(docs_in_cluster,size=3)\n",
        "    print(some_docs)"
      ],
      "metadata": {
        "id": "iYxzthhrgTyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ï **Ejercicio**\n",
        "\n",
        "Repite el experimento variando el par√°metro `max_features` de los vectorizadores para ver si puedes subir las m√©tricas de rendimiento y obtener una mejor separaci√≥n de t√≥picos, esto hazlo *visualmente* (usando las nubes de palabras y algunos documentos).\n",
        "\n",
        "Usa los dos vectorizadores\n"
      ],
      "metadata": {
        "id": "PfeHRFHsmB-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 4: Segmentaci√≥n de clientes\n",
        "\n",
        "l objetivo de este an√°lisis es segmentar a los clientes de un centro comercial en grupos homog√©neos (clusters) basados en su comportamiento y caracter√≠sticas demogr√°ficas, como:\n",
        "\n",
        "* G√©nero\n",
        "* Ingreso anual (Annual Income (k$)).\n",
        "* Gasto en el centro comercial (Spending Score (1-100)).\n",
        "* Edad (Age).\n",
        "\n",
        "Esto permitir√° identificar patrones ocultos y dise√±ar estrategias de marketing personalizadas para cada grupo (ej: ofertas para \"clientes de alto ingreso pero bajo gasto\")."
      ],
      "metadata": {
        "id": "266J2HPStVjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/03%20Machine%20Learning/data/Mall_Customers.csv\"\n",
        "\n",
        "mall_df = pd.read_csv(url)\n",
        "mall_df.drop(columns=['CustomerID'],inplace=True)\n",
        "original_mall_df = mall_df.copy()\n",
        "mall_df"
      ],
      "metadata": {
        "id": "zQXw8edJtVGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü¢ Nos aseguramos que no hay datos faltantes"
      ],
      "metadata": {
        "id": "hCbn_Cnxyf2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mall_df.isna().sum()"
      ],
      "metadata": {
        "id": "bODgImjXkmfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mall_df.dtypes"
      ],
      "metadata": {
        "id": "aCaOm_vrd4qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure()\n",
        "sns.countplot(x='Gender',data=mall_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QhStwQ5UwTpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Con el m√©todo `describe` de pandas determina el rango de las variables num√©ricas"
      ],
      "metadata": {
        "id": "eGa2a4VTyzHS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPzv7V9czCWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Haz *one-hot encoding* con la variable categ√≥rica usando el m√©todo `get_dummies`, no olvides el hiperpar√°metro `drop_first=True`."
      ],
      "metadata": {
        "id": "UVZgkKvNzMH5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KzjPEqn7zL0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Extrae las variables (features) de cada instancia y define la matrix $X$"
      ],
      "metadata": {
        "id": "DdztEGhxymir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X ="
      ],
      "metadata": {
        "id": "kbL0-bdgwsHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùóAqu√≠ no hay divisi√≥n train\\test"
      ],
      "metadata": {
        "id": "dZ7cZu_t0fv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Aplica reescalamiento a `X`"
      ],
      "metadata": {
        "id": "h7PYEzMr0NBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n"
      ],
      "metadata": {
        "id": "eu8R9IIVyyei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Clusteriza las instancias, usa K-Means y prueba con dos valores de tu elecci√≥n para el n√∫mero de clusters"
      ],
      "metadata": {
        "id": "jlpp6dvL0vgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n"
      ],
      "metadata": {
        "id": "_22m0QZZ1Bbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Extrae los clusters (es decir, el arreglo que dice a qu√© cluster pertenece cada instancia) con el atributo `labels_`"
      ],
      "metadata": {
        "id": "TWRvAjQi1EPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "clusters ="
      ],
      "metadata": {
        "id": "kmpJPKSm1Dwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Evalua el clustering usando la m√©trica silueta.\n",
        "\n",
        "**Recuerda que esta m√©trica es un n√∫mero $-1\\leq s\\leq 1$** y entre m√°s alto es mejor."
      ],
      "metadata": {
        "id": "BpfVDI452kEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n"
      ],
      "metadata": {
        "id": "PO4GUT3B2zzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü¢ Visualicemos los resultados. Dado que no son tantos ejemplos, imprimamos un dataframe mostrando cada uno de los clusters.\n",
        "\n",
        "¬øC√≥mo etiquetarias a cada cluster? Es decir, ¬øqu√© comparten en com√∫n cada cluster?"
      ],
      "metadata": {
        "id": "HewZZXAx1d3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_clusters = np.unique(clusters).shape[0]\n",
        "\n",
        "for i in range(num_clusters):\n",
        "    print(f\"Cluster {i}\")\n",
        "    display(original_mall_df[clusters==i])"
      ],
      "metadata": {
        "id": "ap5WviK71dZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ **Extra**: Hacer reducci√≥n de dimensionalidad usando PCA a dos dimensiones y graficar todas las instancias coloreadas por cluster"
      ],
      "metadata": {
        "id": "JK5cm7LT3Vjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "7rj21wtr2fuh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
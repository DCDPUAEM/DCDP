{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/02%20An%C3%A1lisis%20Estad%C3%ADstico/notebooks/4_WebScrapping.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contenido\"></a>\n",
    "<h1><center>Contenido | M√≥dulo 2</center><h1>\n",
    "    \n",
    "---\n",
    "* [Introducci√≥n al Web Scrapping con Python](#a)   \n",
    "* [Breve ejemplo](#b) \n",
    "* [CoronaScrapp](#c) \n",
    "* [Referencias](#d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"a\"></a>\n",
    "<h1><center>2.11. Introducci√≥n - Web Scrapping</center></h1>\n",
    "\n",
    "[Regreso a contenido](#contenido)\n",
    "\n",
    "---\n",
    "![alt text](https://www.grid.cl/blog/wp-content/uploads/2019/03/001-efficient-web-scraping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que **INTERNET** est√° compuesta por muchos millones de documentos enlazados entre s√≠, conocidos tambi√©n como p√°ginas web. \n",
    "\n",
    "El texto fuente de las p√°ginas web est√° escrito en lenguaje Hypertext Markup Language (HTML). Los c√≥digos fuente en HTML son una mezcla de informaciones legibles para los humanos y c√≥digos legibles para las m√°quinas, llamados tags o etiquetas. El navegador, como puede ser Chrome, Firefox, Safari o Edge, procesa el texto fuente, interpreta las etiquetas y presenta al usuario la informaci√≥n que contienen.\n",
    "\n",
    "Para extraer del texto fuente √∫nicamente la informaci√≥n que le interesa al usuario, se utiliza un <font color=red>software especial</font>. Se trata de los programas llamados web scrapers, crawlers, spiders o, simplemente, bots, que examinan el texto fuente de las p√°ginas en busca de patrones concretos y extraen la informaci√≥n que contienen. Los datos conseguidos mediante web scraping posteriormente se resumen, combinan, eval√∫an o almacenan para ser usados m√°s adelante.\n",
    "\n",
    "En esta notebook veremos un poco del por qu√© Python resulta especialmente √∫til para la creaci√≥n de web scrapers y una introducci√≥n a este tema junto con unos ejemplos ([tambi√©n se puede hacer en R](https://www.r-bloggers.com/2019/07/beautifulsoup-vs-rvest/)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping en t√©rminos generales\n",
    "\n",
    "El esquema b√°sico del web scraping es sencillo de explicar.... \n",
    "\n",
    "En primer lugar, el desarrollador del scraper analiza el texto fuente en HTML de la p√°gina web en cuesti√≥n. Por lo general, encontrar√° patrones claros que permitir√°n extraer la informaci√≥n deseada. El scraper ser√° entonces programado para identificar dichos patrones y realizar√° el resto del trabajo autom√°ticamente:\n",
    "\n",
    "   * Abrir la p√°gina web a trav√©s del URL\n",
    "   * Extraer autom√°ticamente los datos estructurados a partir de los patrones\n",
    "   * Resumir, almacenar, evaluar o combinar los datos extra√≠dos, entre otras acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos de aplicaci√≥n del web scraping\n",
    "\n",
    "El web scraping puede tener aplicaciones muy diversas. Adem√°s de la indexaci√≥n de buscadores, el web scraping tambi√©n puede usarse con los siguientes fines, entre muchos otros:\n",
    "\n",
    "  * Crear bases de datos de contactos\n",
    "  * Controlar y comparar ofertas online\n",
    "  * Reunir datos de diversas fuentes online\n",
    "  * Observar la evoluci√≥n de la presencia y la reputaci√≥n online\n",
    "  * Reunir datos financieros, meteorol√≥gicos o de otro tipo\n",
    "  * Observar cambios en el contenido de p√°ginas web\n",
    "  * Reunir datos con fines de investigaci√≥n\n",
    "  * Realizar exploraciones de datos o data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herramientas de scraping para Python\n",
    "\n",
    "Python incluye diversas herramientas consolidadas para realizar proyectos de scraping:\n",
    "\n",
    "   * [Scrapy](https://scrapy.org/)\n",
    "   * [Selenium](https://selenium-python.readthedocs.io/)\n",
    "   * [BeautifulSoup](https://pypi.org/project/beautifulsoup4/)\n",
    "\n",
    "A continuaci√≥n, nos enfocaremos solamente BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura de la p√°gina HTML\n",
    "\n",
    "El lenguaje de marcado de hipertexto (HTML) es el lenguaje de marcado est√°ndar para documentos dise√±ados para mostrarse en un navegador web. HTML describe la estructura de una p√°gina web y se puede utilizar con hojas de estilo en cascada (CSS) y un lenguaje de secuencias de comandos como JavaScript para crear sitios web interactivos. HTML consta de una serie de elementos que \"le dicen\" al navegador c√≥mo mostrar el contenido. Por √∫ltimo, los elementos se representan mediante etiquetas.\n",
    "\n",
    "Aqu√≠ hay algunas etiquetas:\n",
    "\n",
    "    La declaraci√≥n <!DOCTYPE html> define este documento como HTML5.\n",
    "    El elemento <html> es el elemento ra√≠z de una p√°gina HTML.\n",
    "    La etiqueta <div> define una divisi√≥n o una secci√≥n en un documento HTML. Suele ser un contenedor de otros elementos.\n",
    "    El elemento <head> contiene metainformaci√≥n sobre el documento.\n",
    "    El elemento <title> especifica un t√≠tulo para el documento.\n",
    "    El elemento <body> contiene el contenido de la p√°gina visible.\n",
    "    El elemento <h1> define un encabezado grande.\n",
    "    El elemento <p> define un p√°rrafo.\n",
    "    El elemento <a> define un hiperv√≠nculo.\n",
    "\n",
    "Las etiquetas HTML normalmente vienen en pares como $<p>$ y $</p>$. La primera etiqueta de un par es la etiqueta de apertura, la segunda etiqueta es la etiqueta de cierre. La etiqueta final se escribe como la etiqueta inicial, pero con una barra diagonal insertada antes del nombre de la etiqueta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML tiene una estructura en forma de √°rbol üå≥ üå≤ gracias al Modelo de objetos de documento (DOM), una interfaz multiplataforma e independiente del idioma. As√≠ es como se ve un √°rbol HTML muy simple. \n",
    "![img](https://mechomotive.com/wp-content/uploads/2021/07/HTML-document-tree-representation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CALIPSO\\AppData\\Local\\Temp\\ipykernel_8408\\2058709175.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\" dir=\"ltr\">\n",
       "<head>\n",
       "  <title>Intro to HTML</title>\n",
       "</head>\n",
       "\n",
       "<body>\n",
       "  <h1>Heading h1</h1>\n",
       "  <h2>Heading h2</h2>\n",
       "  <h3>Heading h3</h3>\n",
       "  <h4>Heading h4</h4>\n",
       "\n",
       "  <p>\n",
       "    That's a text paragraph. You can also <b>bold</b>, <mark>mark</mark>, <ins>underline</ins>, <del>strikethrough</del> and <i>emphasize</i> words.\n",
       "    You can also add links - here's one to <a href=\"https://en.wikipedia.org/wiki/Main_Page\">Wikipedia</a>.\n",
       "  </p>\n",
       "\n",
       "  <p>\n",
       "    This <br> is a paragraph <br> with <br> line breaks\n",
       "  </p>\n",
       "\n",
       "  <p style=\"color:red\">\n",
       "    Add colour to your paragraphs.\n",
       "  </p>\n",
       "\n",
       "  <p>Unordered list:</p>\n",
       "  <ul>\n",
       "    <li>Python</li>\n",
       "    <li>R</li>\n",
       "    <li>Julia</li>\n",
       "  </ul>\n",
       "\n",
       "  <p>Ordered list:</p>\n",
       "  <ol>\n",
       "    <li>Data collection</li>\n",
       "    <li>Exploratory data analysis</li>\n",
       "    <li>Data analysis</li>\n",
       "    <li>Policy recommendations</li>\n",
       "  </ol>\n",
       "  <hr>\n",
       "\n",
       "  <!-- This is a comment -->\n",
       "\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\" dir=\"ltr\">\n",
    "<head>\n",
    "  <title>Intro to HTML</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "  <h1>Heading h1</h1>\n",
    "  <h2>Heading h2</h2>\n",
    "  <h3>Heading h3</h3>\n",
    "  <h4>Heading h4</h4>\n",
    "\n",
    "  <p>\n",
    "    That's a text paragraph. You can also <b>bold</b>, <mark>mark</mark>, <ins>underline</ins>, <del>strikethrough</del> and <i>emphasize</i> words.\n",
    "    You can also add links - here's one to <a href=\"https://en.wikipedia.org/wiki/Main_Page\">Wikipedia</a>.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    This <br> is a paragraph <br> with <br> line breaks\n",
    "  </p>\n",
    "\n",
    "  <p style=\"color:red\">\n",
    "    Add colour to your paragraphs.\n",
    "  </p>\n",
    "\n",
    "  <p>Unordered list:</p>\n",
    "  <ul>\n",
    "    <li>Python</li>\n",
    "    <li>R</li>\n",
    "    <li>Julia</li>\n",
    "  </ul>\n",
    "\n",
    "  <p>Ordered list:</p>\n",
    "  <ol>\n",
    "    <li>Data collection</li>\n",
    "    <li>Exploratory data analysis</li>\n",
    "    <li>Data analysis</li>\n",
    "    <li>Policy recommendations</li>\n",
    "  </ol>\n",
    "  <hr>\n",
    "\n",
    "  <!-- This is a comment -->\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herramientas para desarrolladores de Chrome\n",
    "\n",
    "[Chrome DevTools](https://developer.chrome.com/docs/devtools/) es un conjunto de herramientas para desarrolladores web integradas directamente en el navegador Google Chrome. DevTools puede ayudar a ver y editar p√°ginas web. Usaremos la herramienta de Chrome para inspeccionar una p√°gina HTML y encontrar qu√© elementos corresponden a los datos que podr√≠amos querer raspar.\n",
    "ejercicio corto\n",
    "\n",
    "Para obtener algo de experiencia con la estructura de la p√°gina HTML y Chrome DevTools, buscaremos y ubicaremos elementos en IMDB.\n",
    "\n",
    "Sugerencia: Pulse Comando+Opci√≥n+C (Mac) o Control+May√∫s+C (Windows, Linux) para acceder al panel de elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping con `requests` y `BeautifulSoup`\n",
    "\n",
    "Usaremos `requests` y `BeautifulSoup` para acceder y raspar el contenido de [la p√°gina de inicio de IMDB](https://www.imdb.com).\n",
    "\n",
    "### ¬øQu√© es `BeautifulSoup`?\n",
    "\n",
    "Es una biblioteca de Python para extraer datos de archivos HTML y XML. Proporciona m√©todos para navegar por la estructura de √°rbol del documento que discutimos antes y raspar su contenido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests import get\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import string\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>403 Forbidden</title>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#¬†IMDB's homepage\n",
    "imdb_url = 'https://www.imdb.com'\n",
    "\n",
    "#¬†Usamos requests para obtener los datos de la URL dada\n",
    "imdb_response = requests.get(imdb_url)\n",
    "\n",
    "#¬†Transformamos todo el codigo HTML usando beautiful soup\n",
    "imdb_soup = BeautifulSoup(imdb_response.text, 'html.parser')\n",
    "\n",
    "#¬†Titulo de la pagina transformada\n",
    "imdb_soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'403 Forbidden'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#¬†Podemos obtenerlo de igual forma sin los tags de HTML\n",
    "imdb_soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://www.imdb.com/chart/top'\n",
    "page=get(url).content\n",
    "soup=BeautifulSoup(page,'html.parser')\n",
    "class_=soup.find_all(name='div',attrs={'class':'wlb_ribbon'})\n",
    "movie_ids=[c['data-tconst'] for c in class_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://www.omdbapi.com/?i=\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#print(url+movie_ids[i]+\"&apikey=de12b217\")\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m r\u001b[38;5;241m=\u001b[39mrequests\u001b[38;5;241m.\u001b[39mget(url\u001b[38;5;241m+\u001b[39mmovie_ids[i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&apikey=de12b217\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      8\u001b[0m     movie_info[i]\u001b[38;5;241m.\u001b[39mappend(r[a])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "movie_info=[[] for i in range(len(movie_ids))]\n",
    "\n",
    "for i in range(250):\n",
    "    url='http://www.omdbapi.com/?i='\n",
    "    #print(url+movie_ids[i]+\"&apikey=de12b217\")\n",
    "    r=requests.get(url+movie_ids[i]+\"&apikey=de12b217\").json()\n",
    "    for a in r.keys():\n",
    "        movie_info[i].append(r[a])\n",
    "        \n",
    "df_omdb=pd.DataFrame(movie_info,columns=r.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://www.imdb.com/title/'\n",
    "t='/plotsummary?ref_=tt_stry_pl'\n",
    "plot=[[] for i in range(len(movie_ids))]\n",
    "for i in range(250):\n",
    "    #print(url+df_omdb.imdbID[i]+t)\n",
    "    page=get(url+df_omdb.imdbID[i]+t).content\n",
    "    soup=BeautifulSoup(page,'html.parser')\n",
    "    class_=soup.find_all(name='li',attrs={'class':'ipl-zebra-list__item'})\n",
    "    for j in class_:\n",
    "        plot[i].append(j.get_text(strip = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb['Plot']=plot\n",
    "df_omdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos\n",
    "\n",
    "El primer paso para limpiar los datos es convertir Year en una variable categ√≥rica. \n",
    "\n",
    "Se elegir√° el a√±o del 2000 como corte adecuado. Las pel√≠culas lanzadas antes del 2000 se convirtieron en 0 y despu√©s de 2000 en 1. Despu√©s de hacer esto, realizamos un one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb.Year=pd.to_numeric(df_omdb.Year)\n",
    "for i in range(250):\n",
    "    if df_omdb.Year[i]<2000:\n",
    "        df_omdb.Year[i]=0\n",
    "    else:\n",
    "        df_omdb.Year[i]=1\n",
    "dummy_year=pd.get_dummies(df_omdb.Year)\n",
    "\n",
    "for i in range(250):\n",
    "    df_omdb.Runtime[i]=df_omdb.Runtime[i].split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb['Runtime'] = pd.to_numeric(df_omdb['Runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(10,5))\n",
    "ax.hist(df_omdb['Runtime'],edgecolor='white',align='right')\n",
    "ax.axvline(x=np.mean(df_omdb['Runtime']),c='r')\n",
    "ax.axvline(x=np.mean(df_omdb['Runtime'])-np.std(df_omdb['Runtime']),c='b',ls='--')\n",
    "ax.axvline(x=np.mean(df_omdb['Runtime'])+np.std(df_omdb['Runtime']),c='b',ls='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb['Runtime']=pd.to_numeric(df_omdb['Runtime'],errors='coerce')\n",
    "for i in range(250):\n",
    "    if df_omdb.Runtime[i]<=125:\n",
    "        df_omdb.Runtime[i]=0\n",
    "    else: \n",
    "        df_omdb.Runtime[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(column_name):\n",
    "    \"\"\"This function takes a column from the dataframe and splits two elements\n",
    "       if they are separated by a comma.\n",
    "       For ex. in Actors column there might be values such as Christian Bale, Morgan Freeman.\n",
    "       This will separate these two actors and store them individually in a list.\"\"\"\n",
    "    name=set()\n",
    "    for name_string in df_omdb[column_name]:\n",
    "        name.update(name_string.split(', '))\n",
    "    name=sorted(name)\n",
    "    return name\n",
    "\n",
    "def top(column_name):\n",
    "    \"\"\"This function takes its input as name of the column and returns a sorted list of the \n",
    "       elements which occur very frequently in that column in descending order.\"\"\"\n",
    "    \n",
    "    name=clean(column_name)\n",
    "    dummy_name=pd.DataFrame()\n",
    "    for n in name:\n",
    "        dummy_name[n]=[int(n in nm.split(', ')) for nm in df_omdb[column_name]] \n",
    "    \n",
    "    namelist=[n for n in name]\n",
    "    nlt=dummy_name[namelist].sum()\n",
    "    nlt=nlt.sort_values(axis=0,ascending=False)\n",
    "    return nlt.index\n",
    "    \n",
    "def plot_column(column_name,n_elem_display=0):\n",
    "    \"\"\" This function is used to plot a bar graph of a column of the dataframe.\n",
    "        It takes its argument as name of column and number of elements to display and\n",
    "        return a bar graph of the user defined number of top elements which occur\n",
    "        frequently in that column.\"\"\"\n",
    "    \n",
    "    name=clean(column_name)\n",
    "    dummy_name=pd.DataFrame()\n",
    "    for n in name:\n",
    "        dummy_name[n]=[int(n in nm.split(', ')) for nm in df_omdb[column_name]] \n",
    "    \n",
    "    namelist=[n for n in name]\n",
    "    nlt=dummy_name[namelist].sum()\n",
    "    nlt=nlt.sort_values(axis=0,ascending=False)\n",
    "    if n_elem_display !=0:\n",
    "        return nlt[:n_elem_display].plot(kind = \"bar\",figsize=(10,10))\n",
    "    else:\n",
    "        return nlt[:].plot(kind = \"bar\",figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column('Genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegiremos todos los g√©neros como nuestros predictores en nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the unique genres contained in the dataframe\n",
    "genres=clean('Genre')\n",
    "#Add one column for every genre in the dataframe\n",
    "for genre in genres:\n",
    "    df_omdb[\"genre:\"+genre] = [int(genre in g.split(', ')) for g in df_omdb.Genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analicemos la cantidad de actores que se pueden usar como predictores en nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column('Actors',30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, podemos tomar a los 30 actores principales, cada uno con m√°s de 3 pel√≠culas, en la lista de 250 pel√≠culas principales de imdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding actors to our dataset\n",
    "actors=top('Actors')\n",
    "actors\n",
    "for actor in actors[:30]:\n",
    "    df_omdb[\"Actor:\"+actor] = [int(actor in a.split(', ')) for a in df_omdb.Actors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ \n",
    "\n",
    "Ahora los Directores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column('Director',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directors=top('Director')\n",
    "    \n",
    "for director in directors[:20]:\n",
    "    df_omdb[\"Director:\"+director] = [int(director in d.split(', ')) for d in df_omdb.Director]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizar si tomar escritores o no como predictores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers1=set()\n",
    "writers2=set()\n",
    "for writer_string in df_omdb.Writer:\n",
    "    writers1.update(writer_string.split(', '))\n",
    "for j in writers1:\n",
    "    writers2.update(j.rsplit(' (')[:1])\n",
    "writers2 = sorted(writers2)\n",
    "\n",
    "dummy_writers=pd.DataFrame()\n",
    "\n",
    "# Add one column for every writer in the dataframe\n",
    "for writer in writers2:\n",
    "    dummy_writers[writer] = [int(writer in w.split(', ')) for w in df_omdb.Writer]   \n",
    "dummy_writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writerlist=[w for w in writers2]\n",
    "wlt=dummy_writers[writerlist].sum()\n",
    "wlt=wlt.sort_values(axis=0,ascending=False)\n",
    "wlt.iloc[0:10].plot(kind = \"bar\",figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que no hay muchos escritores que tengan un n√∫mero significativo de pel√≠culas, decidimos no tomar a los escritores como uno de nuestros predictores.\n",
    "\n",
    "Ahora, exploraremos el predictor de lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column('Language',11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column('Country',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding all of the top 10 countries to our datset\n",
    "countries=top('Country')\n",
    "\n",
    "for country in countries[:10]:\n",
    "    df_omdb[\"Country:\"+country] = [int(country in c.split(', ')) for c in df_omdb.Country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,10)})\n",
    "sns.countplot(df_omdb['Metascore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc(data,bgcolor,title):\n",
    "    plt.figure(figsize = (100,100))\n",
    "    wc = WordCloud(background_color = bgcolor, max_words = 1000,  max_font_size = 50)\n",
    "    wc.generate(' '.join(data))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc(df_omdb,'black','Most Used Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Web scraping con Scrapy\n",
    "\n",
    "Scrapy, una de las herramientas para hacer web scraping con Python que presentamos, utiliza un analizador sint√°ctico o parser HTML para extraer datos del texto fuente (en HTML) de la web siguiendo este esquema:\n",
    "\n",
    "$$URL ‚Üí Solicitud HTTP ‚Üí HTML ‚Üí Scrapy$$\n",
    "\n",
    "El concepto clave del desarrollo de scrapers con Scrapy son los llamados web spiders, programas de scraping sencillos y basados en Scrapy. Cada spider (ara√±a) est√° programado para scrapear una web concreta y va descolg√°ndose de p√°gina a p√°gina. La programaci√≥n usada est√° orientada a objetos: cada spider es una clase de Python propia.\n",
    "\n",
    "Adem√°s del paquete de Python en s√≠, la instalaci√≥n de Scrapy incluye una herramienta de l√≠nea de comandos, la Scrapy Shell, que permite controlar los spiders. Adem√°s, los spiders ya creados pueden almacenarse en la Scrapy Cloud. Desde all√≠, se ejecutan con tiempos establecidos. De esta forma pueden scrapearse tambi√©n sitios web complejos sin necesidad de utilizar para ello el propio ordenador ni la propia conexi√≥n a Internet. Otra manera de hacerlo es crear un servidor de web scraping propio usando el software de c√≥digo abierto Scrapyd.\n",
    "\n",
    "Scrapy es una plataforma consolidada para aplicar t√©cnicas de web scraping con Python. Su arquitectura est√° orientada a las necesidades de proyectos profesionales. Scrapy cuenta, por ejemplo, con un pipeline integrado para procesar los datos extra√≠dos. La apertura de las p√°ginas en Scrapy se produce de forma as√≠ncrona, es decir, con la posibilidad de descargar varias p√°ginas simult√°neamente. Por ello, Scrapy es una buena opci√≥n para proyectos de scraping que hayan de procesar de grandes vol√∫menes de p√°ginas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Web scraping con Selenium\n",
    "\n",
    "El software libre Selenium es un framework para realizar test automatizados de software a aplicaciones web. En principio, fue desarrollado para poner a prueba p√°ginas y apps web, pero el WebDriver de Selenium tambi√©n puede usarse con Python para realizar scraping. Si bien Selenium en s√≠ no est√° escrito en Python, con este lenguaje de programaci√≥n es posible acceder a las funciones del software.\n",
    "\n",
    "A diferencia de Scrapy y de BeautifulSoup, Selenium no trabaja con el texto fuente en HTML de la web en cuesti√≥n, sino que carga la p√°gina en un navegador sin interfaz de usuario. El navegador interpreta entonces el c√≥digo fuente de la p√°gina y crea, a partir de √©l, un Document Object Model (modelo de objetos de documento o DOM). Esta interfaz estandarizada permite poner a prueba las interacciones de los usuarios. De esta forma se consigue, por ejemplo, simular clics y rellenar formularios autom√°ticamente. Los cambios en la web que resultan de dichas acciones se reflejan en el DOM. La estructura del proceso de web scraping con Selenium es la siguiente:\n",
    "\n",
    "$$URL ‚Üí Solicitud HTTP ‚Üí HTML ‚Üí Selenium ‚Üí DOM$$\n",
    "\n",
    "Puesto que el DOM se genera de manera din√°mica, Selenium permite scrapear tambi√©n p√°ginas cuyo contenido ha sido generado mediante JavaScript. El acceso a contenidos din√°micos es la ventaja m√°s importante de Selenium. En t√©rminos pr√°cticos, Selenium tambi√©n puede usarse en combinaci√≥n con Scrapy o con BeautifulSoup: Selenium proporcionar√≠a el texto fuente, mientras que la segunda herramienta se encargar√≠a del an√°lisis sint√°ctico y la evaluaci√≥n de los datos. En este caso, el esquema que se seguir√≠a tendr√≠a esta forma:\n",
    "\n",
    "$$URL ‚Üí Solicitud HTTP ‚Üí HTML ‚Üí Selenium ‚Üí DOM ‚Üí HTML ‚Üí Scrapy / BeautifulSoup$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Web scraping con BeautifulSoup\n",
    "\n",
    "De las tres herramientas que presentamos para realizar web scraping con Python, BeautifulSoup es la m√°s antigua. Al igual que en el caso de Scrapy, se trata de un parser o analizador sint√°ctico HTML. El web scraping con BeautifulSoup tiene la siguiente estructura:\n",
    "\n",
    "$$URL ‚Üí Solicitud HTTP ‚Üí HTML ‚Üí BeautifulSoup$$\n",
    "\n",
    "Sin embargo, a diferencia de Scrapy, en BeautifulSoup el desarrollo del scraper no requiere una programaci√≥n orientada a objetos, sino que el scraper se redacta como una sencilla secuencia de comandos o script. Con ello, BeautifulSoup ofrece el m√©todo m√°s f√°cil para pescar informaci√≥n de la sopa de tags a la que hace honor su nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### En resumen\n",
    "\n",
    "¬øQu√© herramienta deber√≠as elegir para tu proyecto? \n",
    "\n",
    "En resumen: escoge **BeautifulSoup** si necesitas un desarrollo r√°pido o si quieres familiarizarte primero con los conceptos de Python y de web scraping. **Scrap**y, por su parte, te permite realizar complejas aplicaciones de web scraping en Python si dispones de los conocimientos necesarios. **Selenium** ser√° tu mejor opci√≥n si tu prioridad es extraer contenidos din√°micos con Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"b\"></a>\n",
    "### 2.11.1 Breves ejemplos de web scraping\n",
    "[Regreso a contenido](#contenido)\n",
    "\n",
    "---\n",
    "\n",
    " * Extraer citas y autores con Python y BeautifulSoup\n",
    "\n",
    "La p√°gina [web Quotes to Scrape](http://quotes.toscrape.com/) ofrece toda una colecci√≥n de citas de personajes famosos pensadas especialmente para ser objeto de test de scraping, para que no tengas que preocuparte por incumplir las condiciones de uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar m√≥dulos\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "# Direcci√≥n de la p√°gina web\n",
    "url = \"http://quotes.toscrape.com/\"\n",
    "# Ejecutar GET-Request\n",
    "response = requests.get(url)\n",
    "# Analizar sint√°cticamente el archivo HTML de BeautifulSoup del texto fuente\n",
    "html = BeautifulSoup(response.text, 'html.parser')\n",
    "# Extraer todas las citas y autores del archivo HTML\n",
    "quotes_html = html.find_all('span', class_=\"text\")\n",
    "authors_html = html.find_all('small', class_=\"author\")\n",
    "# Crear una lista de las citas\n",
    "quotes = list()\n",
    "for quote in quotes_html:\n",
    "    quotes.append(quote.text)\n",
    "# Crear una lista de los autores\n",
    "authors = list()\n",
    "for author in authors_html:\n",
    "    authors.append(author.text) \n",
    "# Para hacer el test: combinar y mostrar las entradas de ambas listas\n",
    "for t in zip(quotes, authors):\n",
    "    print(t)\n",
    "# Guardar las citas y los autores en un archivo CSV en el directorio actual\n",
    "# Abrir el archivo con Excel / LibreOffice, etc.\n",
    "with open('./zitate.csv', 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, dialect='excel')\n",
    "    csv_writer.writerows(zip(quotes, authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"c\"></a>\n",
    "### 2.11.2 Scrapping Coronavirus\n",
    "[Regreso a contenido](#contenido)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "from tabulate import tabulate \n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today=datetime.date.today().strftime(\"%m-%d-%Y\")\n",
    "data_date=datetime.date.today()-datetime.timedelta(days=1)\n",
    "print(\"Today is {}\".format(today))\n",
    "data_date=data_date.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://www.worldometers.info/coronavirus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get web data\n",
    "req = requests.get(url)\n",
    "response = req.content\n",
    "# parse web data\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the table\n",
    "#table is in the last of the page\n",
    "\n",
    "thead= soup.find_all('thead')[-1]\n",
    "print(thead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all rows in thead\n",
    "head = thead.find_all('tr')\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the table data content\n",
    "tbody = soup.find_all('tbody')[0]\n",
    "tbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = tbody.find_all('tr')\n",
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the table contents\n",
    "\n",
    "# container for  column title\n",
    "head_rows = []\n",
    "\n",
    "\n",
    "# loop through the head and append each row to head\n",
    "for tr in head:\n",
    "    td = tr.find_all(['th', 'td'])\n",
    "    row = [i.text for i in td]\n",
    "    head_rows.append(row)\n",
    "print(head_rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# container for contents\n",
    "body_rows = []\n",
    "\n",
    "# loop through the body and append each row to body\n",
    "for tr in body:\n",
    "    td = tr.find_all(['th', 'td'])\n",
    "    row = [i.text for i in td]\n",
    "    body_rows.append(row)\n",
    "print(body_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bs = pd.DataFrame(body_rows[:len(body_rows)],columns=head_rows[0]) \n",
    "df_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continentdata\n",
    "cols=['Continent','TotalCases', 'NewCases', 'TotalDeaths', 'NewDeaths', 'TotalRecovered',\n",
    "       'NewRecovered', 'ActiveCases', 'Serious,Critical', ]\n",
    "\n",
    "continent_data = df_bs.iloc[:8, :-3].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# drop unwanted columns\n",
    "continent_data = continent_data.drop('#', axis=1)\n",
    "#rearrange Columns Sequence\n",
    "continent_data = continent_data[cols]\n",
    "continent_data['Continent'].loc[6]=\"Not Assigned\"\n",
    "continent_data['Continent'].loc[7]=\"World\"\n",
    "\n",
    "\n",
    "continent_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "<a id=\"d\"></a>\n",
    "<h1><center>Referencias y links de inter√©s</center></h1>\n",
    "\n",
    "[Regreso a contenido](#contenido)\n",
    "\n",
    "---\n",
    "\n",
    "* [Rvest para R](https://www.r-bloggers.com/2019/07/beautifulsoup-vs-rvest/)\n",
    "* [Scrapy](https://scrapy.org/)\n",
    "* [Selenium](https://selenium-python.readthedocs.io/)\n",
    "* [BeautifulSoup](https://pypi.org/project/beautifulsoup4/)\n",
    "\n",
    "-------\n",
    "\n",
    "* [Tutorial Scrapy](https://docs.scrapy.org/en/latest/intro/tutorial.html)\n",
    "* [Ejemplos de Scrapy](https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/)\n",
    "\n",
    "-------\n",
    "* [Tutorial Selenium](https://selenium-python.readthedocs.io/getting-started.html)\n",
    "* [Ejemplos de Selenium](https://www.guru99.com/selenium-python.html)\n",
    "\n",
    "-------\n",
    "\n",
    "* [Tutorial de BeautifulSoap](https://www.dataquest.io/blog/web-scraping-tutorial-python/)\n",
    "* [Ejemplos de BeautifulSoap](https://realpython.com/beautiful-soup-web-scraper-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
